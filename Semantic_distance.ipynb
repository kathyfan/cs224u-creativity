{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Semantic Distance.ipynb","provenance":[{"file_id":"18xltzQFDw5Lsdhzi_4CHL8VuI0Nplu7V","timestamp":1621333562891}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UaWDRfwsg8VS"},"source":["# Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0h1JhtZCgv9","executionInfo":{"status":"ok","timestamp":1621583572966,"user_tz":420,"elapsed":21123,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"02198e73-9128-4f09-f906-96f5769caf84"},"source":["# Mount Google Drive.\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MpUJ3wmbCl7y","executionInfo":{"status":"ok","timestamp":1621583887592,"user_tz":420,"elapsed":192,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import random\n","random.seed(1)\n","import re\n","\n","# Set working directory.\n","os.chdir('/content/gdrive/My Drive/personal/CS224U/project')\n","# os.listdir() # Uncomment to sanity check that you're in the right directory."],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PwCk0EBEg1sS"},"source":["# Word count and score distributions"]},{"cell_type":"code","metadata":{"id":"P4IWfPB5duIZ"},"source":["# Takes a dataframe and adds a 'wordcount' column (modifies original df)\n","# Requirement: df has existing 'text' column\n","# Returns modified df with 'wordcount' column added\n","def add_wordcount_col(df):\n","  assert 'text' in df\n","  df['wordcount'] = df['text'].str.split().str.len()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mIdac2q9elXi"},"source":["# Takes a dataframe and returns a new dataframe containing only rows where wordcount < max_words\n","# Requirement: input df has existing 'wordcount' column\n","def restrict_by_wordcount(df, max_words):\n","  assert 'wordcount' in df\n","  df_restricted = df.loc[df['wordcount'] <= max_words]\n","  return df_restricted"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qV3MkfpFBAna"},"source":["## Calculate Semantic Distance"]},{"cell_type":"code","metadata":{"id":"ZWAUD3t8C231"},"source":["import string\n","import scipy.spatial.distance\n","\n","# Takes two sequence and create a composite vector for each sequence.\n","# Return the cosine similarity between the two vectors.\n","\n","def get_distance_between_texts(text1, text2, VSM, multiply = True,\n","                               tokenizer = nltk.word_tokenize,\n","                               remove_stopwords = True,\n","                               remove_punct = True):\n","  \n","  v1 = get_text_vector(text1, VSM, multiply, tokenizer, remove_stopwords),\n","  v2 = get_text_vector(text2, VSM, multiply, tokenizer, remove_stopwords)\n","\n","  return scipy.spatial.distance(v1, v2)\n","\n","# Takes a sequence and a VSM. Return a composite vector that represents the sequence\n","# Extract word vectors from the VSM and combine them with either multiplication or addition (default is multiplication)\n","# Set multiply = False to use addition\n","# Default tokenizer is nltk word tokenizer. \n","# Remove stopwords and punctuations by default.\n","def get_text_vector(text, \n","                    VSM, # the VSM (a dictionary) used to derive word vectors\n","                    multiply = True,\n","                    tokenizer = nltk.word_tokenize,\n","                    remove_stopwords = True,\n","                    remove_punct = True):\n","  \n","  if remove_punct:\n","    text = text.translate(str.maketrans('','',string.punctuation))\n","  \n","  \n","  words = tokenizer(text)\n","\n","  if remove_stopwords:\n","    stop_words = nltk.corpus.stopwords.words('english')\n","    words = [w for w in words if not w in stop_words] \n","\n","    \n","  \n","  words = [w for w in words if w in VSM] \n","\n","  # Uncomment this for sanity check\n","  #print(len(words))\n","  \n","  v = VSM[words[0]]\n","  for word in words[1:]:\n","    if multiply:\n","      v= np.multiply(v, VSM[word])\n","    else:\n","      v = v+VSM[word]\n","\n","  return v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjP1rMTECkas","executionInfo":{"status":"ok","timestamp":1621584189834,"user_tz":420,"elapsed":36945,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}}},"source":["os.chdir('/content/gdrive/My Drive/personal/CS224U/cs224u-kf/')\n","#Because the folder name has a dash in it, we cannot use relative import.\n","# Here I just changed the working directory temporarily\n","import utils  \n","os.chdir('/content/gdrive/My Drive/personal/CS224U/project')\n","\n","\n","glove_dict = utils.glove2dict('/content/gdrive/MyDrive/personal/CS224U/cs224u-kf/data/glove.6B/glove.6B.300d.txt')"],"execution_count":19,"outputs":[]}]}