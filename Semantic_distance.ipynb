{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaWDRfwsg8VS"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1621583887592,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "MpUJ3wmbCl7y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "random.seed(1)\n",
    "import re\n",
    "import nltk\n",
    "import semdis\n",
    "\n",
    "\n",
    "# Set working directory.\n",
    "#os.chdir('/content/gdrive/My Drive/personal/CS224U/project')\n",
    "# os.listdir() # Uncomment to sanity check that you're in the right directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P4IWfPB5duIZ"
   },
   "outputs": [],
   "source": [
    "# Takes a dataframe and adds a 'wordcount' column (modifies original df)\n",
    "# Requirement: df has existing 'text' column\n",
    "# Returns modified df with 'wordcount' column added\n",
    "def add_wordcount_col(df):\n",
    "  assert 'text' in df\n",
    "  df['wordcount'] = df['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mIdac2q9elXi"
   },
   "outputs": [],
   "source": [
    "# Takes a dataframe and returns a new dataframe containing only rows where wordcount < max_words\n",
    "# Requirement: input df has existing 'wordcount' column\n",
    "def restrict_by_wordcount(df, max_words):\n",
    "  assert 'wordcount' in df\n",
    "  df_restricted = df.loc[df['wordcount'] <= max_words]\n",
    "  return df_restricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 36945,
     "status": "ok",
     "timestamp": 1621584189834,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "kjP1rMTECkas"
   },
   "outputs": [],
   "source": [
    "import utils  \n",
    "\n",
    "glove_dict = utils.glove2dict('glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV3MkfpFBAna"
   },
   "source": [
    "## Functions that Calculate Semantic Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZWAUD3t8C231"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import scipy.spatial.distance\n",
    "\n",
    "# Takes two sequence and create a composite vector for each sequence.\n",
    "# Return the cosine similarity between the two vectors.\n",
    "\n",
    "def get_distance_between_texts(text1, text2, VSM = glove_dict,\n",
    "                               multiply = True,\n",
    "                               tokenizer = nltk.word_tokenize,\n",
    "                               remove_stopwords = True,\n",
    "                               remove_punct = True):\n",
    "  \n",
    "  v1 = get_text_vector(text1, VSM, multiply, tokenizer, remove_stopwords),\n",
    "  v2 = get_text_vector(text2, VSM, multiply, tokenizer, remove_stopwords)\n",
    "\n",
    "  return scipy.spatial.distance.cosine(v1, v2)\n",
    "\n",
    "# Takes a sequence and a VSM. Return a composite vector that represents the sequence\n",
    "# Extract word vectors from the VSM and combine them with either multiplication or addition (default is multiplication)\n",
    "# Set multiply = False to use addition\n",
    "# Default tokenizer is nltk word tokenizer. \n",
    "# Remove stopwords and punctuations by default.\n",
    "\n",
    "## TODO: Trying weighted sum (e.g., IDF weighting)\n",
    "def get_text_vector(text, \n",
    "                    VSM, # the VSM (a dictionary) used to derive word vectors\n",
    "                    multiply = True,\n",
    "                    tokenizer = nltk.word_tokenize,\n",
    "                    remove_stopwords = True,\n",
    "                    remove_punct = True):\n",
    "  \n",
    "  if remove_punct:\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "  \n",
    "  \n",
    "  words = tokenizer(text)\n",
    "\n",
    "  if remove_stopwords:\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    words = [w for w in words if not w in stop_words] \n",
    "\n",
    "    \n",
    "  \n",
    "  words = [w for w in words if w in VSM] \n",
    "\n",
    "  # Uncomment this for sanity check\n",
    "  #print(len(words))\n",
    "  if len(words) > 0:\n",
    "    v = VSM[words[0]]\n",
    "    for word in words[1:]:\n",
    "      if multiply:\n",
    "        v= np.multiply(v, VSM[word])\n",
    "      else:\n",
    "        v = v+VSM[word]\n",
    "  else:\n",
    "    # If no word is found in the dictionary, return a random vector\n",
    "    v = np.random.rand(300)\n",
    "\n",
    "  return v\n",
    "\n",
    "#test the function\n",
    "get_distance_between_texts(\"test, text\", \"test text\", glove_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5940629763576282, 0.5940629763576282]\n",
      "0.3960419842384188\n",
      "0.5940629763576282\n",
      "0.7448144358708455\n"
     ]
    }
   ],
   "source": [
    "imp.reload(semdis)\n",
    "\n",
    "# take a sentence and return\n",
    "def distances_within_text(text,\n",
    "                          tokenizer = nltk.word_tokenize,\n",
    "                         remove_stopwords =True,\n",
    "                             remove_punct = True):\n",
    "    if remove_punct:\n",
    "        text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    words = tokenizer(text)\n",
    "    if remove_stopwords:\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        words = [w for w in words if not w in stop_words] \n",
    "    n = len(words)\n",
    "    distances = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if words[i] in glove_dict and words[j] in glove_dict:\n",
    "                distances.append(semdis.get_word_cosine(words[i], words[j], vsm = glove_dict))\n",
    "        else:\n",
    "            continue\n",
    "    return distances\n",
    "\n",
    "# test the function\n",
    "test_text= 'test, test. score'\n",
    "print(distances_within_sentence(test_text))\n",
    "\n",
    "# Take a sequence and a pooling function (e.g., max, min, average)\n",
    "# Calculate the semantic distances between all word pairs and pool them using the given function.\n",
    "def pool_distances_within_text(text, pool = np.average, **kwarg):\n",
    "    distances = distances_within_text(text, **kwarg)\n",
    "    if len(distances) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return pool(distances)\n",
    "\n",
    "# Test the function\n",
    "print(pool_distances_within_text(test_text))\n",
    "print(pool_distances_within_text(test_text, pool = max))\n",
    "print(pool_distances_within_text(test_text, pool = max, remove_punct = False))\n",
    "\n",
    "# Two-step pooling \n",
    "# Pool distances within each sentence at first using the function given for the \"sentence_pool\" argument\n",
    "# Pool these results using the function provided for the \"pool\" argument\n",
    "def pool_distances_split_sentence(text, pool = np.average, sentence_pool = np.average, **kwarg):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentence_pooled = [pool_distances_within_text(sentence, pool = sentence_pool, **kwarg) for sentence in sentences]\n",
    "    \n",
    "    # When there is only one word in a sentence, distances cannot be calculated and will return nan\n",
    "    # Drop nan using the following line\n",
    "    sentence_pooled = [x for x in sentence_pooled if x is not None]\n",
    "    return pool(sentence_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations of different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# This function can be used to examine whether a certain feature is correlated with a construct\n",
    "# It takes a datafree with a column 'text' and a column 'label'.\n",
    "# The argument \"apply_to_column\" regulates whether the function should be apply to the whole ['text'] column \n",
    "# or each element in the column, the default is element-wise operation.\n",
    "def correlate_feature_with_creativity(df, function, apply_to_column = False, spearman = False, length = False, **kwarg):\n",
    "    if apply_to_column:\n",
    "        feature = function(df['text'], **kwarg)\n",
    "    else:\n",
    "        feature = [function(x, **kwarg) for x in df['text']]\n",
    "    \n",
    "    if length:\n",
    "        add_wordcount_col(df)\n",
    "        y = df['wordcount']\n",
    "    else:\n",
    "        y = df['label']\n",
    "        \n",
    "    if spearman:\n",
    "        return scipy.stats.spearmanr(feature, y)\n",
    "    else:\n",
    "        return scipy.stats.pearsonr(feature, y)\n",
    "\n",
    "## This is not working\n",
    "# def correlate_feature_gridsearch(param_grid, **kwarg):\n",
    "#     grid = ParameterGrid(param_grid)\n",
    "#     results = {}\n",
    "\n",
    "#     for params in grid:\n",
    "#         print(params)\n",
    "#         # Index of the model, represents the parameters\n",
    "#         index = '; '.join(x + '_' + str(y) for x, y in params.items())\n",
    "        \n",
    "#         result = correlate_feature_with_creativity(params, **kwarg)\n",
    "#         print(result)\n",
    "#         results[index] = result\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool_func: <function average at 0x7fed06264b80>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-8f3b1bfeb2f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpool_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pool_func: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     result = correlate_feature_with_creativity(df = car_df,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                       \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_distances_within_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                       \u001b[0mapply_to_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-f6fc6537ef34>\u001b[0m in \u001b[0;36mcorrelate_feature_with_creativity\u001b[0;34m(df, function, apply_to_column, spearman, length, **kwarg)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-f6fc6537ef34>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-fa20c39d8c0a>\u001b[0m in \u001b[0;36mpool_distances_within_text\u001b[0;34m(text, pool, **kwarg)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Calculate the semantic distances between all word pairs and pool them using the given function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpool_distances_within_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances_within_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-fa20c39d8c0a>\u001b[0m in \u001b[0;36mdistances_within_text\u001b[0;34m(text, tokenizer, remove_stopwords, remove_punct)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglove_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemdis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word_cosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Stanford Coursework/Stanford 2021 Spring/CS224U/Github/cs224u-creativity/semdis.py\u001b[0m in \u001b[0;36mget_word_cosine\u001b[0;34m(word1, word2, vsm)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvsm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;31m# cosine distance is also referred to as 'uncentered correlation',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;31m#   or 'reflective correlation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mumu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0muv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m     \u001b[0muu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pool_dict = {'mean': np.average, 'max': max, 'min': min}\n",
    "\n",
    "for pool_func in pool_dict:\n",
    "    print(\"pool_func: \" + pool_func)\n",
    "    result = correlate_feature_with_creativity(df = car_df,\n",
    "                                      function = pool_distances_within_text,\n",
    "                                      apply_to_column = False,\n",
    "                                      spearman = True,\n",
    "                                      length = False,  \n",
    "                                     pool = pool_dict[pool_func])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum word cosine is positively correlated with creativity but also with length\n",
    "\n",
    "Creativity-maximum: SpearmanrResult(correlation=0.37452160762745074, pvalue=5.111302023283955e-21)\n",
    "\n",
    "Minimum word cosine is negatively correlated with creativity but also with length\n",
    "\n",
    "Creativity-minmum: SpearmanrResult(correlation=-0.37735349122002054, pvalue=2.4535790257204983e-21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool_func: mean; sentence_pool: mean\n",
      "SpearmanrResult(correlation=-0.2403277759735626, pvalue=3.592244833240831e-09)\n",
      "pool_func: mean; sentence_pool: max\n",
      "SpearmanrResult(correlation=-0.0038991285913811246, pvalue=0.9248327146046069)\n",
      "pool_func: mean; sentence_pool: min\n",
      "SpearmanrResult(correlation=-0.22093754052073925, pvalue=6.19485844271774e-08)\n",
      "pool_func: max; sentence_pool: mean\n",
      "SpearmanrResult(correlation=0.1723037472178696, pvalue=2.660790908762486e-05)\n",
      "pool_func: max; sentence_pool: max\n",
      "SpearmanrResult(correlation=0.42317239849931104, pvalue=6.010422081881892e-27)\n",
      "pool_func: max; sentence_pool: min\n",
      "SpearmanrResult(correlation=0.22874809208303887, pvalue=2.0280939769626615e-08)\n",
      "pool_func: min; sentence_pool: mean\n",
      "SpearmanrResult(correlation=-0.4774720620380414, pvalue=8.158835269475223e-35)\n",
      "pool_func: min; sentence_pool: max\n",
      "SpearmanrResult(correlation=-0.3832613528225597, pvalue=5.184492974439368e-22)\n",
      "pool_func: min; sentence_pool: min\n",
      "SpearmanrResult(correlation=-0.5166922258004382, pvalue=1.9190793496923896e-41)\n"
     ]
    }
   ],
   "source": [
    "pool_dict = {'mean': np.average, 'max': max, 'min': min}\n",
    "for pool_func in pool_dict:\n",
    "    for sentence_pool in pool_dict:\n",
    "        print(\"pool_func: \" + pool_func + \"; sentence_pool: \" + sentence_pool)\n",
    "        result = correlate_feature_with_creativity(df = car_df,\n",
    "                                          function = pool_distances_split_sentence,\n",
    "                                          apply_to_column = False,\n",
    "                                          spearman = True,\n",
    "                                          length = True,  \n",
    "                                         pool = pool_dict[pool_func],\n",
    "                                          sentence_pool = pool_dict[sentence_pool])\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pool_func: mean; sentence_pool: mean\n",
    "\n",
    "SpearmanrResult(correlation=-0.10681453757761777, pvalue=0.009541355224589723)\n",
    "\n",
    "pool_func: mean; sentence_pool: max\n",
    "\n",
    "SpearmanrResult(correlation=0.02830077354068258, pvalue=0.4933854586159333)\n",
    "\n",
    "pool_func: mean; sentence_pool: min\n",
    "\n",
    "SpearmanrResult(correlation=-0.13007923968097182, pvalue=0.001572383066756145)\n",
    "\n",
    "pool_func: max; sentence_pool: mean\n",
    "\n",
    "SpearmanrResult(correlation=0.1748646004971523, pvalue=2.0069593419130286e-05)\n",
    "\n",
    "pool_func: max; sentence_pool: max\n",
    "\n",
    "SpearmanrResult(correlation=0.3096731969519826, pvalue=1.555880582819005e-14)\n",
    "\n",
    "pool_func: max; sentence_pool: min\n",
    "\n",
    "SpearmanrResult(correlation=0.18085354217737354, pvalue=1.0215051334244021e-05)\n",
    "\n",
    "pool_func: min; sentence_pool: mean\n",
    "\n",
    "SpearmanrResult(correlation=-0.28291090999942947, pvalue=2.7703965033980604e-12)\n",
    "\n",
    "pool_func: min; sentence_pool: max\n",
    "\n",
    "SpearmanrResult(correlation=-0.22320713937402628, pvalue=4.497115355524009e-08)\n",
    "\n",
    "pool_func: min; sentence_pool: min\n",
    "\n",
    "SpearmanrResult(correlation=-0.3248636747910821, pvalue=6.42564834544976e-16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# document count based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def normalized_tfidf(df):\n",
    "    vec = CountVectorizer(tokenizer= nltk.word_tokenize,\n",
    "                          stop_words = {'english'})\n",
    "    dtf = vec.fit_transform(car_df['text']).toarray()\n",
    "\n",
    "    tfidf_vec = TfidfVectorizer(tokenizer= nltk.word_tokenize,\n",
    "                          stop_words = {'english'},\n",
    "                               use_idf=False)\n",
    "    tfidf = tfidf_vec.fit_transform(car_df['text']).toarray()\n",
    "    \n",
    "    normed_tfidf= tfidf.sum(axis = 1)/dtf.sum(axis = 1)\n",
    "    return normed_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6524408182006463, 1.363312275423059e-72)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scipy.stats.pearsonr(normed_tfidf, car_df['label'])\n",
    "#scipy.stats.pearsonr(normed_tfidf, car_df['wordcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.        , 0.        ],\n",
       "       [0.33333333, 0.5       , 0.        ],\n",
       "       [0.33333333, 0.5       , 1.        ]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "mtx = np.array([[1,0,0],\n",
    "              [1,1,0],\n",
    "               [1,1,1]])\n",
    "normalize(mtx, norm = 'l1', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dumas et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-0d9b7e3f3e40>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-0d9b7e3f3e40>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    prompt = \"self-driving car\") ==\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# A wrapper function that applies Dumas et al. method to the whole column of texts.\n",
    "def get_distances_for_df(responses, prompt, VSM = glove_dict, multiply = True):\n",
    "    return [get_distance_between_texts(prompt, x, VSM, multiply = multiply) for x in responses]\n",
    "\n",
    "import dataset\n",
    "fitness_df = dataset.get_data(1, 'Novelty_Combined', shuffle=True)\n",
    "car_df = dataset.get_data(2, 'Novelty_Combined', shuffle=True)\n",
    "\n",
    "# Measures based on Duma et al., 2020.\n",
    "correlate_feature_with_creativity(df = fitness_df,\n",
    "                                  function = get_distances_for_df,\n",
    "                                  apply_to_column = True,\n",
    "                                  spearman = True,\n",
    "                                  multiply = True,\n",
    "                                 prompt = \"fitness equipment\")\n",
    "\n",
    "\n",
    "# The following two functions are equivalent\n",
    "correlate_feature_with_creativity(df = car_df,\n",
    "                                  function = get_distances_for_df,\n",
    "                                  apply_to_column = True,\n",
    "                                  spearman = True,\n",
    "                                  multiply = True,\n",
    "                                 prompt = \"self-driving car\")\n",
    "correlate_feature_with_creativity(df = car_df,\n",
    "                                  function = get_distance_between_texts,\n",
    "                                  apply_to_column = False,\n",
    "                                  spearman = True,\n",
    "                                  multiply = True,\n",
    "                                 text2 = \"self-driving car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.1174380910023567, pvalue=0.004350859526027052)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are word counts correlated with word counts?\n",
    "\n",
    "feature = get_distances_for_responses(prompt = \"fitness equipment\",\n",
    "                             responses = car_df['text'],\n",
    "                             VSM = glove_dict, multiply = True)\n",
    "add_wordcount_col(car_df)\n",
    "scipy.stats.spearmanr(feature, car_df['wordcount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with Beaty & Johnson's finding. Additive composition generates semantic distances that are negatively correlated with word counts; multiplicative composition generates semantic distances that are positively correlated with word counts."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Semantic Distance.ipynb",
   "provenance": [
    {
     "file_id": "18xltzQFDw5Lsdhzi_4CHL8VuI0Nplu7V",
     "timestamp": 1621333562891
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
