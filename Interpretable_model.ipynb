{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of AI Creativity- Interpretable Model (CS224U).ipynb","provenance":[{"file_id":"1UlyrKZjxVZlzW8BaFL297AWZhXF0lfUd","timestamp":1620665218762},{"file_id":"1y-kGbvV44buBmpudtjfVPo9gbfxuxdyB","timestamp":1620552390338},{"file_id":"1QUimrSG8WslzcrrwkpnJGyootJzYpeVc","timestamp":1619061902856},{"file_id":"1JAJW5fuTrFLo1Ct_pX8mQzcD_4bJt7UB","timestamp":1617450069839}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KN0LjWcnT12","executionInfo":{"status":"ok","timestamp":1619964071524,"user_tz":420,"elapsed":23029,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"0bdfe3c7-f843-477f-cac5-3f4f70044531"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C9QkQoiuXuam"},"source":["\n","import os\n","\n","# set working directory\n","os.chdir('/content/gdrive/My Drive/Colab Notebooks')\n","#os.listdir()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POiqhxLUFkVE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619964198088,"user_tz":420,"elapsed":1637,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"2adc61dc-742c-491b-d4c9-0bf89cfa2e27"},"source":["import pandas as pd\n","import numpy as np\n","import vsm\n","import utils\n","import sst\n","from collections import Counter, defaultdict\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn import linear_model\n","from scipy.stats import spearmanr, pearsonr\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n","from torch_rnn_classifier import TorchRNNClassifier\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.decomposition import TruncatedSVD, SparsePCA\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.pipeline import Pipeline\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn\n","from sklearn.model_selection import GridSearchCV\n","\n","import random\n","random.seed(1)\n","\n","import re\n","\n","# Our dataset is an excel sheet with multiple sheets. \n","# Each sheet include ideas from one sample along with ratings on several metrics \n","#3 constructs (creativity, usefulness, novelty) * 3 types of judges (expert, consumers, combined)\n","# the following function allow us to extract a specific type of labels together with the ideas\n","def extract_metric(study, metric, length = 400):\n","  df0 = pd.read_excel(\"data/Idea Ratings_Berg_2019_OBHDP.xlsx\", sheet_name= study-1)\n","  df = df0[[\"Final_Idea\", metric]].rename(columns={'Final_Idea': 'sentence', metric: 'label'})\n","\n","  df = df.iloc[[isinstance(x, str) for x in df['sentence']]]\n","  return(df.iloc[[len(x.split())< length for x in df['sentence']]])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7bDByXkqYjXt"},"source":["# def get_wordnet_edges():\n","#     edges = defaultdict(set)\n","#     for ss in wn.all_synsets():\n","#         lem_names = {lem.name() for lem in ss.lemmas()}\n","#         for lem in lem_names:\n","#             edges[lem] |= lem_names\n","#     return edges\n","# wn_edges = get_wordnet_edges()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gS2o0MhS-vBW"},"source":["# def bigrams_phi(text):\n","#     \"\"\"\n","#     The basis for a bigrams feature function. Downcases all tokens.\n","\n","#     Parameters\n","#     ----------\n","#     text : str\n","#         The example to represent.\n","\n","#     Returns\n","#     -------\n","#     defaultdict\n","#         A map from tuples to their counts in `text`.\n","\n","#     \"\"\"\n","#     toks = text.lower().split()\n","#     left = [utils.START_SYMBOL] + toks\n","#     right = toks + [utils.END_SYMBOL]\n","#     grams = list(zip(left, right))\n","#     return Counter(grams)\n","# def unigrams_phi(text):\n","#   \"\"\"\n","#   The basis for a unigrams feature function. Downcases all tokens.\n","\n","#   Parameters\n","#   ----------\n","#   text : str\n","#       The example to represent.\n","\n","#   Returns\n","#   -------\n","#   defaultdict\n","#       A map from strings to their counts in `tree`. (Counter maps a\n","#       list to a dict of counts of the elements in that list.)\n","\n","#   \"\"\"\n","#   return Counter(text.lower().split())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZogGbTjpjkaT"},"source":["def get_token_counts(df):\n","  ##### YOUR CODE HERE\n","  nrow = df.shape[0]\n","  text = []\n","  for i in range(nrow):\n","      text.append(unigrams_phi(df['sentence'][i]))\n","  return(text)\n","  \n","def get_length(df):\n","  nrow = df.shape[0]\n","  lengths = []\n","  characters = []\n","  average_len = []\n","  for i in range(nrow):\n","      lengths.append(len(df['sentence'][i].split()))\n","      characters.append(len(df['sentence'][i]))\n","      average_len.append(len(df['sentence'][i])/len(df['sentence'][i].split()))\n","  df_len = pd.DataFrame({'lengths': lengths,\n","                        'characters': characters,\n","                        'average_len': average_len})\n","  return(df_len.to_numpy())\n","\n","def get_distance(word1, word2):\n","  v1 = glove_dict[word1],\n","  v2 = glove_dict[word2]\n","\n","  return(vsm.cosine(v1, v2))\n","\n","def get_vector_distance(text):\n","  words = nltk.word_tokenize(text)\n","  stop_words = nltk.corpus.stopwords.words('english')\n","  words = [w for w in words if not w in stop_words] \n","  n = len(words)\n","  distances = []\n","  for i in range(n):\n","    for j in range(i+1, n):\n","      if words[i] in glove_dict and words[j] in glove_dict:\n","        distances.append(get_distance(words[i], words[j]))\n","      else:\n","        continue\n","  return(distances)\n","\n","def get_df_distance(df):\n","  lst_distances = [get_vector_distance(text) for text in df['sentence']]\n","\n","  df_dis = pd.DataFrame({'average': [np.average(distances) for distances in lst_distances],\n","                    'max': [np.max(distances) for distances in lst_distances],\n","                    'min': [np.min(distances) for distances in lst_distances]})\n","  return(df_dis.to_numpy())\n","\n","\n","# def get_pooled_insentence_distance(text, pool = np.average):\n","#   sentences = nltk.sent_tokenize(text)\n","#   distances = [get_pooled_distance(sentence, pool) for sentence in sentences]\n","#   return(np.average(distances))\n","\n","def featurize(df_train, df_test, count = False, tfidf = False, length = True, distance = False):\n","\n","  features_train = []\n","  features_test = []\n","\n","  #Add features here\n","\n","  #Counts\n","  if count:\n","    # vec = DictVectorizer(sparse=False)\n","\n","    # features_train.append(vec.fit_transform(get_token_counts(df_train)))\n","    # features_test.append(vec.transform(get_token_counts(df_test)))# Not `fit_transform`!\n","    vec = CountVectorizer(tokenizer= nltk.word_tokenize)\n","    svd = TruncatedSVD(\n","      n_components=100,\n","      n_iter=10,\n","      random_state=42)\n","    features_train.append(vec.fit_transform(df_train['sentence']).toarray())\n","    features_test.append(vec.transform(df_test['sentence']).toarray())\n","    \n","  if distance:\n","    features_train.append(get_df_distance(df_train))\n","    features_test.append(get_df_distance(df_test))\n","\n","\n","  #Length\n","  if length:\n","    features_train.append(get_length(df_train))\n","    features_test.append(get_length(df_test))\n","\n","  #tfidf\n","  if tfidf:\n","    tfidf = TfidfVectorizer(sparse = False)\n","    features_train.append(tfidf.fit_transform(df_train['sentence']).toarray())\n","    features_test.append(tfidf.transform(df_test['sentence']).toarray())\n","    print('tfidf')\n","\n","\n","  X_train =np.column_stack(features_train)\n","  X_test =np.column_stack(features_test)\n","\n","  y_train = df_train['label']\n","  y_test = df_test['label']\n","\n","  return(X_train, X_test, y_train, y_test)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u0u852_7u4S2"},"source":["glove_dict = utils.glove2dict(\n","    os.path.join('data', 'glove.6B', 'glove.6B.300d.txt'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RY-36u1Jy5Hj"},"source":["# crt = pd.concat([extract_metric(1,\"Creativity_Combined\"),\n","#           extract_metric(2,\"Creativity_Combined\"),\n","#           extract_metric(3,\"Creativity_Combined\")]).reset_index()\n","crt = extract_metric(2,\"Usefulness_Combined\")\n","n = crt.shape[0]\n","index = np.random.randint(1,6,n)\n","\n","crt_train = crt[index < 5].reset_index()\n","crt_test = crt[index == 5].reset_index()\n","crt_sealed = crt[index > 5].reset_index()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qOGmv3x6k2U","executionInfo":{"status":"ok","timestamp":1619923507697,"user_tz":420,"elapsed":281,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"c6ea9296-a57a-4474-8413-526e86b3e93c"},"source":["#distances = get_df_distance(crt_test)\n","#pearsonr(distances[:200], crt_train['label'][range(200)])\n","print(spearmanr(distances[:,0], crt_test['label']))\n","lengs = get_length(crt_test)\n","print(pearsonr(distances[:, 0], lengs[:,0]))\n","# print(pearsonr(distances, lengs[:,1]))\n","# print(pearsonr(distances, lengs[:,2]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SpearmanrResult(correlation=-0.3837380601595334, pvalue=2.7211707494553795e-05)\n","(-0.1420501927276636, 0.13339522045882654)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8CKKse7iNtO","executionInfo":{"status":"ok","timestamp":1619868810705,"user_tz":420,"elapsed":67878,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"ce61bf78-4074-489b-af6f-22edfffcf55b"},"source":["pipeline = Pipeline([('vect', CountVectorizer()),\n","                 ('svd', TruncatedSVD()),\n","                 ('clf', linear_model.SGDRegressor())\n","                 #('MPL', MLPRegressor())\n","                 ])\n","parameters = {\n","  'vect__max_df': (0.5, 0.75, 1.0),\n","  'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n","  'clf__alpha': (1, 0.1, 0.001, 0.0001, 0),\n","  'clf__penalty': ('l1', 'l2', 'elasticnet'),\n","}\n","grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n","\n","grid_search.fit(crt_train['sentence'], crt_train['label'])\n","#pipe.score(crt_test['sentence'], crt_test['label'])\n","#pearsonr(pipe.predict(crt_test['sentence']), crt_test['label'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    7.6s\n","[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   29.7s\n","[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.1min\n","[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  1.1min finished\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=None, error_score=nan,\n","             estimator=Pipeline(memory=None,\n","                                steps=[('vect',\n","                                        CountVectorizer(analyzer='word',\n","                                                        binary=False,\n","                                                        decode_error='strict',\n","                                                        dtype=<class 'numpy.int64'>,\n","                                                        encoding='utf-8',\n","                                                        input='content',\n","                                                        lowercase=True,\n","                                                        max_df=1.0,\n","                                                        max_features=None,\n","                                                        min_df=1,\n","                                                        ngram_range=(1, 1),\n","                                                        preprocessor=None,\n","                                                        stop_words=None,\n","                                                        strip_accents=None,\n","                                                        token_pattern='(...\n","                                                     shuffle=True, tol=0.001,\n","                                                     validation_fraction=0.1,\n","                                                     verbose=0,\n","                                                     warm_start=False))],\n","                                verbose=False),\n","             iid='deprecated', n_jobs=-1,\n","             param_grid={'clf__alpha': (1, 0.1, 0.001, 0.0001, 0),\n","                         'clf__penalty': ('l1', 'l2', 'elasticnet'),\n","                         'vect__max_df': (0.5, 0.75, 1.0),\n","                         'vect__ngram_range': ((1, 1), (1, 2))},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring=None, verbose=1)"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d8A3-uxRtQAK","executionInfo":{"status":"ok","timestamp":1619869069448,"user_tz":420,"elapsed":1078,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"7afc4566-d023-44c6-f524-bbff6458916f"},"source":["pipe = Pipeline([('vect', CountVectorizer()),\n","                 ('svd', TruncatedSVD()),\n","                 ('clf', linear_model.SGDRegressor(penalty='elasticnet'))\n","                 #('MPL', MLPRegressor())\n","                 ])\n","pipe.fit(crt_train['sentence'], crt_train['label'])\n","pipe.score(crt_test['sentence'], crt_test['label'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.20330349194100927"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"w4HU9Dili4n8"},"source":["crt_train = pd.concat([extract_metric(1,\"Creativity_Combined\"),\n","          extract_metric(2,\"Creativity_Combined\")]).reset_index()\n","crt_test = extract_metric(3,\"Creativity_Combined\").reset_index()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TT0XlA1smnrB","executionInfo":{"status":"ok","timestamp":1619964404783,"user_tz":420,"elapsed":492,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"04cbcb85-d29c-4c53-c590-21aa16eac610"},"source":["X_train, X_test, y_train, y_test = featurize(crt_train, crt_test, count=True, length=False, distance=False)\n","\n","def fit_lasso_regressor(X, y):\n"," \n","    clf = linear_model.Lasso()\n","    clf.fit(X, y)\n","    return(clf)\n","mod = fit_lasso_regressor(X_train, y_train)\n","pred = mod.predict(X_test)\n","\n","print(pearsonr(crt_test['label'], pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(0.446630807574013, 1.656954503170394e-16)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RxbjqMTfTHA6","executionInfo":{"status":"ok","timestamp":1619862767796,"user_tz":420,"elapsed":814,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"b5fe6890-414d-4d71-a5de-a87941e4f853"},"source":["# Try to combine the output of two models\n","X_train, X_test, y_train, y_test = featurize(crt_train, crt_test, count=True, length=False)\n","\n","def fit_lasso_regressor(X, y):\n"," \n","    clf = linear_model.Lasso()\n","    clf.fit(X, y)\n","    return(clf)\n","mod = fit_lasso_regressor(X_train, y_train)\n","train1 =  mod.predict(X_train)\n","pred1 = mod.predict(X_test)\n","\n","X_train, X_test, y_train, y_test = featurize(crt_train, crt_test, count=False, length=True)\n","mod = fit_lasso_regressor(X_train, y_train)\n","train2 = mod.predict(X_train)\n","pred2 = mod.predict(X_test)\n","\n","lm = linear_model.LinearRegression()\n","\n","lm.fit(np.column_stack([train1, train2]), y_train)\n","pred = lm.predict(np.column_stack([pred1, pred2]))\n","\n","\n","print(spearmanr(crt_test['label'], pred))\n","print(pearsonr(crt_test['label'], pred1))\n","print(pearsonr(crt_test['label'], pred2))\n","\n","print(pearsonr(crt_test['label'], pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SpearmanrResult(correlation=0.6254302407310216, pvalue=7.74529646063656e-35)\n","(nan, nan)\n","(0.5910400095563775, 2.132523068771589e-30)\n","(0.5910400095563774, 2.1325230687716497e-30)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n","  warnings.warn(PearsonRConstantInputWarning())\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"EXaNxTWeM4Bn"},"source":["SST_HOME = os.path.join('data', 'sentiment')\n","train_dataset = sst.build_dataset(\n","    crt_train,\n","    phi=unigrams_phi,\n","    vectorizer=None)\n","vec.fit_transform(train_feats)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oytr5EduRDll"},"source":["train_dataset['y']"],"execution_count":null,"outputs":[]}]}