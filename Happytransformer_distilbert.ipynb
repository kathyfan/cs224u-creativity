{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of “AI Creativity- DistilBERT Regression (CS224U).ipynb","provenance":[{"file_id":"16RK2b2S6YcgCymNCv7pkFmBdo1kt8lUw","timestamp":1620665239436},{"file_id":"1QUimrSG8WslzcrrwkpnJGyootJzYpeVc","timestamp":1620521420105},{"file_id":"1JAJW5fuTrFLo1Ct_pX8mQzcD_4bJt7UB","timestamp":1617450069839}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ad24b337ef6d45ec87d56583b5ce2022":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ce56d6dcdc7a4bc6b82bfa8e6bd0a2fc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cc996479d79d468198b86b3f24aef0c5","IPY_MODEL_9cbf6c966e1b492eb569b222623d00b3"]}},"ce56d6dcdc7a4bc6b82bfa8e6bd0a2fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc996479d79d468198b86b3f24aef0c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8b24d06b65374448a34a1063f7327cba","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":442,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d97e75d942ad404fa90bb6f0aff9f865"}},"9cbf6c966e1b492eb569b222623d00b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_11b97b9330ba49678d6d6c6e348a7c4a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 442/442 [00:06&lt;00:00, 73.1B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7dbf92f78d7a448b85f1ea4b7989975f"}},"8b24d06b65374448a34a1063f7327cba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d97e75d942ad404fa90bb6f0aff9f865":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"11b97b9330ba49678d6d6c6e348a7c4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7dbf92f78d7a448b85f1ea4b7989975f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c2fac16625ba4623acc2b91ca6f03449":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_53c2533e39704333a270e0033ba5f784","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_987f4e0c48c14b14b998688e39732dd9","IPY_MODEL_592e2888f0514465a6f7c93890d6ca68"]}},"53c2533e39704333a270e0033ba5f784":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"987f4e0c48c14b14b998688e39732dd9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1ef2e50f5d124223af9980a6a09c5e51","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a7137605e8304416a9dd916247a25b28"}},"592e2888f0514465a6f7c93890d6ca68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5d9d8af1257048128a8ff81c6d620a43","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [00:05&lt;00:00, 49.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bbe854badba64ee484b19cb6af2a8904"}},"1ef2e50f5d124223af9980a6a09c5e51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a7137605e8304416a9dd916247a25b28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d9d8af1257048128a8ff81c6d620a43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bbe854badba64ee484b19cb6af2a8904":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"853da3d2f6e84b76a9d92c121927e237":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f5041095e8064d3b9c13200d6559cc83","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_260efece83a7485893f2733ae93c10eb","IPY_MODEL_665e6e8127c247888328e30bce932fb8"]}},"f5041095e8064d3b9c13200d6559cc83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"260efece83a7485893f2733ae93c10eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fcd00a2ec9de40e3bbde08fa9b541652","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_59e9a2b6ac3a49b5ae156b70705aa912"}},"665e6e8127c247888328e30bce932fb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e451aff0d77743db96511b11ea9ef7e2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 664kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_af851466d4424674b5a7aba94624568b"}},"fcd00a2ec9de40e3bbde08fa9b541652":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"59e9a2b6ac3a49b5ae156b70705aa912":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e451aff0d77743db96511b11ea9ef7e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"af851466d4424674b5a7aba94624568b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fcb84339434940e7b725090f9ea00553":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f62f7954fce3473989d58df1f3942544","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_eb5091bce59b4ab68d2ea7f74be3d9e3","IPY_MODEL_f03a29f0d6984d89bf58bbc67452e640"]}},"f62f7954fce3473989d58df1f3942544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eb5091bce59b4ab68d2ea7f74be3d9e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e413d94222a94acea68ebd264b7f9c56","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b9e8ae8f59f24b3a8abcc1494bed6147"}},"f03a29f0d6984d89bf58bbc67452e640":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e4b4b043944e4551bebf0b3f4d118a4a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 3.72MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78554ecd286c4548866da27e9da3e202"}},"e413d94222a94acea68ebd264b7f9c56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b9e8ae8f59f24b3a8abcc1494bed6147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4b4b043944e4551bebf0b3f4d118a4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"78554ecd286c4548866da27e9da3e202":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d68c73a62c94468a06ac7e5f6e64b86":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8cebb91f6d794d189db9076f44e311ca","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5bd1778d21f745d3bd10307131a7a9a9","IPY_MODEL_ca24414717294bf0abeeb2c9e5a143f8"]}},"8cebb91f6d794d189db9076f44e311ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5bd1778d21f745d3bd10307131a7a9a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d933a299de3e460c8b5138aff589e248","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e8c172ad3994f41bc7f57bcfd248782"}},"ca24414717294bf0abeeb2c9e5a143f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8cc8977e282a42f5b74d9b356c21efac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [01:46&lt;00:00, 3.80s/B]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_351ecd694adb492d9ce6741cfe932efa"}},"d933a299de3e460c8b5138aff589e248":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0e8c172ad3994f41bc7f57bcfd248782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8cc8977e282a42f5b74d9b356c21efac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"351ecd694adb492d9ce6741cfe932efa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KN0LjWcnT12","executionInfo":{"status":"ok","timestamp":1620537647266,"user_tz":420,"elapsed":22471,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"de962f9e-28d9-4393-892b-be7a2d62e3bd"},"source":["#Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wZH3RVm9SJJ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620537654775,"user_tz":420,"elapsed":29969,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"0ee938a7-6309-46c1-ae99-e0b61fb10993"},"source":["#Install the happytransformer module. \n","!pip install happytransformer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting happytransformer\n","  Downloading https://files.pythonhosted.org/packages/23/f9/4acd066452e5b543d8362b5e9a18a8e298989f18b24bfcc114a66fc40792/happytransformer-2.2.2-py3-none-any.whl\n","Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from happytransformer) (1.8.1+cu101)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from happytransformer) (3.12.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from happytransformer) (4.41.1)\n","Collecting datasets>=1.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n","\u001b[K     |████████████████████████████████| 225kB 7.9MB/s \n","\u001b[?25hCollecting transformers>=4.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 16.2MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 34.7MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->happytransformer) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->happytransformer) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->happytransformer) (56.1.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->happytransformer) (1.15.0)\n","Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (3.0.0)\n","Collecting fsspec\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n","\u001b[K     |████████████████████████████████| 112kB 51.9MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (1.1.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (0.3.3)\n","Collecting huggingface-hub<0.1.0\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Collecting xxhash\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n","\u001b[K     |████████████████████████████████| 245kB 49.4MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (3.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (20.9)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (2.23.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.6.0->happytransformer) (0.70.11.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 45.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.4.0->happytransformer) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 55.4MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2.8.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.6.0->happytransformer) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.6.0->happytransformer) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.4.0->happytransformer) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.4.0->happytransformer) (1.0.1)\n","Installing collected packages: fsspec, huggingface-hub, xxhash, datasets, sacremoses, tokenizers, transformers, sentencepiece, happytransformer\n","Successfully installed datasets-1.6.2 fsspec-2021.4.0 happytransformer-2.2.2 huggingface-hub-0.0.8 sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.5.1 xxhash-2.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C9QkQoiuXuam","executionInfo":{"status":"ok","timestamp":1620541379761,"user_tz":420,"elapsed":3719,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"405741f2-5bab-4780-a9d4-0f888280add8"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","from happytransformer import HappyTextClassification\n","import nltk \n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# DistilBertTokenizer is identical to BertTokenizer\n","from transformers import DistilBertModel, DistilBertTokenizer\n","import vsm\n","\n","# set working directory\n","os.chdir('/content/gdrive/My Drive/AI Creativity')\n","os.listdir()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[\"Justin's data\",\n"," 'K-Fold',\n"," 'runs',\n"," 'train_bin.csv',\n"," 'df_resid.csv',\n"," 'cross_domain_results.csv',\n"," 'rater_number_function.csv',\n"," 'rater_number_function.gsheet',\n"," 'cross_domain_results.gsheet',\n"," '2fake_prediction.csv',\n"," '3fake_prediction.csv',\n"," '4fake_prediction.csv',\n"," '5fake_prediction.csv',\n"," '1fake_prediction.csv',\n"," 'tmp.csv',\n"," 'cs224u',\n"," '1homo_prediction.csv',\n"," '1scrambled_prediction.csv',\n"," '1regression_prediction.csv',\n"," '2homo_prediction.csv',\n"," '2regression_prediction.csv',\n"," '2scrambled_prediction.csv',\n"," '3homo_prediction.csv',\n"," '3scrambled_prediction.csv',\n"," '3regression_prediction.csv',\n"," '4homo_prediction.csv',\n"," '4regression_prediction.csv',\n"," '4scrambled_prediction.csv',\n"," '5scrambled_prediction.csv',\n"," '5regression_prediction.csv',\n"," '5homo_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split1_scrambled_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split1homo_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split1normal_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split2_scrambled_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split2homo_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split2normal_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split3_scrambled_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split3homo_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split3normal_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split4_scrambled_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split4homo_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split4normal_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split5_scrambled_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split5homo_prediction.csv',\n"," 'Cross_Domain_Creativity_Combined_split5normal_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split1_scrambled_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split1homo_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split1normal_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split2_scrambled_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split2homo_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split2normal_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split3_scrambled_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split3homo_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split3normal_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split4_scrambled_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split4homo_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split4normal_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split5_scrambled_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split5homo_prediction.csv',\n"," 'Cross_Domain_Novelty_Combined_split5normal_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split1_scrambled_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split1homo_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split1normal_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split2_scrambled_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split2homo_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split2normal_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split3_scrambled_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split3homo_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split3normal_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split4_scrambled_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split4homo_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split4normal_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split5_scrambled_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split5homo_prediction.csv',\n"," 'Cross_Domain_Usefulness_Combined_split5normal_prediction.csv',\n"," 'Domain_general_results(0503).csv',\n"," 'Domain_specific_results(0503).csv',\n"," 'Domain_specific_study1_Creativity_Combined_split1_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split1homo_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split1normal_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split2_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split2homo_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split2normal_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split3_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split3normal_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split3homo_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split4homo_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split4normal_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split4_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split5_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split5homo_prediction.csv',\n"," 'Domain_specific_study1_Creativity_Combined_split5normal_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split1_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split1normal_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split1homo_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split2_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split2homo_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split2normal_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split3_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split3homo_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split3normal_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split4normal_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split4_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split4homo_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split5_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split5homo_prediction.csv',\n"," 'Domain_specific_study2_Creativity_Combined_split5normal_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split1_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split1homo_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split1normal_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split2_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split2homo_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split2normal_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split3_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split3homo_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split3normal_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split4_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split4homo_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split4normal_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split5_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split5normal_prediction.csv',\n"," 'Domain_specific_study3_Creativity_Combined_split5homo_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split1_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split1homo_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split1normal_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split2homo_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split2_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split2normal_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split3homo_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split3_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split3normal_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split4_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split4homo_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split4normal_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split5_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split5homo_prediction.csv',\n"," 'Domain_specific_study1_Novelty_Combined_split5normal_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split1_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split1homo_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split1normal_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split2_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split2homo_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split2normal_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split3_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split3homo_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split3normal_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split4_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split4homo_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split4normal_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split5_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split5normal_prediction.csv',\n"," 'Domain_specific_study2_Novelty_Combined_split5homo_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split1_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split1homo_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split1normal_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split2_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split2homo_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split2normal_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split3_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split3homo_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split3normal_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split4_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split4homo_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split4normal_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split5_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split5homo_prediction.csv',\n"," 'Domain_specific_study3_Novelty_Combined_split5normal_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split1_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split1homo_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split1normal_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split2_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split2homo_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split2normal_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split3homo_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split3_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split3normal_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split4_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split4homo_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split4normal_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split5_scrambled_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split5normal_prediction.csv',\n"," 'Domain_specific_study1_Usefulness_Combined_split5homo_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split1_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split1normal_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split1homo_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split2_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split2homo_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split2normal_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split3_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split3homo_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split3normal_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split4_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split4homo_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split4normal_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split5_scrambled_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split5homo_prediction.csv',\n"," 'Domain_specific_study2_Usefulness_Combined_split5normal_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split1_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split1normal_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split1homo_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split2_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split2homo_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split2normal_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split3_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split3homo_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split3normal_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split4_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split4normal_prediction.csv',\n"," 'train_file.csv',\n"," 'test_file.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split4homo_prediction.csv',\n"," 'homo.csv',\n"," 'scrambled.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split5_scrambled_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split5homo_prediction.csv',\n"," 'Domain_specific_study3_Usefulness_Combined_split5normal_prediction.csv',\n"," 'CS224U Lit Review.gdoc']"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"POiqhxLUFkVE"},"source":["import random\n","random.seed(1)\n","import re\n","\n","\n","# Our dataset is an excel sheet with multiple sheets. \n","# Each sheet include ideas from one sample along with ratings on several metrics \n","#3 constructs (creativity, usefulness, novelty) * 3 types of judges (expert, consumers, combined)\n","# the following function allow us to extract a specific type of labels together with the ideas\n","# For unknown reasons, I received an error when using BERT-like models to process a too long sequence (Perhaps because of the limit of 512 tokens of BERT)\n","# The \"length\" argument allows us to keep ideas with less than a certain number of words. The default is 400, which works fine.\n","def extract_metric(study, metric, length = 400):\n","  df0 = pd.read_excel(\"Justin's data/Idea Ratings_Berg_2019_OBHDP.xlsx\", sheet_name= study-1)\n","  df = df0[[\"Final_Idea\", metric]].rename(columns={'Final_Idea': 'text', metric: 'label'})\n","\n","  return(df.iloc[[len(x.split())< length for x in df['text']]])\n","\n","# The \"scramble\" function is for feature masking. It scrambles a sequence by randomizing the order of words\n","def scramble(text):\n","  words = text.split()\n","  n = len(words)\n","  scrambled = random.sample(words, n)\n","  return(\" \".join(scrambled))\n","\n","# word_rep and homogenize are used for feature masking. The \"homogenize\" function replace words \n","# with an arbitrarily chose word with the same length or similar length.\n","\n","def word_rep(word):\n","  if len(word) == 1:\n","    word = \"a\"\n","  if len(word) == 2:\n","    word = \"an\"\n","  if len(word) == 3:\n","    word = \"and\"\n","  if len(word) == 4:\n","    word = \"andy\"\n","  if len(word) == 5:\n","    word = \"antic\"\n","  if len(word) == 6:\n","    word = \"accent\"\n","  if len(word) == 7:\n","    word = \"ancient\"\n","  if len(word) == 8:\n","    word = \"accident\"\n","  if len(word) == 9:\n","    word = \"accidents\"\n","  if len(word) == 10:\n","    word = \"accidental\"\n","  if len(word) >= 11:\n","    word = \"accidentally\"\n","  return(word)\n","def homogenize(text):\n","  words = text.split()\n","  homogenized = [word_rep(word) for word in words]\n","  return(\" \".join(homogenized))\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":759,"referenced_widgets":["ad24b337ef6d45ec87d56583b5ce2022","ce56d6dcdc7a4bc6b82bfa8e6bd0a2fc","cc996479d79d468198b86b3f24aef0c5","9cbf6c966e1b492eb569b222623d00b3","8b24d06b65374448a34a1063f7327cba","d97e75d942ad404fa90bb6f0aff9f865","11b97b9330ba49678d6d6c6e348a7c4a","7dbf92f78d7a448b85f1ea4b7989975f","c2fac16625ba4623acc2b91ca6f03449","53c2533e39704333a270e0033ba5f784","987f4e0c48c14b14b998688e39732dd9","592e2888f0514465a6f7c93890d6ca68","1ef2e50f5d124223af9980a6a09c5e51","a7137605e8304416a9dd916247a25b28","5d9d8af1257048128a8ff81c6d620a43","bbe854badba64ee484b19cb6af2a8904","853da3d2f6e84b76a9d92c121927e237","f5041095e8064d3b9c13200d6559cc83","260efece83a7485893f2733ae93c10eb","665e6e8127c247888328e30bce932fb8","fcd00a2ec9de40e3bbde08fa9b541652","59e9a2b6ac3a49b5ae156b70705aa912","e451aff0d77743db96511b11ea9ef7e2","af851466d4424674b5a7aba94624568b","fcb84339434940e7b725090f9ea00553","f62f7954fce3473989d58df1f3942544","eb5091bce59b4ab68d2ea7f74be3d9e3","f03a29f0d6984d89bf58bbc67452e640","e413d94222a94acea68ebd264b7f9c56","b9e8ae8f59f24b3a8abcc1494bed6147","e4b4b043944e4551bebf0b3f4d118a4a","78554ecd286c4548866da27e9da3e202","0d68c73a62c94468a06ac7e5f6e64b86","8cebb91f6d794d189db9076f44e311ca","5bd1778d21f745d3bd10307131a7a9a9","ca24414717294bf0abeeb2c9e5a143f8","d933a299de3e460c8b5138aff589e248","0e8c172ad3994f41bc7f57bcfd248782","8cc8977e282a42f5b74d9b356c21efac","351ecd694adb492d9ce6741cfe932efa"]},"id":"8xnJOvNcrhc0","executionInfo":{"status":"error","timestamp":1620537680379,"user_tz":420,"elapsed":55560,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"42aaab87-7ca5-4be2-fa61-b925b60d3b4c"},"source":["# Domain General Predictions\n","\n","for metric in [\"Creativity_Combined\", \"Novelty_Combined\", \"Usefulness_Combined\"]: # loop through the three constructs\n","  \n","  # In domain general predictions, we combine the three samples as our corpus\n","  train_file= pd.concat([extract_metric(1,metric),\n","            extract_metric(2,metric),\n","            extract_metric(3,metric)]).reset_index()\n","\n","  # Roughly split the dataset into 5 subsets for cross-validation. \n","  # I believe there are more efficient way to do this, but this is how I often do in R.\n","  n = train_file.shape[0]\n","  index = np.random.randint(1,6,n)\n","  for i in range(1,6):\n","    #Split the dataset into train set and test set.\n","    train_set = train_file[index != i]\n","    # The happytransformer function takes filenames as input. \n","    # Therefore, I write the train set and the test set into csv files\n","    train_set.to_csv(\"train_file.csv\")\n","    test_set = train_file[index == i]\n","    test_set.to_csv(\"test_file.csv\")\n","\n","    #Instantiate a BERT-like model.\n","    # Set num_labels = 1 to make the model run regressions\n","    model = HappyTextClassification(\n","        #\"ROBERTA\", \"roberta-base\",\n","        \"DISTILBERT\", \"distilbert-base-uncased\",\n","        #\"XLNET\", \"xlnet-base-cased\",\n","        #\"ALBERT\", \"albert-base-v2\",\n","                                  num_labels=1)\n","    model.train(\"train_file.csv\")\n","\n","    # create a copy of the test set and scramble it\n","    scrambled = test_set.copy()\n","    scrambled['text'] = [scramble(x) for x in test_set.text]\n","    scrambled.to_csv(\"scrambled.csv\")\n","\n","    # Create a copy of the test set and homogenize it\n","    homo = test_set.copy()\n","    homo['text'] = [homogenize(x) for x in test_set.text]\n","    homo.to_csv(\"homo.csv\")\n","\n","    # the test method of happytransformer models return predicted labels and scores.\n","    # Because we set the num_labels = 1, only the scores matter\n","    pred = model.test(\"test_file.csv\")\n","    scrambled_pred = model.test(\"scrambled.csv\")\n","    homo_pred = model.test(\"homo.csv\")\n","\n","    # Extract the scores\n","    pred_score = [x.score for x in pred]\n","    scrambled_pred_score = [x.score for x in scrambled_pred]\n","    homo_pred_score = [x.score for x in homo_pred]\n","\n","    # This chunk of code might be totally redundant. \n","    df_pred = test_set.copy()\n","    df_scrambled = scrambled.copy()\n","    df_homo = homo.copy()\n","\n","    # Replace the original labels with the predicted scores and write the results into csv files\n","    #(perhaps I should have created new columns for the predicted scores)\n","    df_scrambled['label'] = scrambled_pred_score\n","    df_scrambled.to_csv(\"Cross_Domain_\" + metric + \"_split\" + str(i) + '_scrambled_prediction.csv')\n","    df_homo['label'] = homo_pred_score\n","    df_homo.to_csv(\"Cross_Domain_\" + metric + \"_split\" + str(i) + '_homo_prediction.csv')\n","    df_pred['label'] = pred_score\n","    df_pred.to_csv(\"Cross_Domain_\" + metric + \"_split\" + str(i) + '_normal_prediction.csv')\n","\n","    # Print pearson correlation for each iteration. Not important\n","    print(\"normal: \"+ str(np.corrcoef(pred_score, test_set['label'])))\n","    print(\"scrambled: \"+ str(np.corrcoef(scrambled_pred_score, test_set['label'])))\n","    print(\"homo: \"+ str(np.corrcoef(homo_pred_score, test_set['label'])))\n","\n","\n","#IMPORTANT NOTE: the original happytransformer model only takes labels of integer values, which will lead to the following error:\n","# invalid literal for int() with base 10: '5.7'\n","# To fix this, you can open the following file (it will show up in the error message in colabratory):\n","#/usr/local/lib/python3.7/dist-packages/happytransformer/tc/trainer.py \n","# and replace the following code with my code\n","# Original code\n","        # contexts = []\n","        # labels = []\n","        # with open(filepath, newline='') as csv_file:\n","        #     reader = csv.DictReader(csv_file)\n","        #     for row in reader:\n","        #         contexts.append(row['text'])\n","        #         if not test_data:\n","        #             labels.append(int(row['label']))\n","        # csv_file.close()\n","#My code\n","        # import pandas as pd\n","        # df= pd.read_csv(filepath)\n","        # contexts = list(df['text'])\n","        # labels = list(df['label'])\n","\n","# After the replacement, rerun the notebook without installing the happytransformer module. It should work not\n","# If you import the module from a local directory, you only need to do this once. \n","# However, in colaboratory, I have to repeat this step every time."],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad24b337ef6d45ec87d56583b5ce2022","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2fac16625ba4623acc2b91ca6f03449","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"853da3d2f6e84b76a9d92c121927e237","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fcb84339434940e7b725090f9ea00553","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d68c73a62c94468a06ac7e5f6e64b86","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["05/09/2021 05:21:14 - INFO - happytransformer.happy_transformer -   Using model: cuda\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["05/09/2021 05:21:20 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-9aa97b3d0c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#\"ALBERT\", \"albert-base-v2\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                   num_labels=1)\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_file.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# create a copy of the test set and scramble it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/happytransformer/happy_text_classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_filepath, args)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid args type. Use a TCTrainArgs object or a dictionary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_filepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataclass_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod_dataclass_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTCEvalArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mEvalResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/happytransformer/tc/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_filepath, dataclass_args)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdataclass_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_preprocessed_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessing dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/happytransformer/tc/trainer.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(filepath, test_data)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mcsv_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '5.7'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"NRYZ2H844wZI","executionInfo":{"status":"error","timestamp":1620407439182,"user_tz":420,"elapsed":2508,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"95f93ad6-2de6-47ee-ed3a-5010ce1cec46"},"source":["general_results = {}\n","\n","for metric in [\"Creativity_Combined\", \"Novelty_Combined\", \"Usefulness_Combined\"]:\n","  for pred_type in ['normal', \"_scrambled\", \"homo\"]:\n","    #combine all predicted scores to get the predictions of the whole dataset\n","    all_filenames = [\"Cross_Domain_\" + metric + \"_split\" + str(i)+ pred_type +\"_prediction.csv\" for i in range(1,6)]\n","    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames]) \n","\n","    # Calculate the Pearson correlation between the predicted scores and human ratings\n","    general_results[metric+ '_' + pred_type] = (np.corrcoef(combined_csv.sort_values(by='Unnamed: 0')['label'], train_file['label'])[0,1])\n","\n","# Write out the results into csv files. I use these files to make plots in R\n","pd.DataFrame.from_dict({'key':general_results.keys(), 'value': general_results.values()}).to_csv(\"Domain_general_results(0503).csv\")\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-baa44e0bb893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcombined_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_filenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mgeneral_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpred_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgeneral_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgeneral_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Domain_general_results(0503).csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_file' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"obTZ4F70nhO3","executionInfo":{"status":"ok","timestamp":1620543732168,"user_tz":420,"elapsed":2199004,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"df056ee7-89a1-4da7-dc85-e8741c826269"},"source":["# Domain Specific Predictions\n","for metric in [\"Creativity_Combined\", \"Novelty_Combined\", \"Usefulness_Combined\"]:\n","  for study in [1,2,3]:\n","    \n","    # Everything is the same as in domain general predictions, except that each sample is used separately here\n","    train_file= extract_metric(study,metric).sample(n = 250).reset_index()\n","    n = train_file.shape[0]\n","    index = np.random.randint(1,6,n)\n","    for i in range(1,6):\n","      train_set = train_file[index != i]\n","      train_set.to_csv(\"train_file.csv\")\n","      test_set = train_file[index == i]\n","      test_set.to_csv(\"test_file.csv\")\n","\n","      model = HappyTextClassification(\n","          #\"ROBERTA\", \"roberta-base\",\n","          \"DISTILBERT\", \"distilbert-base-uncased\",\n","          #\"XLNET\", \"xlnet-base-cased\",\n","          #\"ALBERT\", \"albert-base-v2\",\n","                                    num_labels=1)\n","      model.train(\"train_file.csv\")\n","\n","      scrambled = test_set.copy()\n","      homo = test_set.copy()\n","\n","      scrambled['text'] = [scramble(x) for x in test_set.text]\n","      homo['text'] = [homogenize(x) for x in test_set.text]\n","\n","      scrambled.to_csv(\"scrambled.csv\")\n","      homo.to_csv(\"homo.csv\")\n","\n","      pred = model.test(\"test_file.csv\")\n","      scrambled_pred = model.test(\"scrambled.csv\")\n","      homo_pred = model.test(\"homo.csv\")\n","\n","      pred_score = [x.score for x in pred]\n","      scrambled_pred_score = [x.score for x in scrambled_pred]\n","      homo_pred_score = [x.score for x in homo_pred]\n","\n","      df_pred = test_set.copy()\n","      df_scrambled = scrambled.copy()\n","      df_homo = homo.copy()\n","\n","      df_scrambled['label'] = scrambled_pred_score\n","      df_scrambled.to_csv(\"Domain_specific_study\" + str(study) + \"_\" + metric + \"_split\" + str(i) + '_scrambled_prediction.csv')\n","      df_homo['label'] = homo_pred_score\n","      df_homo.to_csv(\"Domain_specific_study\" + str(study) + \"_\" + metric + \"_split\" + str(i) + 'homo_prediction.csv')\n","      df_pred['label'] = pred_score\n","      df_pred.to_csv(\"Domain_specific_study\" + str(study) + \"_\" + metric + \"_split\" + str(i) + 'normal_prediction.csv')\n","\n","      print(\"normal: \"+ str(np.corrcoef(pred_score, test_set['label'])))\n","      print(\"scrambled: \"+ str(np.corrcoef(scrambled_pred_score, test_set['label'])))\n","      print(\"homo: \"+ str(np.corrcoef(homo_pred_score, test_set['label'])))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:25:35 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:25:35 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [597/597 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.693900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 51/51 [00:00<00:00, 181.03it/s]\n","100%|██████████| 51/51 [00:00<00:00, 182.56it/s]\n","100%|██████████| 51/51 [00:00<00:00, 182.80it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.65939717]\n"," [0.65939717 1.        ]]\n","scrambled: [[1.         0.60815799]\n"," [0.60815799 1.        ]]\n","homo: [[1.         0.42781806]\n"," [0.42781806 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:26:25 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:26:26 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [597/597 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.786500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 51/51 [00:00<00:00, 184.59it/s]\n","100%|██████████| 51/51 [00:00<00:00, 187.85it/s]\n","100%|██████████| 51/51 [00:00<00:00, 184.30it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.60429122]\n"," [0.60429122 1.        ]]\n","scrambled: [[1.         0.55783905]\n"," [0.55783905 1.        ]]\n","homo: [[1.         0.51057262]\n"," [0.51057262 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:27:14 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:27:14 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='633' max='633' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [633/633 00:46, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.794000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 39/39 [00:00<00:00, 163.60it/s]\n","100%|██████████| 39/39 [00:00<00:00, 173.11it/s]\n","100%|██████████| 39/39 [00:00<00:00, 176.62it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.63636385]\n"," [0.63636385 1.        ]]\n","scrambled: [[1.         0.70595816]\n"," [0.70595816 1.        ]]\n","homo: [[1.         0.61273466]\n"," [0.61273466 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:28:06 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:28:06 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='591' max='591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [591/591 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.622100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 53/53 [00:00<00:00, 179.56it/s]\n","100%|██████████| 53/53 [00:00<00:00, 184.80it/s]\n","100%|██████████| 53/53 [00:00<00:00, 185.80it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.70457694]\n"," [0.70457694 1.        ]]\n","scrambled: [[1.         0.62197494]\n"," [0.62197494 1.        ]]\n","homo: [[1.         0.42707769]\n"," [0.42707769 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:28:55 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:28:55 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='582' max='582' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [582/582 00:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.673600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 56/56 [00:00<00:00, 185.25it/s]\n","100%|██████████| 56/56 [00:00<00:00, 180.18it/s]\n","100%|██████████| 56/56 [00:00<00:00, 181.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.65180841]\n"," [0.65180841 1.        ]]\n","scrambled: [[1.        0.5654625]\n"," [0.5654625 1.       ]]\n","homo: [[1.         0.53097318]\n"," [0.53097318 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:29:42 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:29:42 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='603' max='603' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [603/603 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.668400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 49/49 [00:00<00:00, 177.71it/s]\n","100%|██████████| 49/49 [00:00<00:00, 179.09it/s]\n","100%|██████████| 49/49 [00:00<00:00, 181.36it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.67433567]\n"," [0.67433567 1.        ]]\n","scrambled: [[1.         0.67645629]\n"," [0.67645629 1.        ]]\n","homo: [[1.         0.49190509]\n"," [0.49190509 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:30:29 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:30:29 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [600/600 00:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.566000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 50/50 [00:00<00:00, 183.44it/s]\n","100%|██████████| 50/50 [00:00<00:00, 181.29it/s]\n","100%|██████████| 50/50 [00:00<00:00, 184.99it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.75878157]\n"," [0.75878157 1.        ]]\n","scrambled: [[1.         0.83278634]\n"," [0.83278634 1.        ]]\n","homo: [[1.         0.66498987]\n"," [0.66498987 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:31:17 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:31:17 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='615' max='615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [615/615 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.801200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 45/45 [00:00<00:00, 162.32it/s]\n","100%|██████████| 45/45 [00:00<00:00, 167.19it/s]\n","100%|██████████| 45/45 [00:00<00:00, 168.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.72030988]\n"," [0.72030988 1.        ]]\n","scrambled: [[1.         0.78149205]\n"," [0.78149205 1.        ]]\n","homo: [[1.         0.69868257]\n"," [0.69868257 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:32:05 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:32:05 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='621' max='621' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [621/621 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.651700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 43/43 [00:00<00:00, 176.83it/s]\n","100%|██████████| 43/43 [00:00<00:00, 182.53it/s]\n","100%|██████████| 43/43 [00:00<00:00, 187.12it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.61834307]\n"," [0.61834307 1.        ]]\n","scrambled: [[1.         0.64504839]\n"," [0.64504839 1.        ]]\n","homo: [[1.         0.54403266]\n"," [0.54403266 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:32:54 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:32:54 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='561' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [561/561 00:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.616500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 63/63 [00:00<00:00, 172.62it/s]\n","100%|██████████| 63/63 [00:00<00:00, 177.65it/s]\n","100%|██████████| 63/63 [00:00<00:00, 180.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.73787995]\n"," [0.73787995 1.        ]]\n","scrambled: [[1.         0.71463069]\n"," [0.71463069 1.        ]]\n","homo: [[1.         0.46675031]\n"," [0.46675031 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:33:40 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:33:40 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='603' max='603' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [603/603 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.801300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 49/49 [00:00<00:00, 148.85it/s]\n","100%|██████████| 49/49 [00:00<00:00, 152.11it/s]\n","100%|██████████| 49/49 [00:00<00:00, 154.36it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.69486574]\n"," [0.69486574 1.        ]]\n","scrambled: [[1.         0.63278657]\n"," [0.63278657 1.        ]]\n","homo: [[1.         0.48577868]\n"," [0.48577868 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:34:28 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:34:28 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='591' max='591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [591/591 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.629400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 53/53 [00:00<00:00, 155.44it/s]\n","100%|██████████| 53/53 [00:00<00:00, 157.97it/s]\n","100%|██████████| 53/53 [00:00<00:00, 159.72it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.72845879]\n"," [0.72845879 1.        ]]\n","scrambled: [[1.         0.72742726]\n"," [0.72742726 1.        ]]\n","homo: [[1.         0.31027049]\n"," [0.31027049 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:35:17 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:35:17 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [630/630 00:47, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.764400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 40/40 [00:00<00:00, 152.80it/s]\n","100%|██████████| 40/40 [00:00<00:00, 153.69it/s]\n","100%|██████████| 40/40 [00:00<00:00, 155.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.72605949]\n"," [0.72605949 1.        ]]\n","scrambled: [[1.         0.76535404]\n"," [0.76535404 1.        ]]\n","homo: [[1.         0.49594425]\n"," [0.49594425 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:36:10 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:36:10 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [597/597 00:45, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.749600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 51/51 [00:00<00:00, 156.36it/s]\n","100%|██████████| 51/51 [00:00<00:00, 158.90it/s]\n","100%|██████████| 51/51 [00:00<00:00, 158.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.        0.6088477]\n"," [0.6088477 1.       ]]\n","scrambled: [[1.         0.59649526]\n"," [0.59649526 1.        ]]\n","homo: [[1.         0.39128103]\n"," [0.39128103 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:36:59 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:36:59 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='579' max='579' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [579/579 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.711400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 57/57 [00:00<00:00, 163.83it/s]\n","100%|██████████| 57/57 [00:00<00:00, 163.10it/s]\n","100%|██████████| 57/57 [00:00<00:00, 162.80it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.76425177]\n"," [0.76425177 1.        ]]\n","scrambled: [[1.         0.74091669]\n"," [0.74091669 1.        ]]\n","homo: [[1.        0.5850197]\n"," [0.5850197 1.       ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:37:49 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:37:49 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [597/597 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.841300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 51/51 [00:00<00:00, 183.35it/s]\n","100%|██████████| 51/51 [00:00<00:00, 179.81it/s]\n","100%|██████████| 51/51 [00:00<00:00, 181.24it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.51477257]\n"," [0.51477257 1.        ]]\n","scrambled: [[1.         0.47605194]\n"," [0.47605194 1.        ]]\n","homo: [[1.         0.36019331]\n"," [0.36019331 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:38:37 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:38:37 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [597/597 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.857000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 51/51 [00:00<00:00, 181.83it/s]\n","100%|██████████| 51/51 [00:00<00:00, 187.16it/s]\n","100%|██████████| 51/51 [00:00<00:00, 190.42it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.54065429]\n"," [0.54065429 1.        ]]\n","scrambled: [[1.         0.56554579]\n"," [0.56554579 1.        ]]\n","homo: [[1.         0.48625087]\n"," [0.48625087 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:39:27 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:39:27 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='633' max='633' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [633/633 00:47, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.908900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 39/39 [00:00<00:00, 176.44it/s]\n","100%|██████████| 39/39 [00:00<00:00, 174.48it/s]\n","100%|██████████| 39/39 [00:00<00:00, 172.52it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.64823238]\n"," [0.64823238 1.        ]]\n","scrambled: [[1.        0.6821186]\n"," [0.6821186 1.       ]]\n","homo: [[1.         0.64180102]\n"," [0.64180102 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:40:18 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:40:18 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='591' max='591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [591/591 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.684100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 53/53 [00:00<00:00, 179.20it/s]\n","100%|██████████| 53/53 [00:00<00:00, 182.62it/s]\n","100%|██████████| 53/53 [00:00<00:00, 182.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.       0.672325]\n"," [0.672325 1.      ]]\n","scrambled: [[1.         0.59414018]\n"," [0.59414018 1.        ]]\n","homo: [[1.        0.4333615]\n"," [0.4333615 1.       ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:41:07 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:41:07 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='582' max='582' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [582/582 00:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.811600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 56/56 [00:00<00:00, 177.52it/s]\n","100%|██████████| 56/56 [00:00<00:00, 178.77it/s]\n","100%|██████████| 56/56 [00:00<00:00, 184.00it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.        0.5739617]\n"," [0.5739617 1.       ]]\n","scrambled: [[1.         0.50150462]\n"," [0.50150462 1.        ]]\n","homo: [[1.       0.486693]\n"," [0.486693 1.      ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:41:54 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:41:54 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='603' max='603' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [603/603 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.812000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 49/49 [00:00<00:00, 177.23it/s]\n","100%|██████████| 49/49 [00:00<00:00, 174.52it/s]\n","100%|██████████| 49/49 [00:00<00:00, 184.50it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.47899381]\n"," [0.47899381 1.        ]]\n","scrambled: [[1.         0.55149911]\n"," [0.55149911 1.        ]]\n","homo: [[1.         0.51210467]\n"," [0.51210467 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:42:41 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:42:42 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [600/600 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.705500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 50/50 [00:00<00:00, 180.76it/s]\n","100%|██████████| 50/50 [00:00<00:00, 186.10it/s]\n","100%|██████████| 50/50 [00:00<00:00, 186.87it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.70403933]\n"," [0.70403933 1.        ]]\n","scrambled: [[1.         0.78680841]\n"," [0.78680841 1.        ]]\n","homo: [[1.         0.70635426]\n"," [0.70635426 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:43:29 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:43:29 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='615' max='615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [615/615 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.683400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 45/45 [00:00<00:00, 161.55it/s]\n","100%|██████████| 45/45 [00:00<00:00, 165.81it/s]\n","100%|██████████| 45/45 [00:00<00:00, 174.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.79200453]\n"," [0.79200453 1.        ]]\n","scrambled: [[1.         0.82539095]\n"," [0.82539095 1.        ]]\n","homo: [[1.         0.71822212]\n"," [0.71822212 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:44:17 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:44:17 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='621' max='621' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [621/621 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.807600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 43/43 [00:00<00:00, 176.83it/s]\n","100%|██████████| 43/43 [00:00<00:00, 178.07it/s]\n","100%|██████████| 43/43 [00:00<00:00, 185.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.        0.4773329]\n"," [0.4773329 1.       ]]\n","scrambled: [[1.         0.64218146]\n"," [0.64218146 1.        ]]\n","homo: [[1.         0.59203992]\n"," [0.59203992 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:45:06 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:45:06 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='561' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [561/561 00:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.677100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 63/63 [00:00<00:00, 171.71it/s]\n","100%|██████████| 63/63 [00:00<00:00, 175.70it/s]\n","100%|██████████| 63/63 [00:00<00:00, 184.33it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.51399943]\n"," [0.51399943 1.        ]]\n","scrambled: [[1.         0.51263655]\n"," [0.51263655 1.        ]]\n","homo: [[1.         0.53280177]\n"," [0.53280177 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:45:51 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:45:51 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='603' max='603' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [603/603 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.876800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 49/49 [00:00<00:00, 155.15it/s]\n","100%|██████████| 49/49 [00:00<00:00, 157.76it/s]\n","100%|██████████| 49/49 [00:00<00:00, 159.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.       0.599762]\n"," [0.599762 1.      ]]\n","scrambled: [[1.         0.55922598]\n"," [0.55922598 1.        ]]\n","homo: [[1.         0.52342929]\n"," [0.52342929 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:46:40 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:46:40 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='591' max='591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [591/591 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.718000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 53/53 [00:00<00:00, 157.88it/s]\n","100%|██████████| 53/53 [00:00<00:00, 161.82it/s]\n","100%|██████████| 53/53 [00:00<00:00, 159.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.69071953]\n"," [0.69071953 1.        ]]\n","scrambled: [[1.         0.59339515]\n"," [0.59339515 1.        ]]\n","homo: [[1.         0.34879426]\n"," [0.34879426 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:47:29 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:47:29 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [630/630 00:47, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.889600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 40/40 [00:00<00:00, 153.48it/s]\n","100%|██████████| 40/40 [00:00<00:00, 154.86it/s]\n","100%|██████████| 40/40 [00:00<00:00, 152.70it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.70068656]\n"," [0.70068656 1.        ]]\n","scrambled: [[1.         0.71662964]\n"," [0.71662964 1.        ]]\n","homo: [[1.         0.32726169]\n"," [0.32726169 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:48:21 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:48:22 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [597/597 00:45, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.860300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 51/51 [00:00<00:00, 159.13it/s]\n","100%|██████████| 51/51 [00:00<00:00, 159.56it/s]\n","100%|██████████| 51/51 [00:00<00:00, 161.20it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.58181316]\n"," [0.58181316 1.        ]]\n","scrambled: [[1.         0.60481415]\n"," [0.60481415 1.        ]]\n","homo: [[1.         0.39186431]\n"," [0.39186431 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:49:11 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:49:11 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='579' max='579' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [579/579 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.976200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 57/57 [00:00<00:00, 162.49it/s]\n","100%|██████████| 57/57 [00:00<00:00, 163.10it/s]\n","100%|██████████| 57/57 [00:00<00:00, 164.69it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.60150189]\n"," [0.60150189 1.        ]]\n","scrambled: [[1.         0.56633224]\n"," [0.56633224 1.        ]]\n","homo: [[1.        0.4745415]\n"," [0.4745415 1.       ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:50:00 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:50:00 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [597/597 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.643200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 51/51 [00:00<00:00, 182.86it/s]\n","100%|██████████| 51/51 [00:00<00:00, 180.85it/s]\n","100%|██████████| 51/51 [00:00<00:00, 180.58it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.65516747]\n"," [0.65516747 1.        ]]\n","scrambled: [[1.        0.6922969]\n"," [0.6922969 1.       ]]\n","homo: [[1.         0.61869257]\n"," [0.61869257 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:50:49 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:50:49 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [597/597 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.653000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 51/51 [00:00<00:00, 180.63it/s]\n","100%|██████████| 51/51 [00:00<00:00, 182.53it/s]\n","100%|██████████| 51/51 [00:00<00:00, 186.12it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.52722717]\n"," [0.52722717 1.        ]]\n","scrambled: [[1.         0.41721052]\n"," [0.41721052 1.        ]]\n","homo: [[1.         0.23952867]\n"," [0.23952867 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:51:38 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:51:38 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='633' max='633' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [633/633 00:46, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.658300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 39/39 [00:00<00:00, 171.82it/s]\n","100%|██████████| 39/39 [00:00<00:00, 175.32it/s]\n","100%|██████████| 39/39 [00:00<00:00, 172.71it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.66831613]\n"," [0.66831613 1.        ]]\n","scrambled: [[1.        0.6842228]\n"," [0.6842228 1.       ]]\n","homo: [[1.         0.58976223]\n"," [0.58976223 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:52:30 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:52:30 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='591' max='591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [591/591 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.602000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 53/53 [00:00<00:00, 186.41it/s]\n","100%|██████████| 53/53 [00:00<00:00, 177.37it/s]\n","100%|██████████| 53/53 [00:00<00:00, 180.32it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.44662045]\n"," [0.44662045 1.        ]]\n","scrambled: [[1.         0.42265695]\n"," [0.42265695 1.        ]]\n","homo: [[1.         0.41386549]\n"," [0.41386549 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:53:19 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:53:19 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='582' max='582' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [582/582 00:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.641900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 56/56 [00:00<00:00, 158.94it/s]\n","100%|██████████| 56/56 [00:00<00:00, 165.99it/s]\n","100%|██████████| 56/56 [00:00<00:00, 173.35it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.42307068]\n"," [0.42307068 1.        ]]\n","scrambled: [[1.        0.4179812]\n"," [0.4179812 1.       ]]\n","homo: [[1.         0.35942737]\n"," [0.35942737 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:54:06 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:54:06 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='603' max='603' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [603/603 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.667100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 49/49 [00:00<00:00, 173.05it/s]\n","100%|██████████| 49/49 [00:00<00:00, 171.96it/s]\n","100%|██████████| 49/49 [00:00<00:00, 181.79it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.76040159]\n"," [0.76040159 1.        ]]\n","scrambled: [[1.         0.75504269]\n"," [0.75504269 1.        ]]\n","homo: [[1.         0.25655759]\n"," [0.25655759 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:54:54 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:54:54 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [600/600 00:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.585200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 50/50 [00:00<00:00, 182.90it/s]\n","100%|██████████| 50/50 [00:00<00:00, 180.96it/s]\n","100%|██████████| 50/50 [00:00<00:00, 185.50it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.76866741]\n"," [0.76866741 1.        ]]\n","scrambled: [[1.         0.76711802]\n"," [0.76711802 1.        ]]\n","homo: [[1.         0.23930606]\n"," [0.23930606 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:55:42 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:55:42 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='615' max='615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [615/615 00:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.558400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 45/45 [00:00<00:00, 162.07it/s]\n","100%|██████████| 45/45 [00:00<00:00, 163.87it/s]\n","100%|██████████| 45/45 [00:00<00:00, 171.37it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.80624729]\n"," [0.80624729 1.        ]]\n","scrambled: [[1.         0.80072275]\n"," [0.80072275 1.        ]]\n","homo: [[1.         0.68953612]\n"," [0.68953612 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:56:30 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:56:30 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='621' max='621' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [621/621 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.749300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 43/43 [00:00<00:00, 178.04it/s]\n","100%|██████████| 43/43 [00:00<00:00, 177.21it/s]\n","100%|██████████| 43/43 [00:00<00:00, 183.22it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.        0.6665465]\n"," [0.6665465 1.       ]]\n","scrambled: [[1.         0.67079542]\n"," [0.67079542 1.        ]]\n","homo: [[1.         0.29588847]\n"," [0.29588847 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:57:19 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:57:19 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='561' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [561/561 00:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.658200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 63/63 [00:00<00:00, 178.73it/s]\n","100%|██████████| 63/63 [00:00<00:00, 179.75it/s]\n","100%|██████████| 63/63 [00:00<00:00, 176.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.        0.8259779]\n"," [0.8259779 1.       ]]\n","scrambled: [[1.         0.74767223]\n"," [0.74767223 1.        ]]\n","homo: [[1.         0.21703148]\n"," [0.21703148 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:58:04 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:58:04 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='603' max='603' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [603/603 00:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.605800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 49/49 [00:00<00:00, 155.67it/s]\n","100%|██████████| 49/49 [00:00<00:00, 161.55it/s]\n","100%|██████████| 49/49 [00:00<00:00, 159.45it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.61670583]\n"," [0.61670583 1.        ]]\n","scrambled: [[1.         0.58360519]\n"," [0.58360519 1.        ]]\n","homo: [[1.         0.30976436]\n"," [0.30976436 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:58:51 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:58:51 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='591' max='591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [591/591 00:45, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.520100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 53/53 [00:00<00:00, 158.43it/s]\n","100%|██████████| 53/53 [00:00<00:00, 156.21it/s]\n","100%|██████████| 53/53 [00:00<00:00, 161.68it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.62730923]\n"," [0.62730923 1.        ]]\n","scrambled: [[1.         0.68973432]\n"," [0.68973432 1.        ]]\n","homo: [[1.       0.293547]\n"," [0.293547 1.      ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 06:59:41 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 06:59:41 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [630/630 00:47, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.632600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 40/40 [00:00<00:00, 152.25it/s]\n","100%|██████████| 40/40 [00:00<00:00, 152.76it/s]\n","100%|██████████| 40/40 [00:00<00:00, 153.09it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.66722112]\n"," [0.66722112 1.        ]]\n","scrambled: [[1.         0.82696951]\n"," [0.82696951 1.        ]]\n","homo: [[1.         0.44289435]\n"," [0.44289435 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 07:00:33 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 07:00:33 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [597/597 00:45, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.619100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 51/51 [00:00<00:00, 154.69it/s]\n","100%|██████████| 51/51 [00:00<00:00, 142.99it/s]\n","100%|██████████| 51/51 [00:00<00:00, 153.15it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.51215589]\n"," [0.51215589 1.        ]]\n","scrambled: [[1.         0.59781985]\n"," [0.59781985 1.        ]]\n","homo: [[1.         0.35087261]\n"," [0.35087261 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","05/09/2021 07:01:24 - INFO - happytransformer.happy_transformer -   Using model: cuda\n","05/09/2021 07:01:24 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='579' max='579' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [579/579 00:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.580200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 57/57 [00:00<00:00, 159.51it/s]\n","100%|██████████| 57/57 [00:00<00:00, 159.16it/s]\n","100%|██████████| 57/57 [00:00<00:00, 165.01it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["normal: [[1.         0.65407312]\n"," [0.65407312 1.        ]]\n","scrambled: [[1.         0.63571671]\n"," [0.63571671 1.        ]]\n","homo: [[1.         0.42945509]\n"," [0.42945509 1.        ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KJoYYO7HGJCl"},"source":["specific_results = {}\n","\n","# It is likely that the results of happytransformer models are normalized using a sigmoid function\n","# Using this invers_sigmoid function allows us to get the raw scores of regression models\n","# Doing this seems to improve accuracy slightly\n","def inverse_sigmoid(y):\n","  return np.log(y/(1-y))\n","\n","for metric in [\"Creativity_Combined\", \"Novelty_Combined\", \"Usefulness_Combined\"]:\n","  for study in [1,2,3]:\n","    for pred_type in ['normal', \"_scrambled\", \"homo\"]:\n","      all_filenames = [\"Domain_specific_study\" + str(study) + \"_\" + metric + \"_split\" + str(i) + pred_type+'_prediction.csv' for i in range(1,6)]\n","      combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames]).sort_values(by='Unnamed: 0')\n","      train_file= extract_metric(study,metric)\n","\n","      y_estimate = combined_csv['label']\n","      y_true = train_file.loc[combined_csv['index'],'label']\n","\n","      #specific_results[metric + \"_study\" + str(study)+ '_' + pred_type] = (np.corrcoef(y_estimate, y_true)[0,1])\n","      specific_results[metric + \"_study\" + str(study)+ '_' + pred_type] = (np.corrcoef(inverse_sigmoid(y_estimate), y_true)[0,1])\n","\n","# pd.DataFrame.from_dict({'key':specific_results.keys(), 'value': specific_results.values()}).to_csv(\"Domain_specific_results(0503).csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQHd-8cy_s12"},"source":["Inverse_sigmoid results\n","\n","{'Creativity_Combined_study1__scrambled': 0.6036988186771806,\n"," 'Creativity_Combined_study1_homo': 0.4823619568751188,\n"," 'Creativity_Combined_study1_normal': 0.6463445792796637,\n"," 'Creativity_Combined_study2__scrambled': 0.7203512372807368,\n"," 'Creativity_Combined_study2_homo': 0.5198173668616751,\n"," 'Creativity_Combined_study2_normal': 0.7117417651112722,\n"," 'Creativity_Combined_study3__scrambled': 0.6659989029153098,\n"," 'Creativity_Combined_study3_homo': 0.38233451080530717,\n"," 'Creativity_Combined_study3_normal': 0.6872549785663923,\n"," 'Novelty_Combined_study1__scrambled': 0.5592626273318138,\n"," 'Novelty_Combined_study1_homo': 0.49202069997900133,\n"," 'Novelty_Combined_study1_normal': 0.5744742944075424,\n"," 'Novelty_Combined_study2__scrambled': 0.6418145060710528,\n"," 'Novelty_Combined_study2_homo': 0.45258013837462935,\n"," 'Novelty_Combined_study2_normal': 0.5910802973383661,\n"," 'Novelty_Combined_study3__scrambled': 0.5311424200434418,\n"," 'Novelty_Combined_study3_homo': 0.3781024469416406,\n"," 'Novelty_Combined_study3_normal': 0.5513141301276988,\n"," 'Usefulness_Combined_study1__scrambled': 0.4715331178125642,\n"," 'Usefulness_Combined_study1_homo': 0.36764497672426866,\n"," 'Usefulness_Combined_study1_normal': 0.5450251915363286,\n"," 'Usefulness_Combined_study2__scrambled': 0.7407053166149862,\n"," 'Usefulness_Combined_study2_homo': 0.21543484128261622,\n"," 'Usefulness_Combined_study2_normal': 0.7717636916972684,\n"," 'Usefulness_Combined_study3__scrambled': 0.5987181160598203,\n"," 'Usefulness_Combined_study3_homo': 0.3271331810590989,\n"," 'Usefulness_Combined_study3_normal': 0.591836376949932}\n","\n","Raw results\n"," {'Creativity_Combined_study1__scrambled': 0.5771698758149193,\n"," 'Creativity_Combined_study1_homo': 0.4300978918035641,\n"," 'Creativity_Combined_study1_normal': 0.6463088483610605,\n"," 'Creativity_Combined_study2__scrambled': 0.701350483161944,\n"," 'Creativity_Combined_study2_homo': 0.5321629182040738,\n"," 'Creativity_Combined_study2_normal': 0.6961954262707623,\n"," 'Creativity_Combined_study3__scrambled': 0.6273087736456405,\n"," 'Creativity_Combined_study3_homo': 0.3786845560571378,\n"," 'Creativity_Combined_study3_normal': 0.67867499963009,\n"," 'Novelty_Combined_study1__scrambled': 0.5331403579091303,\n"," 'Novelty_Combined_study1_homo': 0.4534316982609445,\n"," 'Novelty_Combined_study1_normal': 0.5456892636937443,\n"," 'Novelty_Combined_study2__scrambled': 0.5912638382707073,\n"," 'Novelty_Combined_study2_homo': 0.4668422177205251,\n"," 'Novelty_Combined_study2_normal': 0.539181017446131,\n"," 'Novelty_Combined_study3__scrambled': 0.4840568557620448,\n"," 'Novelty_Combined_study3_homo': 0.37172995291699334,\n"," 'Novelty_Combined_study3_normal': 0.5097986565921832,\n"," 'Usefulness_Combined_study1__scrambled': 0.47613903003544167,\n"," 'Usefulness_Combined_study1_homo': 0.34990235954121973,\n"," 'Usefulness_Combined_study1_normal': 0.524757089413579,\n"," 'Usefulness_Combined_study2__scrambled': 0.7317236140363812,\n"," 'Usefulness_Combined_study2_homo': 0.2474972202827062,\n"," 'Usefulness_Combined_study2_normal': 0.7613030946034497,\n"," 'Usefulness_Combined_study3__scrambled': 0.5717617988538715,\n"," 'Usefulness_Combined_study3_homo': 0.3270518133572374,\n"," 'Usefulness_Combined_study3_normal': 0.5761344859890838}"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EDBLBd_8Lux","executionInfo":{"status":"ok","timestamp":1620013286789,"user_tz":420,"elapsed":91346,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"4573f675-fb4d-4b33-cb88-81d4819e73d5"},"source":["#Here, I try to combine BERT's prediction with predictions based on other features, but it didn't work well\n","# You may want to ignore these for now.\n","\n","all_filenames = [str(i)+\"regression_prediction.csv\" for i in range(1,6)]\n","combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames]).sort_values(by='Unnamed: 0').reset_index()\n","\n","\n","def get_length(df):\n","  nrow = df.shape[0]\n","  lengths = []\n","  characters = []\n","  average_len = []\n","  for i in range(nrow):\n","      lengths.append(len(df['text'][i].split()))\n","      characters.append(len(df['text'][i]))\n","      average_len.append(len(df['text'][i])/len(df['text'][i].split()))\n","  df_len = pd.DataFrame({'lengths': lengths,\n","                        'characters': characters,\n","                        'average_len': average_len})\n","  return(df_len.to_numpy())\n","\n","def glove2dict(src_filename):\n","    \"\"\"\n","    GloVe vectors file reader.\n","\n","    Parameters\n","    ----------\n","    src_filename : str\n","        Full path to the GloVe file to be processed.\n","\n","    Returns\n","    -------\n","    dict\n","        Mapping words to their GloVe vectors as `np.array`.\n","\n","    \"\"\"\n","    # This distribution has some words with spaces, so we have to\n","    # assume its dimensionality and parse out the lines specially:\n","    if '840B.300d' in src_filename:\n","        line_parser = lambda line: line.rsplit(\" \", 300)\n","    else:\n","        line_parser = lambda line: line.strip().split()\n","    data = {}\n","    with open(src_filename, encoding='utf8') as f:\n","        while True:\n","            try:\n","                line = next(f)\n","                line = line_parser(line)\n","                data[line[0]] = np.array(line[1: ], dtype=np.float)\n","            except StopIteration:\n","                break\n","            except UnicodeDecodeError:\n","                pass\n","    return data\n","\n","# glove_dict = glove2dict(\n","#     os.path.join('cs224u','data', 'glove.6B', 'glove.6B.300d.txt'))\n","\n","def get_distance(word1, word2):\n","  from scipy.spatial.distance import cosine\n","  v1 = glove_dict[word1],\n","  v2 = glove_dict[word2]\n","\n","  return(cosine(v1, v2))\n","\n","def get_vector_distance(text):\n","  words = nltk.word_tokenize(text)\n","  stop_words = nltk.corpus.stopwords.words('english')\n","  words = [w for w in words if not w in stop_words] \n","  n = len(words)\n","  distances = []\n","  for i in range(n):\n","    for j in range(i+1, n):\n","      if words[i] in glove_dict and words[j] in glove_dict:\n","        distances.append(get_distance(words[i], words[j]))\n","      else:\n","        continue\n","  return(distances)\n","\n","\n","def get_df_distance(df):\n","  lst_distances = [get_vector_distance(text) for text in df['text']]\n","\n","  df_dis = pd.DataFrame({'average': [np.average(distances) for distances in lst_distances],\n","                    'max': [np.max(distances) for distances in lst_distances],\n","                    'min': [np.min(distances) for distances in lst_distances]})\n","  return(df_dis.to_numpy())\n","bert_pred = combined_csv['label']\n","df_length = get_length(combined_csv)\n","df_distance = get_df_distance(combined_csv)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5985653734163079"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSA51s8B_xus","executionInfo":{"status":"ok","timestamp":1620013428893,"user_tz":420,"elapsed":437,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"19ad4500-f0be-47ce-b89d-af2868bcca25"},"source":["from sklearn import linear_model\n","lr = linear_model.LinearRegression()\n","X = np.column_stack([bert_pred,df_length,df_distance])\n","y = train_file['label'].to_numpy().reshape(-1,1)\n","lr.fit(X,y)\n","lr.score(X,y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5819160869112776"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"id":"9fXlxmcazeWp","executionInfo":{"status":"ok","timestamp":1618393304837,"user_tz":420,"elapsed":199182,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"10438589586656072815"}},"outputId":"563e2cfa-1722-4b80-d6fe-494e3b9b497f"},"source":["#Trying to use two sample two predict the other sample\n","train_set = pd.concat([extract_metric(1,\"Novelty_Consumer\"),\n","          extract_metric(2,\"Novelty_Consumer\")]).reset_index()\n","\n","train_set.to_csv(\"train_file.csv\")\n","test_set = extract_metric(3,\"Novelty_Consumer\")\n","test_set.to_csv(\"test_file.csv\")\n","\n","model = HappyTextClassification(\n","    #\"ROBERTA\", \"roberta-base\",\n","    \"DISTILBERT\", \"distilbert-base-uncased\",\n","    #\"ALBERT\", \"albert-base-v2\",\n","                              num_labels=1)\n","model.train(\"train_file.csv\")\n","pred = model.test(\"test_file.csv\")\n","\n","pred_score = [x.score for x in pred]\n","\n","df_pred = test_set.copy()\n","df_pred['label'] = pred_score\n","print(np.corrcoef(pred_score, test_set['label']))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","04/14/2021 09:38:33 - INFO - happytransformer.happy_transformer -   Using model: cuda\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='333' max='333' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [333/333 02:53, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 308/308 [00:15<00:00, 19.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["[[1.         0.58764012]\n"," [0.58764012 1.        ]]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}