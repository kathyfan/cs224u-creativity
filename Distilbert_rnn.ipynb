{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p270iyuaIsPb"
   },
   "source": [
    "This notebook is written based on [this reference implementation](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb).\n",
    "\n",
    "Other refs for model:\n",
    "* https://stackoverflow.com/questions/65205582/how-can-i-add-a-bi-lstm-layer-on-top-of-bert-model\n",
    "* https://discuss.pytorch.org/t/how-to-connect-hook-two-or-even-more-models-together/21033\n",
    "* https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "* https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "\n",
    "Other refs for torchtext:\n",
    "* https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84\n",
    "* https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496\n",
    "* http://anie.me/On-Torchtext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRXm2FfxzC9A"
   },
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5222,
     "status": "ok",
     "timestamp": 1621625578255,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "QtVQ8yVAzMTS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "# random.seed(1)\n",
    "import re\n",
    "\n",
    "# Data processing.\n",
    "import constants # constants.py\n",
    "import dataset # dataset.py\n",
    "import torch\n",
    "\n",
    "# Model.\n",
    "import models # models.py\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "# Training.\n",
    "import training # training.py\n",
    "import utils # utils.py\n",
    "\n",
    "# If you make a code change that doesn't get picked up by\n",
    "# Jupyter notebook, try reloading like below:\n",
    "# import imp\n",
    "# imp.reload(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PL4p_28SzGCv"
   },
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2809,
     "status": "ok",
     "timestamp": 1621625592336,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "HwNKmWDey-q9"
   },
   "outputs": [],
   "source": [
    "# data_df = dataset.get_multiple_datasets([1,2,3], 'Creativity_Combined', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1621625593325,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "wvWVLrupUEnt"
   },
   "outputs": [],
   "source": [
    "'''This cell is commented out because the csvs should already exist in the directory.\n",
    "If you are running the notebook for the first time, run them to generate the csvs.'''\n",
    "# split into train, test sets. (Train set will be further split into \n",
    "# train+validation sets, via k-fold CV.)\n",
    "# train_df = data_df[:1000]\n",
    "# test_df = data_df[1000:] # roughly 203 test examples set aside\n",
    "\n",
    "# write them to CSV files\n",
    "# train_df.to_csv('ktrain.csv', index=False, header=False)\n",
    "# test_df.to_csv('ktest.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXVH4t-UC_wD"
   },
   "source": [
    "## Preprocessing and transform into torchtext Dataset format.\n",
    "\n",
    "From what I understand, some preprocessing is done when data.Field() is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2679,
     "status": "ok",
     "timestamp": 1621625596002,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "ifqSE8yj76Vb"
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset.get_train_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1621625596006,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "zfxEmIj65PFa"
   },
   "outputs": [],
   "source": [
    "# Transform train_dataset into an np array representation.\n",
    "# This will be used for generating the K folds.\n",
    "train_exs_arr = np.array(train_dataset.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wb9qlt-_0xlO"
   },
   "source": [
    "# Training pipeline begins here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6750,
     "status": "ok",
     "timestamp": 1621625602743,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "Ht9cEYK_TeGB",
    "outputId": "01d6745b-e8d8-46b7-efd2-569654d1a558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eid 0, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 128, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 25.232 | Train Corr: 0.03\n",
      "\t Val. Loss: 5.094 |  Val. Corr: 0.41\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 5.509 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.337 |  Val. Corr: 0.44\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 4.452 | Train Corr: 0.36\n",
      "\t Val. Loss: 4.108 |  Val. Corr: 0.42\n",
      "Epoch: 03 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 3.905 | Train Corr: 0.47\n",
      "\t Val. Loss: 4.680 |  Val. Corr: 0.45\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 3.407 | Train Corr: 0.57\n",
      "\t Val. Loss: 3.048 |  Val. Corr: 0.60\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 2.683 | Train Corr: 0.68\n",
      "\t Val. Loss: 2.912 |  Val. Corr: 0.63\n",
      "updating saved weights of best model\n",
      "Epoch: 06 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 2.020 | Train Corr: 0.77\n",
      "\t Val. Loss: 2.753 |  Val. Corr: 0.62\n",
      "Epoch: 07 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 1.583 | Train Corr: 0.82\n",
      "\t Val. Loss: 3.094 |  Val. Corr: 0.64\n",
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 41.768 | Train Corr: -0.01\n",
      "\t Val. Loss: 9.163 |  Val. Corr: 0.42\n",
      "Epoch: 01 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 7.213 | Train Corr: -0.02\n",
      "\t Val. Loss: 4.748 |  Val. Corr: 0.42\n",
      "Epoch: 02 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 5.501 | Train Corr: -0.01\n",
      "\t Val. Loss: 4.917 |  Val. Corr: 0.20\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 32.702 | Train Corr: 0.00\n",
      "\t Val. Loss: 6.692 |  Val. Corr: 0.47\n",
      "Epoch: 01 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 6.006 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.002 |  Val. Corr: 0.39\n",
      "Epoch: 02 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 4.967 | Train Corr: 0.23\n",
      "\t Val. Loss: 4.119 |  Val. Corr: 0.41\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 27.860 | Train Corr: 0.01\n",
      "\t Val. Loss: 6.452 |  Val. Corr: 0.41\n",
      "Epoch: 01 | Epoch Time: 0m 50s\n",
      "\t Train Loss: 5.426 | Train Corr: 0.07\n",
      "\t Val. Loss: 4.465 |  Val. Corr: 0.36\n",
      "Epoch: 02 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 4.295 | Train Corr: 0.38\n",
      "\t Val. Loss: 4.169 |  Val. Corr: 0.55\n",
      "Epoch: 03 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 3.716 | Train Corr: 0.47\n",
      "\t Val. Loss: 4.325 |  Val. Corr: 0.56\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 49s\n",
      "\t Train Loss: 29.726 | Train Corr: -0.01\n",
      "\t Val. Loss: 7.273 |  Val. Corr: 0.43\n",
      "Epoch: 01 | Epoch Time: 0m 49s\n",
      "\t Train Loss: 5.450 | Train Corr: 0.05\n",
      "\t Val. Loss: 4.650 |  Val. Corr: 0.29\n",
      "Epoch: 02 | Epoch Time: 0m 48s\n",
      "\t Train Loss: 4.143 | Train Corr: 0.41\n",
      "\t Val. Loss: 4.189 |  Val. Corr: 0.47\n",
      "eid 1, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 128, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 2}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 68.571 | Train Corr: 0.01\n",
      "\t Val. Loss: 20.921 |  Val. Corr: 0.45\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 8.987 | Train Corr: -0.05\n",
      "\t Val. Loss: 4.588 |  Val. Corr: -0.25\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 4.930 | Train Corr: 0.23\n",
      "\t Val. Loss: 3.597 |  Val. Corr: 0.49\n",
      "Epoch: 03 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 4.356 | Train Corr: 0.36\n",
      "\t Val. Loss: 4.699 |  Val. Corr: 0.08\n",
      "Epoch: 04 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 3.884 | Train Corr: 0.46\n",
      "\t Val. Loss: 3.932 |  Val. Corr: 0.36\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 65.979 | Train Corr: -0.02\n",
      "\t Val. Loss: 11.897 |  Val. Corr: 0.46\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 6.934 | Train Corr: -0.06\n",
      "\t Val. Loss: 4.814 |  Val. Corr: 0.43\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 5.214 | Train Corr: 0.08\n",
      "\t Val. Loss: 4.962 |  Val. Corr: 0.42\n",
      "Epoch: 03 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 4.393 | Train Corr: 0.33\n",
      "\t Val. Loss: 4.023 |  Val. Corr: 0.60\n",
      "Epoch: 04 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 3.600 | Train Corr: 0.50\n",
      "\t Val. Loss: 4.340 |  Val. Corr: 0.57\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 66.426 | Train Corr: 0.01\n",
      "\t Val. Loss: 16.859 |  Val. Corr: 0.49\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 8.219 | Train Corr: -0.03\n",
      "\t Val. Loss: 4.056 |  Val. Corr: 0.48\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 5.546 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.036 |  Val. Corr: 0.33\n",
      "Epoch: 03 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 4.755 | Train Corr: 0.30\n",
      "\t Val. Loss: 4.883 |  Val. Corr: 0.29\n",
      "Epoch: 04 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 4.274 | Train Corr: 0.42\n",
      "\t Val. Loss: 4.326 |  Val. Corr: 0.47\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 66.275 | Train Corr: -0.00\n",
      "\t Val. Loss: 20.598 |  Val. Corr: 0.47\n",
      "Epoch: 01 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 8.516 | Train Corr: -0.05\n",
      "\t Val. Loss: 4.808 |  Val. Corr: 0.45\n",
      "Epoch: 02 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 5.377 | Train Corr: 0.10\n",
      "\t Val. Loss: 4.632 |  Val. Corr: 0.24\n",
      "Epoch: 03 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 4.474 | Train Corr: 0.32\n",
      "\t Val. Loss: 4.635 |  Val. Corr: 0.34\n",
      "Epoch: 04 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 4.006 | Train Corr: 0.43\n",
      "\t Val. Loss: 5.263 |  Val. Corr: 0.32\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 50s\n",
      "\t Train Loss: 60.463 | Train Corr: -0.02\n",
      "\t Val. Loss: 18.207 |  Val. Corr: 0.45\n",
      "Epoch: 01 | Epoch Time: 0m 50s\n",
      "\t Train Loss: 7.059 | Train Corr: 0.01\n",
      "\t Val. Loss: 4.839 |  Val. Corr: 0.00\n",
      "Epoch: 02 | Epoch Time: 0m 49s\n",
      "\t Train Loss: 5.260 | Train Corr: 0.11\n",
      "\t Val. Loss: 3.978 |  Val. Corr: 0.46\n",
      "Epoch: 03 | Epoch Time: 0m 49s\n",
      "\t Train Loss: 4.037 | Train Corr: 0.42\n",
      "\t Val. Loss: 4.380 |  Val. Corr: 0.37\n",
      "eid 2, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 128, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 3}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 82.967 | Train Corr: 0.00\n",
      "\t Val. Loss: 32.776 |  Val. Corr: 0.46\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 11.593 | Train Corr: -0.03\n",
      "\t Val. Loss: 4.588 |  Val. Corr: 0.47\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 5.661 | Train Corr: -0.00\n",
      "\t Val. Loss: 4.537 |  Val. Corr: 0.48\n",
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 83.181 | Train Corr: -0.02\n",
      "\t Val. Loss: 25.126 |  Val. Corr: 0.36\n",
      "Epoch: 01 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 11.742 | Train Corr: -0.01\n",
      "\t Val. Loss: 4.759 |  Val. Corr: 0.36\n",
      "Epoch: 02 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 5.816 | Train Corr: -0.03\n",
      "\t Val. Loss: 4.864 |  Val. Corr: 0.36\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 83.692 | Train Corr: -0.02\n",
      "\t Val. Loss: 21.633 |  Val. Corr: 0.53\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 8.923 | Train Corr: -0.04\n",
      "\t Val. Loss: 4.066 |  Val. Corr: 0.52\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 5.604 | Train Corr: 0.01\n",
      "\t Val. Loss: 4.054 |  Val. Corr: 0.22\n",
      "Epoch: 03 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 4.916 | Train Corr: 0.29\n",
      "\t Val. Loss: 4.271 |  Val. Corr: 0.42\n",
      "Epoch: 04 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 4.240 | Train Corr: 0.41\n",
      "\t Val. Loss: 4.367 |  Val. Corr: 0.47\n",
      "updating saved weights of best model\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 70.592 | Train Corr: -0.01\n",
      "\t Val. Loss: 12.720 |  Val. Corr: 0.59\n",
      "Epoch: 01 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 5.992 | Train Corr: -0.00\n",
      "\t Val. Loss: 4.916 |  Val. Corr: 0.14\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 87.142 | Train Corr: -0.03\n",
      "\t Val. Loss: 35.763 |  Val. Corr: 0.42\n",
      "Epoch: 01 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 11.809 | Train Corr: -0.03\n",
      "\t Val. Loss: 5.079 |  Val. Corr: 0.41\n",
      "Epoch: 02 | Epoch Time: 0m 50s\n",
      "\t Train Loss: 5.897 | Train Corr: -0.04\n",
      "\t Val. Loss: 4.885 |  Val. Corr: 0.14\n",
      "Epoch: 03 | Epoch Time: 0m 50s\n",
      "\t Train Loss: 4.883 | Train Corr: 0.27\n",
      "\t Val. Loss: 4.638 |  Val. Corr: 0.33\n",
      "eid 3, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 256, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 1}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 19.386 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.344 |  Val. Corr: 0.18\n",
      "Epoch: 01 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 3.884 | Train Corr: 0.44\n",
      "\t Val. Loss: 4.646 |  Val. Corr: 0.38\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 3.176 | Train Corr: 0.58\n",
      "\t Val. Loss: 3.936 |  Val. Corr: 0.54\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 2.270 | Train Corr: 0.72\n",
      "\t Val. Loss: 3.252 |  Val. Corr: 0.61\n",
      "Epoch: 04 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 1.557 | Train Corr: 0.82\n",
      "\t Val. Loss: 3.683 |  Val. Corr: 0.60\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 1.289 | Train Corr: 0.85\n",
      "\t Val. Loss: 2.794 |  Val. Corr: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 1.132 | Train Corr: 0.87\n",
      "\t Val. Loss: 2.889 |  Val. Corr: 0.59\n",
      "Epoch: 07 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 0.958 | Train Corr: 0.89\n",
      "\t Val. Loss: 3.003 |  Val. Corr: 0.62\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 25.039 | Train Corr: 0.01\n",
      "\t Val. Loss: 4.973 |  Val. Corr: 0.33\n",
      "Epoch: 01 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 4.717 | Train Corr: 0.22\n",
      "\t Val. Loss: 6.033 |  Val. Corr: 0.35\n",
      "Epoch: 02 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 3.720 | Train Corr: 0.46\n",
      "\t Val. Loss: 5.949 |  Val. Corr: 0.55\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 25.272 | Train Corr: 0.01\n",
      "\t Val. Loss: 3.931 |  Val. Corr: 0.42\n",
      "Epoch: 01 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 4.221 | Train Corr: 0.37\n",
      "\t Val. Loss: 4.665 |  Val. Corr: 0.44\n",
      "Epoch: 02 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 3.703 | Train Corr: 0.49\n",
      "\t Val. Loss: 4.424 |  Val. Corr: 0.59\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 21.092 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.753 |  Val. Corr: 0.45\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 3.638 | Train Corr: 0.47\n",
      "\t Val. Loss: 4.204 |  Val. Corr: 0.54\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 22.774 | Train Corr: 0.01\n",
      "\t Val. Loss: 4.581 |  Val. Corr: 0.50\n",
      "Epoch: 01 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 3.872 | Train Corr: 0.43\n",
      "\t Val. Loss: 4.377 |  Val. Corr: 0.53\n",
      "Epoch: 02 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 2.965 | Train Corr: 0.61\n",
      "\t Val. Loss: 3.020 |  Val. Corr: 0.66\n",
      "Epoch: 03 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 2.184 | Train Corr: 0.73\n",
      "\t Val. Loss: 3.562 |  Val. Corr: 0.64\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 1.591 | Train Corr: 0.81\n",
      "\t Val. Loss: 2.748 |  Val. Corr: 0.67\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 1.298 | Train Corr: 0.85\n",
      "\t Val. Loss: 2.606 |  Val. Corr: 0.68\n",
      "Epoch: 06 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 1.030 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.626 |  Val. Corr: 0.67\n",
      "Epoch: 07 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 0.877 | Train Corr: 0.90\n",
      "\t Val. Loss: 2.945 |  Val. Corr: 0.63\n",
      "eid 4, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 256, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 2}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 37.570 | Train Corr: 0.01\n",
      "\t Val. Loss: 4.434 |  Val. Corr: 0.34\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 4.726 | Train Corr: 0.16\n",
      "\t Val. Loss: 4.316 |  Val. Corr: 0.38\n",
      "Epoch: 02 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 3.712 | Train Corr: 0.48\n",
      "\t Val. Loss: 4.534 |  Val. Corr: 0.39\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 3.132 | Train Corr: 0.58\n",
      "\t Val. Loss: 2.963 |  Val. Corr: 0.59\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 2.150 | Train Corr: 0.74\n",
      "\t Val. Loss: 2.820 |  Val. Corr: 0.63\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.651 | Train Corr: 0.81\n",
      "\t Val. Loss: 2.759 |  Val. Corr: 0.64\n",
      "updating saved weights of best model\n",
      "Epoch: 06 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.342 | Train Corr: 0.85\n",
      "\t Val. Loss: 2.728 |  Val. Corr: 0.62\n",
      "updating saved weights of best model\n",
      "Epoch: 07 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 1.071 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.677 |  Val. Corr: 0.64\n",
      "Epoch: 08 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 0.920 | Train Corr: 0.90\n",
      "\t Val. Loss: 3.013 |  Val. Corr: 0.62\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 34.985 | Train Corr: 0.00\n",
      "\t Val. Loss: 4.874 |  Val. Corr: 0.03\n",
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 4.305 | Train Corr: 0.30\n",
      "\t Val. Loss: 7.151 |  Val. Corr: 0.55\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.535 | Train Corr: 0.49\n",
      "\t Val. Loss: 3.869 |  Val. Corr: 0.62\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 35.457 | Train Corr: 0.01\n",
      "\t Val. Loss: 4.095 |  Val. Corr: -0.03\n",
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 4.394 | Train Corr: 0.33\n",
      "\t Val. Loss: 4.395 |  Val. Corr: 0.61\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.761 | Train Corr: 0.48\n",
      "\t Val. Loss: 2.937 |  Val. Corr: 0.67\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.059 | Train Corr: 0.61\n",
      "\t Val. Loss: 2.447 |  Val. Corr: 0.72\n",
      "Epoch: 04 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 2.194 | Train Corr: 0.74\n",
      "\t Val. Loss: 2.600 |  Val. Corr: 0.69\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.663 | Train Corr: 0.81\n",
      "\t Val. Loss: 2.408 |  Val. Corr: 0.77\n",
      "Epoch: 06 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.303 | Train Corr: 0.86\n",
      "\t Val. Loss: 2.871 |  Val. Corr: 0.72\n",
      "updating saved weights of best model\n",
      "Epoch: 07 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 1.117 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.245 |  Val. Corr: 0.76\n",
      "updating saved weights of best model\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 39.923 | Train Corr: -0.03\n",
      "\t Val. Loss: 4.760 |  Val. Corr: 0.30\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 4.360 | Train Corr: 0.29\n",
      "\t Val. Loss: 4.376 |  Val. Corr: 0.46\n",
      "Epoch: 02 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 3.697 | Train Corr: 0.47\n",
      "\t Val. Loss: 3.969 |  Val. Corr: 0.51\n",
      "eid 5, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 256, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 3}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 48.097 | Train Corr: 0.01\n",
      "\t Val. Loss: 4.481 |  Val. Corr: -0.22\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 5.433 | Train Corr: -0.05\n",
      "\t Val. Loss: 4.429 |  Val. Corr: 0.50\n",
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 51.030 | Train Corr: -0.01\n",
      "\t Val. Loss: 5.147 |  Val. Corr: 0.32\n",
      "Epoch: 01 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 5.109 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.877 |  Val. Corr: 0.38\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 47.193 | Train Corr: -0.00\n",
      "\t Val. Loss: 4.170 |  Val. Corr: 0.51\n",
      "Epoch: 01 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 4.902 | Train Corr: 0.15\n",
      "\t Val. Loss: 4.359 |  Val. Corr: 0.46\n",
      "Epoch: 02 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 4.009 | Train Corr: 0.43\n",
      "\t Val. Loss: 4.711 |  Val. Corr: 0.30\n",
      "updating saved weights of best model\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 52.079 | Train Corr: -0.00\n",
      "\t Val. Loss: 4.817 |  Val. Corr: 0.57\n",
      "Epoch: 01 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 5.689 | Train Corr: -0.02\n",
      "\t Val. Loss: 4.796 |  Val. Corr: 0.55\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 4.516 | Train Corr: 0.25\n",
      "\t Val. Loss: 4.603 |  Val. Corr: 0.30\n",
      "Epoch: 03 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 3.977 | Train Corr: 0.39\n",
      "\t Val. Loss: 4.498 |  Val. Corr: 0.48\n",
      "Epoch: 04 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 3.452 | Train Corr: 0.51\n",
      "\t Val. Loss: 4.353 |  Val. Corr: 0.52\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 42.469 | Train Corr: -0.03\n",
      "\t Val. Loss: 5.099 |  Val. Corr: 0.41\n",
      "Epoch: 01 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 4.361 | Train Corr: 0.29\n",
      "\t Val. Loss: 4.482 |  Val. Corr: 0.40\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 3.375 | Train Corr: 0.53\n",
      "\t Val. Loss: 3.945 |  Val. Corr: 0.52\n",
      "updating saved weights of best model\n",
      "eid 6, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 512, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 1}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 17.907 | Train Corr: 0.09\n",
      "\t Val. Loss: 4.474 |  Val. Corr: 0.35\n",
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.357 | Train Corr: 0.53\n",
      "\t Val. Loss: 4.783 |  Val. Corr: 0.50\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 2.954 | Train Corr: 0.61\n",
      "\t Val. Loss: 3.349 |  Val. Corr: 0.56\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 2.032 | Train Corr: 0.75\n",
      "\t Val. Loss: 3.167 |  Val. Corr: 0.55\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 1.402 | Train Corr: 0.84\n",
      "\t Val. Loss: 3.091 |  Val. Corr: 0.60\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.119 | Train Corr: 0.87\n",
      "\t Val. Loss: 2.756 |  Val. Corr: 0.61\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 17.528 | Train Corr: 0.12\n",
      "\t Val. Loss: 4.758 |  Val. Corr: 0.48\n",
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.298 | Train Corr: 0.53\n",
      "\t Val. Loss: 5.925 |  Val. Corr: 0.65\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 2.568 | Train Corr: 0.66\n",
      "\t Val. Loss: 5.369 |  Val. Corr: 0.67\n",
      "Epoch: 03 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.882 | Train Corr: 0.77\n",
      "\t Val. Loss: 4.099 |  Val. Corr: 0.69\n",
      "Epoch: 04 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.262 | Train Corr: 0.85\n",
      "\t Val. Loss: 3.001 |  Val. Corr: 0.70\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 0.995 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.533 |  Val. Corr: 0.71\n",
      "Epoch: 06 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 0.846 | Train Corr: 0.90\n",
      "\t Val. Loss: 3.708 |  Val. Corr: 0.68\n",
      "Epoch: 07 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 0.594 | Train Corr: 0.93\n",
      "\t Val. Loss: 2.780 |  Val. Corr: 0.70\n",
      "Epoch: 08 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 0.666 | Train Corr: 0.93\n",
      "\t Val. Loss: 2.568 |  Val. Corr: 0.68\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 17.492 | Train Corr: 0.10\n",
      "\t Val. Loss: 3.925 |  Val. Corr: 0.52\n",
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.762 | Train Corr: 0.47\n",
      "\t Val. Loss: 3.891 |  Val. Corr: 0.61\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 2.862 | Train Corr: 0.63\n",
      "\t Val. Loss: 3.655 |  Val. Corr: 0.72\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 2.339 | Train Corr: 0.72\n",
      "\t Val. Loss: 1.623 |  Val. Corr: 0.79\n",
      "Epoch: 04 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 1.655 | Train Corr: 0.81\n",
      "\t Val. Loss: 1.910 |  Val. Corr: 0.76\n",
      "Epoch: 05 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.133 | Train Corr: 0.87\n",
      "\t Val. Loss: 1.657 |  Val. Corr: 0.77\n",
      "Epoch: 06 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 0.914 | Train Corr: 0.90\n",
      "\t Val. Loss: 2.822 |  Val. Corr: 0.77\n",
      "Epoch: 07 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 0.748 | Train Corr: 0.92\n",
      "\t Val. Loss: 1.922 |  Val. Corr: 0.78\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 17.634 | Train Corr: 0.10\n",
      "\t Val. Loss: 4.480 |  Val. Corr: 0.45\n",
      "Epoch: 01 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 3.423 | Train Corr: 0.51\n",
      "\t Val. Loss: 3.920 |  Val. Corr: 0.56\n",
      "Epoch: 02 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 3.020 | Train Corr: 0.59\n",
      "\t Val. Loss: 3.395 |  Val. Corr: 0.69\n",
      "Epoch: 03 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 2.157 | Train Corr: 0.73\n",
      "\t Val. Loss: 2.579 |  Val. Corr: 0.72\n",
      "Epoch: 04 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 1.531 | Train Corr: 0.82\n",
      "\t Val. Loss: 3.275 |  Val. Corr: 0.73\n",
      "Epoch: 05 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 1.165 | Train Corr: 0.86\n",
      "\t Val. Loss: 2.524 |  Val. Corr: 0.74\n",
      "Epoch: 06 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 0.917 | Train Corr: 0.89\n",
      "\t Val. Loss: 2.260 |  Val. Corr: 0.74\n",
      "Epoch: 07 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 0.735 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.850 |  Val. Corr: 0.76\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 17.982 | Train Corr: 0.09\n",
      "\t Val. Loss: 4.048 |  Val. Corr: 0.54\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 3.224 | Train Corr: 0.55\n",
      "\t Val. Loss: 3.525 |  Val. Corr: 0.59\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 2.540 | Train Corr: 0.67\n",
      "\t Val. Loss: 2.761 |  Val. Corr: 0.72\n",
      "Epoch: 03 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.785 | Train Corr: 0.78\n",
      "\t Val. Loss: 3.347 |  Val. Corr: 0.68\n",
      "Epoch: 04 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 1.176 | Train Corr: 0.86\n",
      "\t Val. Loss: 2.385 |  Val. Corr: 0.72\n",
      "Epoch: 05 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.023 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.344 |  Val. Corr: 0.73\n",
      "Epoch: 06 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 0.764 | Train Corr: 0.91\n",
      "\t Val. Loss: 2.451 |  Val. Corr: 0.70\n",
      "Epoch: 07 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 0.646 | Train Corr: 0.93\n",
      "\t Val. Loss: 2.266 |  Val. Corr: 0.73\n",
      "eid 7, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 512, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 2}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 25.200 | Train Corr: 0.05\n",
      "\t Val. Loss: 4.413 |  Val. Corr: 0.49\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 4.234 | Train Corr: 0.32\n",
      "\t Val. Loss: 4.324 |  Val. Corr: 0.38\n",
      "Epoch: 02 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 3.416 | Train Corr: 0.53\n",
      "\t Val. Loss: 4.996 |  Val. Corr: 0.41\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 2.680 | Train Corr: 0.65\n",
      "\t Val. Loss: 3.159 |  Val. Corr: 0.54\n",
      "Epoch: 04 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 1.910 | Train Corr: 0.77\n",
      "\t Val. Loss: 3.468 |  Val. Corr: 0.58\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 1.424 | Train Corr: 0.83\n",
      "\t Val. Loss: 2.980 |  Val. Corr: 0.64\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 24.681 | Train Corr: 0.06\n",
      "\t Val. Loss: 4.696 |  Val. Corr: 0.52\n",
      "Epoch: 01 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 3.699 | Train Corr: 0.44\n",
      "\t Val. Loss: 5.774 |  Val. Corr: 0.63\n",
      "Epoch: 02 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 3.016 | Train Corr: 0.58\n",
      "\t Val. Loss: 4.314 |  Val. Corr: 0.70\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 2.397 | Train Corr: 0.69\n",
      "\t Val. Loss: 2.918 |  Val. Corr: 0.71\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 1.722 | Train Corr: 0.79\n",
      "\t Val. Loss: 2.748 |  Val. Corr: 0.71\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 1.286 | Train Corr: 0.85\n",
      "\t Val. Loss: 2.524 |  Val. Corr: 0.69\n",
      "Epoch: 06 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 1.128 | Train Corr: 0.87\n",
      "\t Val. Loss: 2.766 |  Val. Corr: 0.69\n",
      "Epoch: 07 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.900 | Train Corr: 0.90\n",
      "\t Val. Loss: 3.214 |  Val. Corr: 0.70\n",
      "Epoch: 08 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.744 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.712 |  Val. Corr: 0.69\n",
      "Epoch: 09 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 0.551 | Train Corr: 0.94\n",
      "\t Val. Loss: 4.072 |  Val. Corr: 0.69\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 25.054 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.084 |  Val. Corr: 0.44\n",
      "Epoch: 01 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 4.471 | Train Corr: 0.28\n",
      "\t Val. Loss: 4.309 |  Val. Corr: 0.56\n",
      "Epoch: 02 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 3.619 | Train Corr: 0.49\n",
      "\t Val. Loss: 5.085 |  Val. Corr: 0.58\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 22.906 | Train Corr: 0.05\n",
      "\t Val. Loss: 4.990 |  Val. Corr: 0.40\n",
      "Epoch: 01 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 3.704 | Train Corr: 0.45\n",
      "\t Val. Loss: 4.159 |  Val. Corr: 0.53\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 23.911 | Train Corr: 0.02\n",
      "\t Val. Loss: 5.160 |  Val. Corr: 0.48\n",
      "Epoch: 01 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 3.811 | Train Corr: 0.41\n",
      "\t Val. Loss: 4.509 |  Val. Corr: 0.41\n",
      "eid 8, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 512, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 3}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 30.738 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.458 |  Val. Corr: 0.29\n",
      "Epoch: 01 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 4.962 | Train Corr: 0.09\n",
      "\t Val. Loss: 4.620 |  Val. Corr: 0.40\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 29.050 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.879 |  Val. Corr: 0.30\n",
      "Epoch: 01 | Epoch Time: 1m 7s\n",
      "\t Train Loss: 4.373 | Train Corr: 0.22\n",
      "\t Val. Loss: 6.013 |  Val. Corr: 0.49\n",
      "Epoch: 02 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 3.742 | Train Corr: 0.42\n",
      "\t Val. Loss: 5.537 |  Val. Corr: 0.64\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 29.080 | Train Corr: 0.03\n",
      "\t Val. Loss: 4.065 |  Val. Corr: 0.17\n",
      "Epoch: 01 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 5.300 | Train Corr: -0.00\n",
      "\t Val. Loss: 4.089 |  Val. Corr: 0.47\n",
      "updating saved weights of best model\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 28.669 | Train Corr: 0.03\n",
      "\t Val. Loss: 4.771 |  Val. Corr: 0.37\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 1m 4s\n",
      "\t Train Loss: 4.225 | Train Corr: 0.29\n",
      "\t Val. Loss: 3.819 |  Val. Corr: 0.63\n",
      "Epoch: 02 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 3.591 | Train Corr: 0.47\n",
      "\t Val. Loss: 3.827 |  Val. Corr: 0.57\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 29.157 | Train Corr: 0.00\n",
      "\t Val. Loss: 5.341 |  Val. Corr: 0.20\n",
      "Epoch: 01 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 5.428 | Train Corr: -0.02\n",
      "\t Val. Loss: 5.092 |  Val. Corr: 0.37\n",
      "Epoch: 02 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 4.853 | Train Corr: 0.01\n",
      "\t Val. Loss: 4.937 |  Val. Corr: 0.25\n",
      "eid 9, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 768, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 1}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 14.143 | Train Corr: 0.12\n",
      "\t Val. Loss: 4.136 |  Val. Corr: 0.30\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 3.270 | Train Corr: 0.55\n",
      "\t Val. Loss: 3.760 |  Val. Corr: 0.59\n",
      "Epoch: 02 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 2.549 | Train Corr: 0.68\n",
      "\t Val. Loss: 3.918 |  Val. Corr: 0.62\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 1.449 | Train Corr: 0.83\n",
      "\t Val. Loss: 3.150 |  Val. Corr: 0.59\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 1.054 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.669 |  Val. Corr: 0.64\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 15.465 | Train Corr: 0.13\n",
      "\t Val. Loss: 6.054 |  Val. Corr: 0.46\n",
      "Epoch: 01 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 3.358 | Train Corr: 0.52\n",
      "\t Val. Loss: 5.788 |  Val. Corr: 0.64\n",
      "Epoch: 02 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 2.607 | Train Corr: 0.65\n",
      "\t Val. Loss: 4.344 |  Val. Corr: 0.69\n",
      "Epoch: 03 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 1.745 | Train Corr: 0.78\n",
      "\t Val. Loss: 3.577 |  Val. Corr: 0.63\n",
      "Epoch: 04 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 1.106 | Train Corr: 0.87\n",
      "\t Val. Loss: 3.199 |  Val. Corr: 0.65\n",
      "Epoch: 05 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 0.849 | Train Corr: 0.90\n",
      "\t Val. Loss: 2.842 |  Val. Corr: 0.70\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 16.312 | Train Corr: 0.13\n",
      "\t Val. Loss: 3.774 |  Val. Corr: 0.49\n",
      "Epoch: 01 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 3.207 | Train Corr: 0.57\n",
      "\t Val. Loss: 2.773 |  Val. Corr: 0.73\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 2.245 | Train Corr: 0.73\n",
      "\t Val. Loss: 2.111 |  Val. Corr: 0.73\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 1.783 | Train Corr: 0.79\n",
      "\t Val. Loss: 2.057 |  Val. Corr: 0.74\n",
      "Epoch: 04 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 1.286 | Train Corr: 0.86\n",
      "\t Val. Loss: 2.184 |  Val. Corr: 0.76\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 0.839 | Train Corr: 0.91\n",
      "\t Val. Loss: 2.040 |  Val. Corr: 0.76\n",
      "Epoch: 06 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 0.624 | Train Corr: 0.93\n",
      "\t Val. Loss: 2.264 |  Val. Corr: 0.76\n",
      "Epoch: 07 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 0.527 | Train Corr: 0.94\n",
      "\t Val. Loss: 2.828 |  Val. Corr: 0.77\n",
      "updating saved weights of best model\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 14.518 | Train Corr: 0.13\n",
      "\t Val. Loss: 4.301 |  Val. Corr: 0.51\n",
      "Epoch: 01 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 3.233 | Train Corr: 0.54\n",
      "\t Val. Loss: 3.553 |  Val. Corr: 0.63\n",
      "Epoch: 02 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 2.595 | Train Corr: 0.66\n",
      "\t Val. Loss: 3.655 |  Val. Corr: 0.72\n",
      "Epoch: 03 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 1.821 | Train Corr: 0.78\n",
      "\t Val. Loss: 2.355 |  Val. Corr: 0.75\n",
      "Epoch: 04 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 1.355 | Train Corr: 0.84\n",
      "\t Val. Loss: 2.291 |  Val. Corr: 0.77\n",
      "Epoch: 05 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.937 | Train Corr: 0.89\n",
      "\t Val. Loss: 2.160 |  Val. Corr: 0.77\n",
      "Epoch: 06 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.654 | Train Corr: 0.93\n",
      "\t Val. Loss: 1.975 |  Val. Corr: 0.77\n",
      "Epoch: 07 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.537 | Train Corr: 0.94\n",
      "\t Val. Loss: 1.973 |  Val. Corr: 0.78\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 14.932 | Train Corr: 0.12\n",
      "\t Val. Loss: 5.007 |  Val. Corr: 0.44\n",
      "Epoch: 01 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 3.108 | Train Corr: 0.57\n",
      "\t Val. Loss: 3.595 |  Val. Corr: 0.66\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 2.550 | Train Corr: 0.67\n",
      "\t Val. Loss: 2.885 |  Val. Corr: 0.68\n",
      "Epoch: 03 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 1.834 | Train Corr: 0.77\n",
      "\t Val. Loss: 3.021 |  Val. Corr: 0.67\n",
      "Epoch: 04 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.175 | Train Corr: 0.86\n",
      "\t Val. Loss: 2.697 |  Val. Corr: 0.66\n",
      "Epoch: 05 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 0.869 | Train Corr: 0.90\n",
      "\t Val. Loss: 2.572 |  Val. Corr: 0.71\n",
      "Epoch: 06 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 0.748 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.485 |  Val. Corr: 0.68\n",
      "Epoch: 07 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 0.670 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.749 |  Val. Corr: 0.66\n",
      "Epoch: 08 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 0.512 | Train Corr: 0.94\n",
      "\t Val. Loss: 2.909 |  Val. Corr: 0.68\n",
      "eid 10, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 768, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 2}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 19.910 | Train Corr: 0.07\n",
      "\t Val. Loss: 4.351 |  Val. Corr: 0.35\n",
      "Epoch: 01 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 3.629 | Train Corr: 0.47\n",
      "\t Val. Loss: 4.701 |  Val. Corr: 0.45\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 19.248 | Train Corr: 0.06\n",
      "\t Val. Loss: 5.013 |  Val. Corr: 0.25\n",
      "Epoch: 01 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 3.917 | Train Corr: 0.39\n",
      "\t Val. Loss: 5.860 |  Val. Corr: 0.54\n",
      "Epoch: 02 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 3.010 | Train Corr: 0.58\n",
      "\t Val. Loss: 5.510 |  Val. Corr: 0.62\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 18.521 | Train Corr: 0.10\n",
      "\t Val. Loss: 4.420 |  Val. Corr: 0.46\n",
      "Epoch: 01 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 3.596 | Train Corr: 0.49\n",
      "\t Val. Loss: 4.615 |  Val. Corr: 0.47\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 18.789 | Train Corr: 0.07\n",
      "\t Val. Loss: 4.704 |  Val. Corr: 0.50\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 1m 10s\n",
      "\t Train Loss: 3.219 | Train Corr: 0.54\n",
      "\t Val. Loss: 3.171 |  Val. Corr: 0.69\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 2.251 | Train Corr: 0.71\n",
      "\t Val. Loss: 2.634 |  Val. Corr: 0.74\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 1.860 | Train Corr: 0.77\n",
      "\t Val. Loss: 2.622 |  Val. Corr: 0.76\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 1.256 | Train Corr: 0.85\n",
      "\t Val. Loss: 2.382 |  Val. Corr: 0.76\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 0.892 | Train Corr: 0.90\n",
      "\t Val. Loss: 1.960 |  Val. Corr: 0.78\n",
      "Epoch: 06 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 0.717 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.144 |  Val. Corr: 0.78\n",
      "Epoch: 07 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 0.563 | Train Corr: 0.94\n",
      "\t Val. Loss: 2.077 |  Val. Corr: 0.77\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 8s\n",
      "\t Train Loss: 19.535 | Train Corr: 0.07\n",
      "\t Val. Loss: 4.530 |  Val. Corr: 0.34\n",
      "Epoch: 01 | Epoch Time: 1m 9s\n",
      "\t Train Loss: 3.579 | Train Corr: 0.48\n",
      "\t Val. Loss: 4.590 |  Val. Corr: 0.45\n",
      "eid 11, params {'batch_size': 8, 'bidirectional': False, 'dropout': 0.2, 'hidden_dim': 768, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 3}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 22s\n",
      "\t Train Loss: 22.350 | Train Corr: 0.03\n",
      "\t Val. Loss: 4.549 |  Val. Corr: -0.25\n",
      "Epoch: 01 | Epoch Time: 1m 22s\n",
      "\t Train Loss: 5.053 | Train Corr: 0.02\n",
      "\t Val. Loss: 4.565 |  Val. Corr: -0.24\n",
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 23s\n",
      "\t Train Loss: 23.865 | Train Corr: 0.04\n",
      "\t Val. Loss: 5.025 |  Val. Corr: 0.31\n",
      "Epoch: 01 | Epoch Time: 1m 23s\n",
      "\t Train Loss: 4.706 | Train Corr: 0.11\n",
      "\t Val. Loss: 5.033 |  Val. Corr: 0.55\n",
      "Epoch: 02 | Epoch Time: 1m 23s\n",
      "\t Train Loss: 3.579 | Train Corr: 0.46\n",
      "\t Val. Loss: 4.839 |  Val. Corr: 0.61\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 22s\n",
      "\t Train Loss: 23.264 | Train Corr: 0.03\n",
      "\t Val. Loss: 4.071 |  Val. Corr: 0.17\n",
      "Epoch: 01 | Epoch Time: 1m 22s\n",
      "\t Train Loss: 5.113 | Train Corr: -0.04\n",
      "\t Val. Loss: 4.079 |  Val. Corr: 0.32\n",
      "updating saved weights of best model\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 20s\n",
      "\t Train Loss: 22.430 | Train Corr: 0.04\n",
      "\t Val. Loss: 4.777 |  Val. Corr: 0.36\n",
      "Epoch: 01 | Epoch Time: 1m 20s\n",
      "\t Train Loss: 4.798 | Train Corr: -0.01\n",
      "\t Val. Loss: 4.844 |  Val. Corr: 0.23\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 22.314 | Train Corr: 0.02\n",
      "\t Val. Loss: 4.769 |  Val. Corr: 0.33\n",
      "Epoch: 01 | Epoch Time: 1m 18s\n",
      "\t Train Loss: 4.954 | Train Corr: -0.01\n",
      "\t Val. Loss: 5.070 |  Val. Corr: 0.36\n",
      "eid 12, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 128, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 1}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 23.974 | Train Corr: 0.03\n",
      "\t Val. Loss: 4.430 |  Val. Corr: 0.27\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 3.994 | Train Corr: 0.41\n",
      "\t Val. Loss: 4.171 |  Val. Corr: 0.48\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 3.152 | Train Corr: 0.59\n",
      "\t Val. Loss: 2.882 |  Val. Corr: 0.60\n",
      "Epoch: 03 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 2.373 | Train Corr: 0.71\n",
      "\t Val. Loss: 3.144 |  Val. Corr: 0.58\n",
      "Epoch: 04 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 1.920 | Train Corr: 0.78\n",
      "\t Val. Loss: 3.470 |  Val. Corr: 0.60\n",
      "Epoch: 05 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.391 | Train Corr: 0.84\n",
      "\t Val. Loss: 3.241 |  Val. Corr: 0.62\n",
      "Epoch: 06 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 1.189 | Train Corr: 0.87\n",
      "\t Val. Loss: 3.124 |  Val. Corr: 0.61\n",
      "updating saved weights of best model\n",
      "Epoch: 07 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.018 | Train Corr: 0.89\n",
      "\t Val. Loss: 2.605 |  Val. Corr: 0.64\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 17.903 | Train Corr: 0.07\n",
      "\t Val. Loss: 6.510 |  Val. Corr: 0.53\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 3.955 | Train Corr: 0.41\n",
      "\t Val. Loss: 6.501 |  Val. Corr: 0.62\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 3.558 | Train Corr: 0.49\n",
      "\t Val. Loss: 7.055 |  Val. Corr: 0.59\n",
      "Epoch: 03 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 3.170 | Train Corr: 0.57\n",
      "\t Val. Loss: 4.165 |  Val. Corr: 0.58\n",
      "Epoch: 04 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 2.604 | Train Corr: 0.66\n",
      "\t Val. Loss: 3.506 |  Val. Corr: 0.67\n",
      "Epoch: 05 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 2.232 | Train Corr: 0.72\n",
      "\t Val. Loss: 2.813 |  Val. Corr: 0.70\n",
      "Epoch: 06 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 1.687 | Train Corr: 0.80\n",
      "\t Val. Loss: 4.015 |  Val. Corr: 0.66\n",
      "updating saved weights of best model\n",
      "Epoch: 07 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.440 | Train Corr: 0.83\n",
      "\t Val. Loss: 2.590 |  Val. Corr: 0.70\n",
      "Epoch: 08 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.159 | Train Corr: 0.87\n",
      "\t Val. Loss: 2.790 |  Val. Corr: 0.69\n",
      "Epoch: 09 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 0.958 | Train Corr: 0.89\n",
      "\t Val. Loss: 3.220 |  Val. Corr: 0.65\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 20.331 | Train Corr: 0.04\n",
      "\t Val. Loss: 3.854 |  Val. Corr: 0.57\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 4.133 | Train Corr: 0.40\n",
      "\t Val. Loss: 4.139 |  Val. Corr: 0.62\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 3.231 | Train Corr: 0.58\n",
      "\t Val. Loss: 4.199 |  Val. Corr: 0.66\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 2.719 | Train Corr: 0.67\n",
      "\t Val. Loss: 2.494 |  Val. Corr: 0.74\n",
      "Epoch: 04 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 2.315 | Train Corr: 0.73\n",
      "\t Val. Loss: 2.815 |  Val. Corr: 0.75\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 1.937 | Train Corr: 0.78\n",
      "\t Val. Loss: 1.524 |  Val. Corr: 0.81\n",
      "Epoch: 06 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.343 | Train Corr: 0.85\n",
      "\t Val. Loss: 1.693 |  Val. Corr: 0.77\n",
      "Epoch: 07 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.097 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.125 |  Val. Corr: 0.80\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 20.784 | Train Corr: 0.05\n",
      "\t Val. Loss: 3.932 |  Val. Corr: 0.55\n",
      "Epoch: 01 | Epoch Time: 0m 51s\n",
      "\t Train Loss: 3.626 | Train Corr: 0.48\n",
      "\t Val. Loss: 2.956 |  Val. Corr: 0.69\n",
      "Epoch: 02 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 2.876 | Train Corr: 0.62\n",
      "\t Val. Loss: 2.494 |  Val. Corr: 0.74\n",
      "Epoch: 03 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 2.222 | Train Corr: 0.72\n",
      "\t Val. Loss: 2.237 |  Val. Corr: 0.75\n",
      "Epoch: 04 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 1.622 | Train Corr: 0.81\n",
      "\t Val. Loss: 2.086 |  Val. Corr: 0.77\n",
      "Epoch: 05 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 1.403 | Train Corr: 0.84\n",
      "\t Val. Loss: 2.035 |  Val. Corr: 0.76\n",
      "Epoch: 06 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.232 | Train Corr: 0.86\n",
      "\t Val. Loss: 3.243 |  Val. Corr: 0.75\n",
      "Epoch: 07 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 1.041 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.568 |  Val. Corr: 0.76\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 50s\n",
      "\t Train Loss: 20.811 | Train Corr: 0.02\n",
      "\t Val. Loss: 4.283 |  Val. Corr: 0.54\n",
      "Epoch: 01 | Epoch Time: 0m 50s\n",
      "\t Train Loss: 3.687 | Train Corr: 0.47\n",
      "\t Val. Loss: 4.269 |  Val. Corr: 0.58\n",
      "Epoch: 02 | Epoch Time: 0m 49s\n",
      "\t Train Loss: 3.269 | Train Corr: 0.55\n",
      "\t Val. Loss: 3.501 |  Val. Corr: 0.64\n",
      "Epoch: 03 | Epoch Time: 0m 49s\n",
      "\t Train Loss: 2.646 | Train Corr: 0.66\n",
      "\t Val. Loss: 3.436 |  Val. Corr: 0.69\n",
      "Epoch: 04 | Epoch Time: 0m 49s\n",
      "\t Train Loss: 2.162 | Train Corr: 0.74\n",
      "\t Val. Loss: 2.366 |  Val. Corr: 0.72\n",
      "Epoch: 05 | Epoch Time: 0m 50s\n",
      "\t Train Loss: 1.667 | Train Corr: 0.80\n",
      "\t Val. Loss: 2.144 |  Val. Corr: 0.74\n",
      "Epoch: 06 | Epoch Time: 0m 49s\n",
      "\t Train Loss: 1.257 | Train Corr: 0.86\n",
      "\t Val. Loss: 2.311 |  Val. Corr: 0.72\n",
      "Epoch: 07 | Epoch Time: 0m 50s\n",
      "\t Train Loss: 1.123 | Train Corr: 0.87\n",
      "\t Val. Loss: 2.310 |  Val. Corr: 0.74\n",
      "Epoch: 08 | Epoch Time: 0m 48s\n",
      "\t Train Loss: 0.909 | Train Corr: 0.90\n",
      "\t Val. Loss: 2.280 |  Val. Corr: 0.73\n",
      "eid 13, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 128, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 2}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 32.316 | Train Corr: 0.02\n",
      "\t Val. Loss: 4.603 |  Val. Corr: 0.33\n",
      "Epoch: 01 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 4.066 | Train Corr: 0.39\n",
      "\t Val. Loss: 4.741 |  Val. Corr: 0.29\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 3.273 | Train Corr: 0.55\n",
      "\t Val. Loss: 4.369 |  Val. Corr: 0.36\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 37.585 | Train Corr: 0.00\n",
      "\t Val. Loss: 4.214 |  Val. Corr: 0.47\n",
      "Epoch: 01 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 4.579 | Train Corr: 0.25\n",
      "\t Val. Loss: 6.098 |  Val. Corr: 0.36\n",
      "Epoch: 02 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 3.396 | Train Corr: 0.52\n",
      "\t Val. Loss: 7.014 |  Val. Corr: 0.44\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 45.469 | Train Corr: -0.00\n",
      "\t Val. Loss: 3.678 |  Val. Corr: 0.54\n",
      "Epoch: 01 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 4.616 | Train Corr: 0.29\n",
      "\t Val. Loss: 4.158 |  Val. Corr: 0.51\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 34.382 | Train Corr: 0.00\n",
      "\t Val. Loss: 4.433 |  Val. Corr: 0.49\n",
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.952 | Train Corr: 0.41\n",
      "\t Val. Loss: 4.021 |  Val. Corr: 0.58\n",
      "updating saved weights of best model\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 39.885 | Train Corr: -0.03\n",
      "\t Val. Loss: 4.655 |  Val. Corr: 0.43\n",
      "Epoch: 01 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 4.430 | Train Corr: 0.29\n",
      "\t Val. Loss: 4.097 |  Val. Corr: 0.39\n",
      "Epoch: 02 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 3.576 | Train Corr: 0.49\n",
      "\t Val. Loss: 3.506 |  Val. Corr: 0.55\n",
      "eid 14, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 128, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 3}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 51.566 | Train Corr: 0.02\n",
      "\t Val. Loss: 3.969 |  Val. Corr: 0.46\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 3.922 | Train Corr: 0.42\n",
      "\t Val. Loss: 3.698 |  Val. Corr: 0.54\n",
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 53.082 | Train Corr: -0.01\n",
      "\t Val. Loss: 4.598 |  Val. Corr: 0.36\n",
      "Epoch: 01 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 4.780 | Train Corr: 0.16\n",
      "\t Val. Loss: 5.088 |  Val. Corr: 0.35\n",
      "Epoch: 02 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 3.860 | Train Corr: 0.43\n",
      "\t Val. Loss: 3.669 |  Val. Corr: 0.53\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 48.656 | Train Corr: -0.00\n",
      "\t Val. Loss: 3.408 |  Val. Corr: 0.59\n",
      "Epoch: 01 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 4.611 | Train Corr: 0.28\n",
      "\t Val. Loss: 3.527 |  Val. Corr: 0.44\n",
      "Epoch: 02 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 3.880 | Train Corr: 0.45\n",
      "\t Val. Loss: 3.962 |  Val. Corr: 0.38\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 4s\n",
      "\t Train Loss: 47.765 | Train Corr: -0.00\n",
      "\t Val. Loss: 4.720 |  Val. Corr: 0.42\n",
      "Epoch: 01 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 4.244 | Train Corr: 0.34\n",
      "\t Val. Loss: 4.449 |  Val. Corr: 0.47\n",
      "Epoch: 02 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 3.390 | Train Corr: 0.52\n",
      "\t Val. Loss: 4.454 |  Val. Corr: 0.51\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 49.727 | Train Corr: -0.04\n",
      "\t Val. Loss: 4.535 |  Val. Corr: 0.44\n",
      "Epoch: 01 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 4.172 | Train Corr: 0.33\n",
      "\t Val. Loss: 4.330 |  Val. Corr: 0.54\n",
      "Epoch: 02 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 3.242 | Train Corr: 0.55\n",
      "\t Val. Loss: 3.754 |  Val. Corr: 0.54\n",
      "eid 15, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 256, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 1}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 16.221 | Train Corr: 0.14\n",
      "\t Val. Loss: 4.000 |  Val. Corr: 0.45\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.173 | Train Corr: 0.57\n",
      "\t Val. Loss: 3.637 |  Val. Corr: 0.59\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 2.594 | Train Corr: 0.67\n",
      "\t Val. Loss: 3.274 |  Val. Corr: 0.62\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 1.680 | Train Corr: 0.80\n",
      "\t Val. Loss: 3.177 |  Val. Corr: 0.57\n",
      "Epoch: 04 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 1.313 | Train Corr: 0.85\n",
      "\t Val. Loss: 3.270 |  Val. Corr: 0.63\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.017 | Train Corr: 0.89\n",
      "\t Val. Loss: 2.803 |  Val. Corr: 0.62\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 15.414 | Train Corr: 0.12\n",
      "\t Val. Loss: 4.921 |  Val. Corr: 0.52\n",
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.563 | Train Corr: 0.48\n",
      "\t Val. Loss: 7.408 |  Val. Corr: 0.52\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 2.922 | Train Corr: 0.60\n",
      "\t Val. Loss: 4.647 |  Val. Corr: 0.70\n",
      "Epoch: 03 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 2.130 | Train Corr: 0.73\n",
      "\t Val. Loss: 2.969 |  Val. Corr: 0.64\n",
      "Epoch: 04 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.401 | Train Corr: 0.83\n",
      "\t Val. Loss: 3.489 |  Val. Corr: 0.67\n",
      "Epoch: 05 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 1.040 | Train Corr: 0.88\n",
      "\t Val. Loss: 3.099 |  Val. Corr: 0.62\n",
      "Epoch: 06 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 0.753 | Train Corr: 0.91\n",
      "\t Val. Loss: 3.239 |  Val. Corr: 0.64\n",
      "Epoch: 07 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 0.635 | Train Corr: 0.93\n",
      "\t Val. Loss: 2.958 |  Val. Corr: 0.70\n",
      "updating saved weights of best model\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 15.680 | Train Corr: 0.13\n",
      "\t Val. Loss: 4.495 |  Val. Corr: 0.47\n",
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 3.521 | Train Corr: 0.52\n",
      "\t Val. Loss: 3.765 |  Val. Corr: 0.63\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 15.294 | Train Corr: 0.13\n",
      "\t Val. Loss: 5.208 |  Val. Corr: 0.38\n",
      "Epoch: 01 | Epoch Time: 0m 54s\n",
      "\t Train Loss: 3.203 | Train Corr: 0.55\n",
      "\t Val. Loss: 4.159 |  Val. Corr: 0.51\n",
      "Epoch: 02 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 2.745 | Train Corr: 0.64\n",
      "\t Val. Loss: 3.838 |  Val. Corr: 0.69\n",
      "Epoch: 03 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 1.873 | Train Corr: 0.77\n",
      "\t Val. Loss: 2.802 |  Val. Corr: 0.70\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 0m 55s\n",
      "\t Train Loss: 1.344 | Train Corr: 0.84\n",
      "\t Val. Loss: 2.712 |  Val. Corr: 0.72\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 1.011 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.590 |  Val. Corr: 0.72\n",
      "Epoch: 06 | Epoch Time: 0m 56s\n",
      "\t Train Loss: 0.779 | Train Corr: 0.91\n",
      "\t Val. Loss: 4.585 |  Val. Corr: 0.74\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 16.203 | Train Corr: 0.09\n",
      "\t Val. Loss: 4.903 |  Val. Corr: 0.45\n",
      "Epoch: 01 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 3.379 | Train Corr: 0.52\n",
      "\t Val. Loss: 4.307 |  Val. Corr: 0.56\n",
      "Epoch: 02 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 2.812 | Train Corr: 0.62\n",
      "\t Val. Loss: 3.274 |  Val. Corr: 0.63\n",
      "Epoch: 03 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 2.090 | Train Corr: 0.74\n",
      "\t Val. Loss: 4.776 |  Val. Corr: 0.61\n",
      "Epoch: 04 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 1.702 | Train Corr: 0.80\n",
      "\t Val. Loss: 3.504 |  Val. Corr: 0.55\n",
      "Epoch: 05 | Epoch Time: 0m 53s\n",
      "\t Train Loss: 1.376 | Train Corr: 0.84\n",
      "\t Val. Loss: 3.025 |  Val. Corr: 0.60\n",
      "Epoch: 06 | Epoch Time: 0m 52s\n",
      "\t Train Loss: 0.987 | Train Corr: 0.89\n",
      "\t Val. Loss: 2.875 |  Val. Corr: 0.62\n",
      "eid 16, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 256, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 2}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 25.267 | Train Corr: 0.07\n",
      "\t Val. Loss: 4.903 |  Val. Corr: 0.20\n",
      "Epoch: 01 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 3.807 | Train Corr: 0.46\n",
      "\t Val. Loss: 5.182 |  Val. Corr: 0.20\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 2.768 | Train Corr: 0.64\n",
      "\t Val. Loss: 4.707 |  Val. Corr: 0.46\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 2.178 | Train Corr: 0.73\n",
      "\t Val. Loss: 3.131 |  Val. Corr: 0.56\n",
      "Epoch: 04 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 1.532 | Train Corr: 0.82\n",
      "\t Val. Loss: 3.244 |  Val. Corr: 0.57\n",
      "Epoch: 05 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 1.294 | Train Corr: 0.85\n",
      "\t Val. Loss: 3.227 |  Val. Corr: 0.56\n",
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 24.454 | Train Corr: 0.08\n",
      "\t Val. Loss: 4.753 |  Val. Corr: 0.55\n",
      "Epoch: 01 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 3.667 | Train Corr: 0.46\n",
      "\t Val. Loss: 4.267 |  Val. Corr: 0.60\n",
      "Epoch: 02 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 3.083 | Train Corr: 0.57\n",
      "\t Val. Loss: 5.664 |  Val. Corr: 0.63\n",
      "Epoch: 03 | Epoch Time: 1m 7s\n",
      "\t Train Loss: 2.645 | Train Corr: 0.65\n",
      "\t Val. Loss: 3.155 |  Val. Corr: 0.65\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 22.700 | Train Corr: 0.09\n",
      "\t Val. Loss: 4.486 |  Val. Corr: 0.57\n",
      "Epoch: 01 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 3.567 | Train Corr: 0.51\n",
      "\t Val. Loss: 3.745 |  Val. Corr: 0.64\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 3.114 | Train Corr: 0.59\n",
      "\t Val. Loss: 2.804 |  Val. Corr: 0.80\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 2.388 | Train Corr: 0.71\n",
      "\t Val. Loss: 2.510 |  Val. Corr: 0.78\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 1.603 | Train Corr: 0.82\n",
      "\t Val. Loss: 2.237 |  Val. Corr: 0.81\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 7s\n",
      "\t Train Loss: 1.232 | Train Corr: 0.86\n",
      "\t Val. Loss: 1.525 |  Val. Corr: 0.81\n",
      "Epoch: 06 | Epoch Time: 1m 6s\n",
      "\t Train Loss: 0.819 | Train Corr: 0.91\n",
      "\t Val. Loss: 1.740 |  Val. Corr: 0.79\n",
      "Epoch: 07 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 0.709 | Train Corr: 0.92\n",
      "\t Val. Loss: 1.646 |  Val. Corr: 0.78\n",
      "Epoch: 08 | Epoch Time: 1m 5s\n",
      "\t Train Loss: 0.561 | Train Corr: 0.94\n",
      "\t Val. Loss: 1.575 |  Val. Corr: 0.79\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 4s\n",
      "\t Train Loss: 26.467 | Train Corr: 0.06\n",
      "\t Val. Loss: 4.808 |  Val. Corr: 0.48\n",
      "Epoch: 01 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 3.284 | Train Corr: 0.54\n",
      "\t Val. Loss: 3.265 |  Val. Corr: 0.67\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 21.043 | Train Corr: 0.06\n",
      "\t Val. Loss: 4.784 |  Val. Corr: 0.54\n",
      "Epoch: 01 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 3.270 | Train Corr: 0.54\n",
      "\t Val. Loss: 4.656 |  Val. Corr: 0.39\n",
      "eid 17, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 256, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 3}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 33.241 | Train Corr: 0.04\n",
      "\t Val. Loss: 3.948 |  Val. Corr: 0.50\n",
      "Epoch: 01 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 3.651 | Train Corr: 0.47\n",
      "\t Val. Loss: 5.294 |  Val. Corr: 0.17\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 2.842 | Train Corr: 0.63\n",
      "\t Val. Loss: 3.396 |  Val. Corr: 0.57\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 1.864 | Train Corr: 0.77\n",
      "\t Val. Loss: 2.893 |  Val. Corr: 0.61\n",
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 16s\n",
      "\t Train Loss: 32.127 | Train Corr: 0.03\n",
      "\t Val. Loss: 5.800 |  Val. Corr: 0.44\n",
      "Epoch: 01 | Epoch Time: 1m 16s\n",
      "\t Train Loss: 3.843 | Train Corr: 0.41\n",
      "\t Val. Loss: 5.810 |  Val. Corr: 0.51\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 14s\n",
      "\t Train Loss: 32.095 | Train Corr: 0.02\n",
      "\t Val. Loss: 3.977 |  Val. Corr: 0.47\n",
      "Epoch: 01 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 3.988 | Train Corr: 0.40\n",
      "\t Val. Loss: 4.590 |  Val. Corr: 0.37\n",
      "Epoch: 02 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 3.481 | Train Corr: 0.53\n",
      "\t Val. Loss: 4.294 |  Val. Corr: 0.55\n",
      "Epoch: 03 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 2.655 | Train Corr: 0.67\n",
      "\t Val. Loss: 2.764 |  Val. Corr: 0.64\n",
      "updating saved weights of best model\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 28.849 | Train Corr: 0.04\n",
      "\t Val. Loss: 5.243 |  Val. Corr: 0.24\n",
      "Epoch: 01 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 3.976 | Train Corr: 0.41\n",
      "\t Val. Loss: 4.000 |  Val. Corr: 0.61\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 10s\n",
      "\t Train Loss: 30.387 | Train Corr: 0.02\n",
      "\t Val. Loss: 4.878 |  Val. Corr: 0.44\n",
      "Epoch: 01 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 3.347 | Train Corr: 0.52\n",
      "\t Val. Loss: 4.350 |  Val. Corr: 0.53\n",
      "eid 18, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 512, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 1}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 13.173 | Train Corr: 0.17\n",
      "\t Val. Loss: 5.270 |  Val. Corr: 0.44\n",
      "Epoch: 01 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 3.118 | Train Corr: 0.56\n",
      "\t Val. Loss: 6.754 |  Val. Corr: 0.62\n",
      "Epoch: 02 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 2.387 | Train Corr: 0.69\n",
      "\t Val. Loss: 6.568 |  Val. Corr: 0.67\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 1.582 | Train Corr: 0.81\n",
      "\t Val. Loss: 3.062 |  Val. Corr: 0.65\n",
      "Epoch: 04 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 1.203 | Train Corr: 0.86\n",
      "\t Val. Loss: 3.900 |  Val. Corr: 0.66\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 1.036 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.970 |  Val. Corr: 0.69\n",
      "Epoch: 06 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 0.813 | Train Corr: 0.91\n",
      "\t Val. Loss: 3.864 |  Val. Corr: 0.65\n",
      "Epoch: 07 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.684 | Train Corr: 0.92\n",
      "\t Val. Loss: 3.172 |  Val. Corr: 0.66\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 13.088 | Train Corr: 0.16\n",
      "\t Val. Loss: 4.961 |  Val. Corr: 0.51\n",
      "Epoch: 01 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 3.779 | Train Corr: 0.47\n",
      "\t Val. Loss: 5.233 |  Val. Corr: 0.59\n",
      "Epoch: 02 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 2.906 | Train Corr: 0.62\n",
      "\t Val. Loss: 5.124 |  Val. Corr: 0.75\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 2.065 | Train Corr: 0.75\n",
      "\t Val. Loss: 1.827 |  Val. Corr: 0.75\n",
      "Epoch: 04 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 1.446 | Train Corr: 0.84\n",
      "\t Val. Loss: 2.798 |  Val. Corr: 0.75\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 3s\n",
      "\t Train Loss: 1.122 | Train Corr: 0.87\n",
      "\t Val. Loss: 1.795 |  Val. Corr: 0.76\n",
      "Epoch: 06 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 0.911 | Train Corr: 0.90\n",
      "\t Val. Loss: 2.569 |  Val. Corr: 0.73\n",
      "Epoch: 07 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.557 | Train Corr: 0.94\n",
      "\t Val. Loss: 2.376 |  Val. Corr: 0.72\n",
      "Epoch: 08 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.421 | Train Corr: 0.95\n",
      "\t Val. Loss: 2.202 |  Val. Corr: 0.74\n",
      "Epoch: 09 | Epoch Time: 1m 2s\n",
      "\t Train Loss: 0.347 | Train Corr: 0.96\n",
      "\t Val. Loss: 2.386 |  Val. Corr: 0.74\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 12.686 | Train Corr: 0.15\n",
      "\t Val. Loss: 4.638 |  Val. Corr: 0.50\n",
      "Epoch: 01 | Epoch Time: 0m 59s\n",
      "\t Train Loss: 2.920 | Train Corr: 0.60\n",
      "\t Val. Loss: 2.945 |  Val. Corr: 0.73\n",
      "Epoch: 02 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 2.470 | Train Corr: 0.68\n",
      "\t Val. Loss: 2.475 |  Val. Corr: 0.78\n",
      "Epoch: 03 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 1.635 | Train Corr: 0.80\n",
      "\t Val. Loss: 2.408 |  Val. Corr: 0.77\n",
      "Epoch: 04 | Epoch Time: 1m 0s\n",
      "\t Train Loss: 1.087 | Train Corr: 0.87\n",
      "\t Val. Loss: 1.806 |  Val. Corr: 0.80\n",
      "Epoch: 05 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.858 | Train Corr: 0.90\n",
      "\t Val. Loss: 1.868 |  Val. Corr: 0.80\n",
      "Epoch: 06 | Epoch Time: 1m 1s\n",
      "\t Train Loss: 0.757 | Train Corr: 0.91\n",
      "\t Val. Loss: 2.336 |  Val. Corr: 0.79\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 13.542 | Train Corr: 0.14\n",
      "\t Val. Loss: 4.525 |  Val. Corr: 0.46\n",
      "Epoch: 01 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 3.180 | Train Corr: 0.56\n",
      "\t Val. Loss: 3.773 |  Val. Corr: 0.63\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 2.673 | Train Corr: 0.65\n",
      "\t Val. Loss: 4.178 |  Val. Corr: 0.60\n",
      "Epoch: 03 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.897 | Train Corr: 0.76\n",
      "\t Val. Loss: 3.998 |  Val. Corr: 0.71\n",
      "Epoch: 04 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 1.146 | Train Corr: 0.87\n",
      "\t Val. Loss: 2.453 |  Val. Corr: 0.71\n",
      "Epoch: 05 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 0.937 | Train Corr: 0.89\n",
      "\t Val. Loss: 2.164 |  Val. Corr: 0.74\n",
      "Epoch: 06 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 0.632 | Train Corr: 0.93\n",
      "\t Val. Loss: 2.353 |  Val. Corr: 0.72\n",
      "Epoch: 07 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 0.477 | Train Corr: 0.95\n",
      "\t Val. Loss: 2.349 |  Val. Corr: 0.72\n",
      "Epoch: 08 | Epoch Time: 0m 57s\n",
      "\t Train Loss: 0.421 | Train Corr: 0.95\n",
      "\t Val. Loss: 2.460 |  Val. Corr: 0.72\n",
      "Epoch: 09 | Epoch Time: 0m 58s\n",
      "\t Train Loss: 0.486 | Train Corr: 0.95\n",
      "\t Val. Loss: 2.311 |  Val. Corr: 0.72\n",
      "eid 19, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 512, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 2}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 17.107 | Train Corr: 0.10\n",
      "\t Val. Loss: 4.747 |  Val. Corr: 0.23\n",
      "Epoch: 01 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 3.311 | Train Corr: 0.54\n",
      "\t Val. Loss: 4.941 |  Val. Corr: 0.46\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 2.803 | Train Corr: 0.64\n",
      "\t Val. Loss: 4.212 |  Val. Corr: 0.59\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 2.173 | Train Corr: 0.73\n",
      "\t Val. Loss: 3.355 |  Val. Corr: 0.57\n",
      "Epoch: 04 | Epoch Time: 1m 16s\n",
      "\t Train Loss: 1.469 | Train Corr: 0.83\n",
      "\t Val. Loss: 4.759 |  Val. Corr: 0.61\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 1.132 | Train Corr: 0.87\n",
      "\t Val. Loss: 2.924 |  Val. Corr: 0.60\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 16.903 | Train Corr: 0.12\n",
      "\t Val. Loss: 4.546 |  Val. Corr: 0.33\n",
      "Epoch: 01 | Epoch Time: 1m 18s\n",
      "\t Train Loss: 3.360 | Train Corr: 0.52\n",
      "\t Val. Loss: 6.544 |  Val. Corr: 0.52\n",
      "Epoch: 02 | Epoch Time: 1m 18s\n",
      "\t Train Loss: 3.010 | Train Corr: 0.58\n",
      "\t Val. Loss: 7.383 |  Val. Corr: 0.60\n",
      "Epoch: 03 | Epoch Time: 1m 18s\n",
      "\t Train Loss: 2.131 | Train Corr: 0.73\n",
      "\t Val. Loss: 4.416 |  Val. Corr: 0.58\n",
      "Epoch: 04 | Epoch Time: 1m 18s\n",
      "\t Train Loss: 1.684 | Train Corr: 0.80\n",
      "\t Val. Loss: 4.485 |  Val. Corr: 0.63\n",
      "Epoch: 05 | Epoch Time: 1m 19s\n",
      "\t Train Loss: 1.336 | Train Corr: 0.84\n",
      "\t Val. Loss: 3.352 |  Val. Corr: 0.63\n",
      "Epoch: 06 | Epoch Time: 1m 18s\n",
      "\t Train Loss: 1.012 | Train Corr: 0.88\n",
      "\t Val. Loss: 3.166 |  Val. Corr: 0.62\n",
      "Epoch: 07 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 0.789 | Train Corr: 0.91\n",
      "\t Val. Loss: 4.135 |  Val. Corr: 0.64\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 16s\n",
      "\t Train Loss: 17.609 | Train Corr: 0.11\n",
      "\t Val. Loss: 4.374 |  Val. Corr: 0.49\n",
      "Epoch: 01 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 3.815 | Train Corr: 0.46\n",
      "\t Val. Loss: 3.872 |  Val. Corr: 0.68\n",
      "Epoch: 02 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 3.034 | Train Corr: 0.60\n",
      "\t Val. Loss: 3.503 |  Val. Corr: 0.72\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 2.004 | Train Corr: 0.76\n",
      "\t Val. Loss: 2.191 |  Val. Corr: 0.74\n",
      "Epoch: 04 | Epoch Time: 1m 16s\n",
      "\t Train Loss: 1.311 | Train Corr: 0.85\n",
      "\t Val. Loss: 2.662 |  Val. Corr: 0.74\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 18s\n",
      "\t Train Loss: 0.916 | Train Corr: 0.90\n",
      "\t Val. Loss: 2.084 |  Val. Corr: 0.77\n",
      "Epoch: 06 | Epoch Time: 1m 18s\n",
      "\t Train Loss: 0.673 | Train Corr: 0.93\n",
      "\t Val. Loss: 2.314 |  Val. Corr: 0.73\n",
      "updating saved weights of best model\n",
      "Epoch: 07 | Epoch Time: 1m 17s\n",
      "\t Train Loss: 0.496 | Train Corr: 0.95\n",
      "\t Val. Loss: 1.951 |  Val. Corr: 0.74\n",
      "Epoch: 08 | Epoch Time: 1m 16s\n",
      "\t Train Loss: 0.400 | Train Corr: 0.96\n",
      "\t Val. Loss: 2.512 |  Val. Corr: 0.72\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 16.407 | Train Corr: 0.14\n",
      "\t Val. Loss: 4.711 |  Val. Corr: 0.49\n",
      "Epoch: 01 | Epoch Time: 1m 14s\n",
      "\t Train Loss: 3.093 | Train Corr: 0.57\n",
      "\t Val. Loss: 3.687 |  Val. Corr: 0.60\n",
      "Epoch: 02 | Epoch Time: 1m 16s\n",
      "\t Train Loss: 2.703 | Train Corr: 0.64\n",
      "\t Val. Loss: 2.585 |  Val. Corr: 0.72\n",
      "Epoch: 03 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 2.382 | Train Corr: 0.70\n",
      "\t Val. Loss: 3.552 |  Val. Corr: 0.72\n",
      "Epoch: 04 | Epoch Time: 1m 15s\n",
      "\t Train Loss: 1.373 | Train Corr: 0.84\n",
      "\t Val. Loss: 2.430 |  Val. Corr: 0.76\n",
      "Epoch: 05 | Epoch Time: 1m 16s\n",
      "\t Train Loss: 0.952 | Train Corr: 0.89\n",
      "\t Val. Loss: 2.487 |  Val. Corr: 0.75\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 16.094 | Train Corr: 0.13\n",
      "\t Val. Loss: 4.215 |  Val. Corr: 0.45\n",
      "Epoch: 01 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 3.097 | Train Corr: 0.57\n",
      "\t Val. Loss: 3.702 |  Val. Corr: 0.56\n",
      "Epoch: 02 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 2.757 | Train Corr: 0.63\n",
      "\t Val. Loss: 3.214 |  Val. Corr: 0.66\n",
      "Epoch: 03 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 1.818 | Train Corr: 0.78\n",
      "\t Val. Loss: 3.291 |  Val. Corr: 0.67\n",
      "Epoch: 04 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 1.306 | Train Corr: 0.85\n",
      "\t Val. Loss: 2.864 |  Val. Corr: 0.65\n",
      "Epoch: 05 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 1.064 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.616 |  Val. Corr: 0.70\n",
      "Epoch: 06 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 0.705 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.866 |  Val. Corr: 0.62\n",
      "eid 20, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 512, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 3}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 33s\n",
      "\t Train Loss: 20.477 | Train Corr: 0.03\n",
      "\t Val. Loss: 4.114 |  Val. Corr: 0.43\n",
      "Epoch: 01 | Epoch Time: 1m 33s\n",
      "\t Train Loss: 3.749 | Train Corr: 0.44\n",
      "\t Val. Loss: 4.286 |  Val. Corr: 0.40\n",
      "Epoch: 02 | Epoch Time: 1m 33s\n",
      "\t Train Loss: 3.318 | Train Corr: 0.54\n",
      "\t Val. Loss: 5.586 |  Val. Corr: 0.39\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 33s\n",
      "\t Train Loss: 20.651 | Train Corr: 0.05\n",
      "\t Val. Loss: 5.521 |  Val. Corr: 0.38\n",
      "Epoch: 01 | Epoch Time: 1m 33s\n",
      "\t Train Loss: 3.706 | Train Corr: 0.44\n",
      "\t Val. Loss: 7.047 |  Val. Corr: 0.40\n",
      "Epoch: 02 | Epoch Time: 1m 33s\n",
      "\t Train Loss: 3.210 | Train Corr: 0.54\n",
      "\t Val. Loss: 7.648 |  Val. Corr: 0.31\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 32s\n",
      "\t Train Loss: 20.302 | Train Corr: 0.08\n",
      "\t Val. Loss: 3.876 |  Val. Corr: 0.53\n",
      "Epoch: 01 | Epoch Time: 1m 33s\n",
      "\t Train Loss: 4.008 | Train Corr: 0.42\n",
      "\t Val. Loss: 4.462 |  Val. Corr: 0.58\n",
      "Epoch: 02 | Epoch Time: 1m 33s\n",
      "\t Train Loss: 3.304 | Train Corr: 0.55\n",
      "\t Val. Loss: 4.211 |  Val. Corr: 0.72\n",
      "updating saved weights of best model\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 30s\n",
      "\t Train Loss: 20.163 | Train Corr: 0.08\n",
      "\t Val. Loss: 4.305 |  Val. Corr: 0.56\n",
      "Epoch: 01 | Epoch Time: 1m 29s\n",
      "\t Train Loss: 3.311 | Train Corr: 0.53\n",
      "\t Val. Loss: 3.566 |  Val. Corr: 0.64\n",
      "Epoch: 02 | Epoch Time: 1m 31s\n",
      "\t Train Loss: 2.938 | Train Corr: 0.60\n",
      "\t Val. Loss: 3.581 |  Val. Corr: 0.69\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 31s\n",
      "\t Train Loss: 1.911 | Train Corr: 0.76\n",
      "\t Val. Loss: 2.488 |  Val. Corr: 0.72\n",
      "updating saved weights of best model\n",
      "Epoch: 04 | Epoch Time: 1m 30s\n",
      "\t Train Loss: 1.403 | Train Corr: 0.83\n",
      "\t Val. Loss: 2.077 |  Val. Corr: 0.76\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 32s\n",
      "\t Train Loss: 1.028 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.058 |  Val. Corr: 0.76\n",
      "Epoch: 06 | Epoch Time: 1m 32s\n",
      "\t Train Loss: 0.719 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.502 |  Val. Corr: 0.76\n",
      "Epoch: 07 | Epoch Time: 1m 32s\n",
      "\t Train Loss: 0.530 | Train Corr: 0.94\n",
      "\t Val. Loss: 2.304 |  Val. Corr: 0.74\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 27s\n",
      "\t Train Loss: 19.962 | Train Corr: 0.10\n",
      "\t Val. Loss: 4.440 |  Val. Corr: 0.50\n",
      "Epoch: 01 | Epoch Time: 1m 28s\n",
      "\t Train Loss: 3.283 | Train Corr: 0.53\n",
      "\t Val. Loss: 3.710 |  Val. Corr: 0.54\n",
      "Epoch: 02 | Epoch Time: 1m 26s\n",
      "\t Train Loss: 2.726 | Train Corr: 0.64\n",
      "\t Val. Loss: 3.072 |  Val. Corr: 0.64\n",
      "Epoch: 03 | Epoch Time: 1m 27s\n",
      "\t Train Loss: 1.925 | Train Corr: 0.76\n",
      "\t Val. Loss: 3.089 |  Val. Corr: 0.69\n",
      "Epoch: 04 | Epoch Time: 1m 26s\n",
      "\t Train Loss: 1.535 | Train Corr: 0.82\n",
      "\t Val. Loss: 2.676 |  Val. Corr: 0.70\n",
      "Epoch: 05 | Epoch Time: 1m 28s\n",
      "\t Train Loss: 1.048 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.386 |  Val. Corr: 0.71\n",
      "Epoch: 06 | Epoch Time: 1m 26s\n",
      "\t Train Loss: 0.704 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.717 |  Val. Corr: 0.65\n",
      "Epoch: 07 | Epoch Time: 1m 28s\n",
      "\t Train Loss: 0.572 | Train Corr: 0.94\n",
      "\t Val. Loss: 2.563 |  Val. Corr: 0.69\n",
      "Epoch: 08 | Epoch Time: 1m 26s\n",
      "\t Train Loss: 0.461 | Train Corr: 0.95\n",
      "\t Val. Loss: 2.415 |  Val. Corr: 0.72\n",
      "Epoch: 09 | Epoch Time: 1m 28s\n",
      "\t Train Loss: 0.408 | Train Corr: 0.95\n",
      "\t Val. Loss: 2.308 |  Val. Corr: 0.72\n",
      "eid 21, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 768, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 1}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 11.279 | Train Corr: 0.20\n",
      "\t Val. Loss: 4.554 |  Val. Corr: 0.16\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 3.085 | Train Corr: 0.58\n",
      "\t Val. Loss: 4.331 |  Val. Corr: 0.45\n",
      "Epoch: 02 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 2.344 | Train Corr: 0.71\n",
      "\t Val. Loss: 6.668 |  Val. Corr: 0.53\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 1.785 | Train Corr: 0.78\n",
      "\t Val. Loss: 3.230 |  Val. Corr: 0.54\n",
      "Epoch: 04 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 1.064 | Train Corr: 0.88\n",
      "\t Val. Loss: 3.406 |  Val. Corr: 0.59\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 11.178 | Train Corr: 0.17\n",
      "\t Val. Loss: 4.841 |  Val. Corr: 0.41\n",
      "Epoch: 01 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 3.359 | Train Corr: 0.51\n",
      "\t Val. Loss: 6.417 |  Val. Corr: 0.52\n",
      "Epoch: 02 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 2.960 | Train Corr: 0.59\n",
      "\t Val. Loss: 6.613 |  Val. Corr: 0.66\n",
      "Epoch: 03 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 1.968 | Train Corr: 0.75\n",
      "\t Val. Loss: 3.416 |  Val. Corr: 0.66\n",
      "Epoch: 04 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 1.140 | Train Corr: 0.87\n",
      "\t Val. Loss: 4.618 |  Val. Corr: 0.69\n",
      "Epoch: 05 | Epoch Time: 1m 14s\n",
      "\t Train Loss: 0.808 | Train Corr: 0.91\n",
      "\t Val. Loss: 4.033 |  Val. Corr: 0.66\n",
      "updating saved weights of best model\n",
      "Epoch: 06 | Epoch Time: 1m 14s\n",
      "\t Train Loss: 0.609 | Train Corr: 0.93\n",
      "\t Val. Loss: 3.182 |  Val. Corr: 0.64\n",
      "updating saved weights of best model\n",
      "Epoch: 07 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 0.486 | Train Corr: 0.94\n",
      "\t Val. Loss: 2.821 |  Val. Corr: 0.68\n",
      "Epoch: 08 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 0.400 | Train Corr: 0.96\n",
      "\t Val. Loss: 2.918 |  Val. Corr: 0.66\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 12.364 | Train Corr: 0.18\n",
      "\t Val. Loss: 3.720 |  Val. Corr: 0.61\n",
      "Epoch: 01 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 3.404 | Train Corr: 0.54\n",
      "\t Val. Loss: 3.546 |  Val. Corr: 0.72\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 2.466 | Train Corr: 0.70\n",
      "\t Val. Loss: 1.850 |  Val. Corr: 0.77\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 1.691 | Train Corr: 0.80\n",
      "\t Val. Loss: 1.690 |  Val. Corr: 0.79\n",
      "Epoch: 04 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 1.144 | Train Corr: 0.87\n",
      "\t Val. Loss: 1.820 |  Val. Corr: 0.81\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 14s\n",
      "\t Train Loss: 0.796 | Train Corr: 0.91\n",
      "\t Val. Loss: 1.643 |  Val. Corr: 0.80\n",
      "Epoch: 06 | Epoch Time: 1m 13s\n",
      "\t Train Loss: 0.502 | Train Corr: 0.95\n",
      "\t Val. Loss: 1.701 |  Val. Corr: 0.79\n",
      "updating saved weights of best model\n",
      "Epoch: 07 | Epoch Time: 1m 12s\n",
      "\t Train Loss: 0.394 | Train Corr: 0.96\n",
      "\t Val. Loss: 1.594 |  Val. Corr: 0.80\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 11.204 | Train Corr: 0.21\n",
      "\t Val. Loss: 3.874 |  Val. Corr: 0.53\n",
      "Epoch: 01 | Epoch Time: 1m 10s\n",
      "\t Train Loss: 2.750 | Train Corr: 0.63\n",
      "\t Val. Loss: 2.046 |  Val. Corr: 0.76\n",
      "Epoch: 02 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 2.075 | Train Corr: 0.74\n",
      "\t Val. Loss: 2.664 |  Val. Corr: 0.75\n",
      "Epoch: 03 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 1.387 | Train Corr: 0.83\n",
      "\t Val. Loss: 1.998 |  Val. Corr: 0.78\n",
      "Epoch: 04 | Epoch Time: 1m 11s\n",
      "\t Train Loss: 0.808 | Train Corr: 0.91\n",
      "\t Val. Loss: 2.002 |  Val. Corr: 0.77\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 8s\n",
      "\t Train Loss: 11.496 | Train Corr: 0.19\n",
      "\t Val. Loss: 3.930 |  Val. Corr: 0.60\n",
      "Epoch: 01 | Epoch Time: 1m 8s\n",
      "\t Train Loss: 2.902 | Train Corr: 0.60\n",
      "\t Val. Loss: 3.669 |  Val. Corr: 0.67\n",
      "Epoch: 02 | Epoch Time: 1m 7s\n",
      "\t Train Loss: 2.149 | Train Corr: 0.73\n",
      "\t Val. Loss: 2.696 |  Val. Corr: 0.71\n",
      "Epoch: 03 | Epoch Time: 1m 8s\n",
      "\t Train Loss: 1.482 | Train Corr: 0.82\n",
      "\t Val. Loss: 2.549 |  Val. Corr: 0.70\n",
      "Epoch: 04 | Epoch Time: 1m 7s\n",
      "\t Train Loss: 0.878 | Train Corr: 0.90\n",
      "\t Val. Loss: 3.904 |  Val. Corr: 0.66\n",
      "Epoch: 05 | Epoch Time: 1m 8s\n",
      "\t Train Loss: 0.700 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.281 |  Val. Corr: 0.72\n",
      "Epoch: 06 | Epoch Time: 1m 7s\n",
      "\t Train Loss: 0.610 | Train Corr: 0.93\n",
      "\t Val. Loss: 2.710 |  Val. Corr: 0.70\n",
      "Epoch: 07 | Epoch Time: 1m 9s\n",
      "\t Train Loss: 0.480 | Train Corr: 0.95\n",
      "\t Val. Loss: 2.398 |  Val. Corr: 0.70\n",
      "Epoch: 08 | Epoch Time: 1m 7s\n",
      "\t Train Loss: 0.355 | Train Corr: 0.96\n",
      "\t Val. Loss: 2.669 |  Val. Corr: 0.68\n",
      "Epoch: 09 | Epoch Time: 1m 9s\n",
      "\t Train Loss: 0.406 | Train Corr: 0.95\n",
      "\t Val. Loss: 2.398 |  Val. Corr: 0.71\n",
      "eid 22, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 768, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 2}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 1m 41s\n",
      "\t Train Loss: 13.890 | Train Corr: 0.19\n",
      "\t Val. Loss: 4.120 |  Val. Corr: 0.33\n",
      "Epoch: 01 | Epoch Time: 1m 41s\n",
      "\t Train Loss: 3.407 | Train Corr: 0.53\n",
      "\t Val. Loss: 4.965 |  Val. Corr: 0.42\n",
      "updating saved weights of best model\n",
      "Epoch: 02 | Epoch Time: 1m 41s\n",
      "\t Train Loss: 2.802 | Train Corr: 0.63\n",
      "\t Val. Loss: 3.837 |  Val. Corr: 0.59\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 41s\n",
      "\t Train Loss: 2.029 | Train Corr: 0.75\n",
      "\t Val. Loss: 3.029 |  Val. Corr: 0.59\n",
      "Epoch: 04 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 1.123 | Train Corr: 0.87\n",
      "\t Val. Loss: 3.283 |  Val. Corr: 0.64\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 41s\n",
      "\t Train Loss: 0.844 | Train Corr: 0.91\n",
      "\t Val. Loss: 2.807 |  Val. Corr: 0.64\n",
      "Epoch: 06 | Epoch Time: 1m 42s\n",
      "\t Train Loss: 0.687 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.975 |  Val. Corr: 0.63\n",
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 42s\n",
      "\t Train Loss: 14.259 | Train Corr: 0.15\n",
      "\t Val. Loss: 5.593 |  Val. Corr: 0.26\n",
      "Epoch: 01 | Epoch Time: 1m 42s\n",
      "\t Train Loss: 3.474 | Train Corr: 0.50\n",
      "\t Val. Loss: 7.470 |  Val. Corr: 0.60\n",
      "Epoch: 02 | Epoch Time: 1m 41s\n",
      "\t Train Loss: 2.650 | Train Corr: 0.65\n",
      "\t Val. Loss: 6.289 |  Val. Corr: 0.68\n",
      "Epoch: 03 | Epoch Time: 1m 42s\n",
      "\t Train Loss: 1.698 | Train Corr: 0.79\n",
      "\t Val. Loss: 3.254 |  Val. Corr: 0.64\n",
      "Epoch: 04 | Epoch Time: 1m 42s\n",
      "\t Train Loss: 1.126 | Train Corr: 0.87\n",
      "\t Val. Loss: 3.428 |  Val. Corr: 0.69\n",
      "Epoch: 05 | Epoch Time: 1m 43s\n",
      "\t Train Loss: 0.845 | Train Corr: 0.90\n",
      "\t Val. Loss: 4.971 |  Val. Corr: 0.69\n",
      "Epoch: 06 | Epoch Time: 1m 43s\n",
      "\t Train Loss: 0.676 | Train Corr: 0.92\n",
      "\t Val. Loss: 3.332 |  Val. Corr: 0.66\n",
      "Epoch: 07 | Epoch Time: 1m 41s\n",
      "\t Train Loss: 0.470 | Train Corr: 0.95\n",
      "\t Val. Loss: 4.072 |  Val. Corr: 0.68\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 13.902 | Train Corr: 0.15\n",
      "\t Val. Loss: 3.697 |  Val. Corr: 0.50\n",
      "Epoch: 01 | Epoch Time: 1m 41s\n",
      "\t Train Loss: 3.473 | Train Corr: 0.52\n",
      "\t Val. Loss: 4.315 |  Val. Corr: 0.47\n",
      "Epoch: 02 | Epoch Time: 1m 42s\n",
      "\t Train Loss: 2.982 | Train Corr: 0.61\n",
      "\t Val. Loss: 5.274 |  Val. Corr: 0.70\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 1m 41s\n",
      "\t Train Loss: 2.172 | Train Corr: 0.74\n",
      "\t Val. Loss: 2.160 |  Val. Corr: 0.76\n",
      "Epoch: 04 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 1.483 | Train Corr: 0.83\n",
      "\t Val. Loss: 2.744 |  Val. Corr: 0.77\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 1m 43s\n",
      "\t Train Loss: 1.010 | Train Corr: 0.89\n",
      "\t Val. Loss: 1.941 |  Val. Corr: 0.77\n",
      "Epoch: 06 | Epoch Time: 1m 42s\n",
      "\t Train Loss: 0.761 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.206 |  Val. Corr: 0.75\n",
      "Epoch: 07 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 0.558 | Train Corr: 0.94\n",
      "\t Val. Loss: 2.523 |  Val. Corr: 0.74\n",
      "Epoch: 08 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 0.360 | Train Corr: 0.96\n",
      "\t Val. Loss: 2.147 |  Val. Corr: 0.70\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 39s\n",
      "\t Train Loss: 13.947 | Train Corr: 0.17\n",
      "\t Val. Loss: 3.794 |  Val. Corr: 0.54\n",
      "Epoch: 01 | Epoch Time: 1m 37s\n",
      "\t Train Loss: 3.087 | Train Corr: 0.57\n",
      "\t Val. Loss: 3.702 |  Val. Corr: 0.66\n",
      "Epoch: 02 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 2.594 | Train Corr: 0.66\n",
      "\t Val. Loss: 2.653 |  Val. Corr: 0.74\n",
      "Epoch: 03 | Epoch Time: 1m 39s\n",
      "\t Train Loss: 1.979 | Train Corr: 0.75\n",
      "\t Val. Loss: 2.354 |  Val. Corr: 0.76\n",
      "Epoch: 04 | Epoch Time: 1m 39s\n",
      "\t Train Loss: 1.207 | Train Corr: 0.86\n",
      "\t Val. Loss: 2.123 |  Val. Corr: 0.76\n",
      "Epoch: 05 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 1.041 | Train Corr: 0.88\n",
      "\t Val. Loss: 2.171 |  Val. Corr: 0.77\n",
      "Epoch: 06 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 0.703 | Train Corr: 0.92\n",
      "\t Val. Loss: 2.236 |  Val. Corr: 0.75\n",
      "Epoch: 07 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 0.521 | Train Corr: 0.94\n",
      "\t Val. Loss: 2.314 |  Val. Corr: 0.78\n",
      "Epoch: 08 | Epoch Time: 1m 37s\n",
      "\t Train Loss: 0.447 | Train Corr: 0.95\n",
      "\t Val. Loss: 2.179 |  Val. Corr: 0.77\n",
      "Epoch: 09 | Epoch Time: 1m 40s\n",
      "\t Train Loss: 0.352 | Train Corr: 0.96\n",
      "\t Val. Loss: 2.011 |  Val. Corr: 0.79\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 1m 35s\n",
      "\t Train Loss: 14.068 | Train Corr: 0.15\n",
      "\t Val. Loss: 4.684 |  Val. Corr: 0.43\n",
      "Epoch: 01 | Epoch Time: 1m 36s\n",
      "\t Train Loss: 3.170 | Train Corr: 0.56\n",
      "\t Val. Loss: 3.753 |  Val. Corr: 0.58\n",
      "Epoch: 02 | Epoch Time: 1m 34s\n",
      "\t Train Loss: 2.704 | Train Corr: 0.65\n",
      "\t Val. Loss: 3.139 |  Val. Corr: 0.71\n",
      "Epoch: 03 | Epoch Time: 1m 34s\n",
      "\t Train Loss: 1.759 | Train Corr: 0.78\n",
      "\t Val. Loss: 3.400 |  Val. Corr: 0.69\n",
      "Epoch: 04 | Epoch Time: 1m 34s\n",
      "\t Train Loss: 1.076 | Train Corr: 0.87\n",
      "\t Val. Loss: 2.774 |  Val. Corr: 0.67\n",
      "eid 23, params {'batch_size': 8, 'bidirectional': True, 'dropout': 0.2, 'hidden_dim': 768, 'lr': 5e-05, 'max_epochs': 10, 'num_layers': 3}\n",
      "training on fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating saved weights of best model\n",
      "Epoch: 00 | Epoch Time: 2m 9s\n",
      "\t Train Loss: 15.640 | Train Corr: 0.14\n",
      "\t Val. Loss: 4.321 |  Val. Corr: 0.37\n",
      "updating saved weights of best model\n",
      "Epoch: 01 | Epoch Time: 2m 9s\n",
      "\t Train Loss: 3.224 | Train Corr: 0.56\n",
      "\t Val. Loss: 3.404 |  Val. Corr: 0.57\n",
      "Epoch: 02 | Epoch Time: 2m 9s\n",
      "\t Train Loss: 2.388 | Train Corr: 0.70\n",
      "\t Val. Loss: 4.137 |  Val. Corr: 0.56\n",
      "updating saved weights of best model\n",
      "Epoch: 03 | Epoch Time: 2m 9s\n",
      "\t Train Loss: 1.699 | Train Corr: 0.80\n",
      "\t Val. Loss: 2.903 |  Val. Corr: 0.59\n",
      "Epoch: 04 | Epoch Time: 2m 8s\n",
      "\t Train Loss: 1.211 | Train Corr: 0.86\n",
      "\t Val. Loss: 2.911 |  Val. Corr: 0.59\n",
      "updating saved weights of best model\n",
      "training on fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 2m 10s\n",
      "\t Train Loss: 16.077 | Train Corr: 0.11\n",
      "\t Val. Loss: 4.789 |  Val. Corr: 0.42\n",
      "Epoch: 01 | Epoch Time: 2m 10s\n",
      "\t Train Loss: 3.624 | Train Corr: 0.47\n",
      "\t Val. Loss: 7.604 |  Val. Corr: 0.37\n",
      "Epoch: 02 | Epoch Time: 2m 10s\n",
      "\t Train Loss: 3.175 | Train Corr: 0.55\n",
      "\t Val. Loss: 7.242 |  Val. Corr: 0.60\n",
      "training on fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 2m 8s\n",
      "\t Train Loss: 16.953 | Train Corr: 0.05\n",
      "\t Val. Loss: 4.085 |  Val. Corr: 0.20\n",
      "Epoch: 01 | Epoch Time: 2m 10s\n",
      "\t Train Loss: 4.719 | Train Corr: 0.22\n",
      "\t Val. Loss: 4.356 |  Val. Corr: 0.40\n",
      "Epoch: 02 | Epoch Time: 2m 10s\n",
      "\t Train Loss: 3.683 | Train Corr: 0.48\n",
      "\t Val. Loss: 5.084 |  Val. Corr: 0.44\n",
      "Epoch: 03 | Epoch Time: 2m 10s\n",
      "\t Train Loss: 3.278 | Train Corr: 0.56\n",
      "\t Val. Loss: 4.013 |  Val. Corr: 0.50\n",
      "training on fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 2m 6s\n",
      "\t Train Loss: 16.490 | Train Corr: 0.08\n",
      "\t Val. Loss: 5.250 |  Val. Corr: 0.25\n",
      "Epoch: 01 | Epoch Time: 2m 5s\n",
      "\t Train Loss: 3.378 | Train Corr: 0.51\n",
      "\t Val. Loss: 4.898 |  Val. Corr: 0.28\n",
      "training on fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Epoch Time: 2m 2s\n",
      "\t Train Loss: 16.431 | Train Corr: 0.07\n",
      "\t Val. Loss: 5.157 |  Val. Corr: 0.44\n",
      "Epoch: 01 | Epoch Time: 2m 3s\n",
      "\t Train Loss: 3.308 | Train Corr: 0.53\n",
      "\t Val. Loss: 4.068 |  Val. Corr: 0.49\n",
      "Epoch: 02 | Epoch Time: 2m 1s\n",
      "\t Train Loss: 2.607 | Train Corr: 0.66\n",
      "\t Val. Loss: 4.563 |  Val. Corr: 0.59\n",
      "Epoch: 03 | Epoch Time: 2m 1s\n",
      "\t Train Loss: 1.976 | Train Corr: 0.75\n",
      "\t Val. Loss: 5.861 |  Val. Corr: 0.55\n",
      "Epoch: 04 | Epoch Time: 2m 1s\n",
      "\t Train Loss: 1.308 | Train Corr: 0.85\n",
      "\t Val. Loss: 3.163 |  Val. Corr: 0.63\n",
      "updating saved weights of best model\n",
      "Epoch: 05 | Epoch Time: 2m 3s\n",
      "\t Train Loss: 0.973 | Train Corr: 0.89\n",
      "\t Val. Loss: 2.697 |  Val. Corr: 0.70\n",
      "('batch_size_8; bidirectional_True; dropout_0.2; hidden_dim_768; lr_5e-05; max_epochs_10; num_layers_1', 0.6996348896257089)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "param_grid = {\n",
    "    'dropout': [0.2],\n",
    "    'batch_size': [8],\n",
    "    'max_epochs': [10],\n",
    "    'lr': [5e-05],\n",
    "    'hidden_dim': [128,256,512,768],\n",
    "    'num_layers': [1,2,3],\n",
    "    'bidirectional': [False,True],\n",
    "}\n",
    "\n",
    "results, best_model = training.perform_hyperparameter_search(param_grid, train_exs_arr, rnn=True, save_weights=True)\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIXP6rB_FY8K"
   },
   "source": [
    "# Test the trained model on held-out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-1w-zVceFsx3"
   },
   "outputs": [],
   "source": [
    "# Get a test iterator\n",
    "# use batch size from best params!!\n",
    "test_iterator = training.get_iterator(test_dataset, 8, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1621580211381,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "XgAhdm2rTGrP",
    "outputId": "4e305da0-ec5d-4406-9fb9-7f634df6c507"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: tensor([4.3092, 2.5972, 4.9821, 3.7691, 4.0425, 4.0092, 4.1976, 3.6279],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.8000, 2.7500, 5.3250, 3.9250, 4.7250, 5.2000, 4.7500, 4.7250],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([4.4185, 4.0181, 3.9511, 4.4189, 4.6874, 2.4615, 4.5022, 3.6038],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.4500, 4.5000, 4.2750, 4.3750, 4.8750, 3.3250, 4.8000, 3.7250],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([4.6404, 3.5096, 3.9365, 3.2364, 3.2019, 4.2997, 3.9174, 2.8325],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([5.0000, 3.3500, 4.0250, 4.1000, 4.9000, 4.3000, 3.1750, 2.6750],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.5482, 3.5646, 4.7370, 2.4181, 4.3563, 3.4085, 3.8179, 3.4318],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.8250, 4.1500, 5.2750, 2.6500, 5.0000, 4.7000, 4.3000, 3.7750],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.8663, 3.7502, 3.8978, 3.4236, 4.2574, 4.9229, 4.0129, 4.6737],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([3.1500, 3.7750, 4.9000, 3.9500, 4.2250, 4.7250, 4.9250, 4.9000],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.1987, 3.2017, 4.0737, 3.0965, 3.8119, 3.7326, 4.4747, 4.4015],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([3.8750, 4.5500, 4.0250, 2.9000, 5.0750, 4.5750, 5.0500, 4.6500],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([2.9702, 4.3608, 3.7866, 2.5064, 3.8155, 4.5996, 4.1313, 3.7940],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.0500, 4.7750, 4.2500, 1.8250, 3.8750, 3.8250, 4.6000, 4.0500],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.8346, 3.5420, 3.6046, 4.5945, 4.7478, 4.5304, 2.7201, 3.4985],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.6000, 3.9500, 4.0750, 4.5000, 5.5000, 4.4750, 3.3000, 2.9250],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([4.5688, 3.5067, 3.6246, 4.4407, 4.6634, 3.3046, 4.5411, 4.4159],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.8500, 3.9750, 4.7000, 3.5500, 4.5500, 4.0750, 4.7000, 4.7000],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.6790, 3.1435, 2.7459, 4.3951, 3.6908, 3.7777, 3.5558, 4.3580],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([3.3000, 4.3000, 4.0500, 4.6000, 3.8750, 4.1000, 3.7000, 5.0250],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([4.5027, 3.9379, 2.5096], device='cuda:0')\n",
      "true labels: tensor([4.5250, 4.7000, 2.5250], device='cuda:0')\n",
      "predictions: tensor([3.7464, 3.5150, 3.6372, 4.0706, 4.0866, 3.8210, 3.5661, 3.7780],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([3.8000, 3.5500, 3.6750, 4.9500, 5.1000, 4.4500, 5.0250, 5.2750],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.9019, 3.3105, 4.0496, 3.6380, 4.3973, 3.9670, 4.1428, 3.1121],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([3.9000, 3.8250, 4.7750, 4.7750, 5.0500, 4.3250, 5.0750, 3.1750],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.7145, 3.0152, 4.0382, 3.8633, 4.4692, 3.9151, 4.0691, 4.1985],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([3.3250, 4.4500, 4.7250, 4.2250, 4.7000, 4.8500, 3.8000, 4.5750],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([2.4525, 4.7103, 4.3873, 3.6642, 3.2185, 4.0406, 4.0360, 3.7420],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([2.0750, 5.2500, 5.0250, 3.7250, 3.6000, 4.2250, 5.0500, 3.5500],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.7059, 4.0052, 4.7788, 4.2282, 3.3843, 3.6957, 4.1332, 3.3057],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.1250, 4.3500, 4.7500, 5.2750, 3.0250, 4.6000, 4.5000, 3.4750],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([4.4290, 3.7980, 3.4572, 2.9452, 3.9538, 3.5708, 2.3388, 3.6600],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.8750, 4.4500, 3.5000, 3.8250, 4.0250, 3.8500, 3.4000, 4.0250],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.9488, 4.1736, 3.3003, 3.6435, 4.1705, 3.2218, 4.3165, 4.0453],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.5500, 4.6000, 4.6000, 4.3250, 5.2500, 3.3250, 4.9000, 4.6250],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([4.1292, 3.1448, 3.6964, 4.1883, 4.5388, 4.1625, 2.6336, 4.0216],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.3750, 2.7250, 4.3750, 4.6000, 4.5750, 5.1750, 3.6750, 4.2000],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.3845, 3.3739, 4.3259, 4.2323, 4.0786, 3.9330, 4.6502, 4.7125],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([3.8500, 3.5500, 5.4250, 4.7250, 4.0500, 4.1000, 5.1500, 5.1250],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.0159, 4.6685, 3.7356, 3.5518, 3.9106, 3.3193, 3.0341, 3.4093],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([2.1750, 4.7750, 3.3750, 4.6000, 4.7250, 3.9750, 2.8500, 2.6250],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.8156, 4.5837, 3.8050, 3.9286, 4.3320, 4.1736, 4.1397, 4.1212],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.8500, 5.5250, 4.5500, 4.6500, 3.2750, 4.3500, 4.8500, 4.2000],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([4.2656, 2.7562, 4.3539, 3.7897, 4.1695, 4.1315, 3.9900, 3.7035],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.2500, 4.0500, 5.0500, 4.1250, 4.1750, 4.8000, 4.8750, 4.1250],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.6401, 3.6404, 4.2887, 4.3262, 3.5170, 4.6684, 2.4365, 3.3867],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([3.8750, 3.8750, 4.7500, 4.3500, 4.4250, 5.2750, 2.8500, 2.7750],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([4.8871, 4.2194, 3.9834, 3.4067, 3.7614, 2.7377, 3.6209, 3.2689],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([5.1500, 4.5000, 3.5000, 4.0250, 4.5500, 2.5500, 4.4000, 4.6750],\n",
      "       device='cuda:0')\n",
      "predictions: tensor([3.9044, 4.3816, 4.1317, 3.9065, 4.4806, 3.5390, 4.6812, 3.6610],\n",
      "       device='cuda:0')\n",
      "true labels: tensor([4.1500, 4.2500, 4.7500, 5.0500, 5.3250, 2.9750, 4.3250, 4.6750],\n",
      "       device='cuda:0')\n",
      "3.3660289576420417\n",
      "0.6992915015370212\n"
     ]
    }
   ],
   "source": [
    "# load the best model saved\n",
    "bert = DistilBertModel.from_pretrained(constants.WEIGHTS_NAME)\n",
    "# use the params from the best model!!! \n",
    "model = models.BERTRNN(bert, constants.OUTPUT_DIM, 768, 1, True, 0.2)\n",
    "model.load_state_dict(torch.load(\"21_best_valid_loss.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# If you change the criterion, make sure it matches with the training criterion in training.py\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "criterion = criterion.to(device)\n",
    "test_loss, test_corr = training.evaluate(model, test_iterator, criterion, debug=True)\n",
    "print(test_loss)\n",
    "print(test_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5g0MO_uYqn7"
   },
   "source": [
    "# Misc other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IebHhmzl_anV"
   },
   "source": [
    "Link to the trainer class: https://huggingface.co/transformers/main_classes/trainer.html\n",
    "\n",
    "\n",
    "\n",
    "Default training arguments: https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments\n",
    "\n",
    "Batch size per device: 8\n",
    "\n",
    "Epoch: 3\n",
    "\n",
    "\n",
    "\n",
    "This should be the model I used to generate my initial results: https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification\n",
    "\"DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Regression_pipeline.ipynb",
   "provenance": [
    {
     "file_id": "1TbT1oPbbeEW7w-1XoUmceB67Z6hgJv9L",
     "timestamp": 1621569604758
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "082ca58e4f2f42d7af0230ed3b5c40e4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0feedfdfac074b0fb7a7950a8a65fe6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64b90e94382b477cad9365457ce1476f",
      "placeholder": "",
      "style": "IPY_MODEL_b7ce597d0d0741f5aca0711118f7c9f0",
      "value": " 442/442 [00:00&lt;00:00, 1.22kB/s]"
     }
    },
    "1397386380fe4080ad53a54958c2fbe6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15f6d4fd47d0447597adc90e439d2483": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2200957fd62b45999a0bf09d10aafd09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca00d8df97514714ba60deb51512397f",
       "IPY_MODEL_be5d412ffb7b439f977f29c84fef6974"
      ],
      "layout": "IPY_MODEL_2fd29741371347b29d3ab79702293f89"
     }
    },
    "2f9ea867a0594f9f98d88b79106f03a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_382c65ed728d4b2ca46a7b83468c6d73",
      "placeholder": "",
      "style": "IPY_MODEL_f0b8894c06e5406098b1b70403041316",
      "value": " 268M/268M [00:08&lt;00:00, 31.6MB/s]"
     }
    },
    "2fd29741371347b29d3ab79702293f89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35389ca9c4134cd2bd3db516a30f805b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3548e016675142abb7dc0200635130b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "382c65ed728d4b2ca46a7b83468c6d73": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5290ba323ad94ad3b3a02d291fed4cb0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57d1101e366c43e488111e81e56639f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d0a6d051ee204c33a851ed98dc6cbf33",
       "IPY_MODEL_746f4e383e0047b28e64d9d93fffe0aa"
      ],
      "layout": "IPY_MODEL_c60890f71d0241f698caf3304e727668"
     }
    },
    "5d4d62208b0e45b5a063e2d85ab07ce1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5f785c79f13a48099bbb1477df8aa16d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_082ca58e4f2f42d7af0230ed3b5c40e4",
      "max": 442,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_35389ca9c4134cd2bd3db516a30f805b",
      "value": 442
     }
    },
    "64b90e94382b477cad9365457ce1476f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e67df8d4c044ab0b4e11d19b7183d30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d6916f93ffc41cca2bf8be42251ab5f",
       "IPY_MODEL_2f9ea867a0594f9f98d88b79106f03a5"
      ],
      "layout": "IPY_MODEL_d2eca679c7ac4bb7acb8a8950a83cbff"
     }
    },
    "746f4e383e0047b28e64d9d93fffe0aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7dbec6e19ec470c9f7289056fc4bfb1",
      "placeholder": "",
      "style": "IPY_MODEL_15f6d4fd47d0447597adc90e439d2483",
      "value": " 466k/466k [00:00&lt;00:00, 1.05MB/s]"
     }
    },
    "7927b9590f3b435da0347bce52ce60dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f14314f8e9947a4bc9f83737f137386": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5290ba323ad94ad3b3a02d291fed4cb0",
      "placeholder": "",
      "style": "IPY_MODEL_9254fe5fc823456bb534029578e4afb1",
      "value": " 232k/232k [00:02&lt;00:00, 94.9kB/s]"
     }
    },
    "8774db4d488540d5a640356bceefdb04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d474015aada485883408a7c89d6d190": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d6916f93ffc41cca2bf8be42251ab5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8774db4d488540d5a640356bceefdb04",
      "max": 267967963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3548e016675142abb7dc0200635130b8",
      "value": 267967963
     }
    },
    "9254fe5fc823456bb534029578e4afb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a52fa27484f34ebdb0abc12c73bf07df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acd20cabcaca47898ce9e056f81dab57": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0861ec9ac15401783bfd9e92487fe43": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f785c79f13a48099bbb1477df8aa16d",
       "IPY_MODEL_0feedfdfac074b0fb7a7950a8a65fe6e"
      ],
      "layout": "IPY_MODEL_a52fa27484f34ebdb0abc12c73bf07df"
     }
    },
    "b21f2efe553e4b388d8647e06e8288f2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b60402a9beae452eb791e6d3caf644ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7ce597d0d0741f5aca0711118f7c9f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be5d412ffb7b439f977f29c84fef6974": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1397386380fe4080ad53a54958c2fbe6",
      "placeholder": "",
      "style": "IPY_MODEL_8d474015aada485883408a7c89d6d190",
      "value": " 28.0/28.0 [00:01&lt;00:00, 27.6B/s]"
     }
    },
    "c084b8dc32d442fe9310792cd36e657c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7927b9590f3b435da0347bce52ce60dc",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c8314794652f4c09922340116c972962",
      "value": 231508
     }
    },
    "c60890f71d0241f698caf3304e727668": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8314794652f4c09922340116c972962": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ca00d8df97514714ba60deb51512397f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acd20cabcaca47898ce9e056f81dab57",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cae5484150364375b7e97bd83366b9f9",
      "value": 28
     }
    },
    "cae5484150364375b7e97bd83366b9f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d0a6d051ee204c33a851ed98dc6cbf33": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b21f2efe553e4b388d8647e06e8288f2",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d4d62208b0e45b5a063e2d85ab07ce1",
      "value": 466062
     }
    },
    "d2eca679c7ac4bb7acb8a8950a83cbff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7dbec6e19ec470c9f7289056fc4bfb1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0b8894c06e5406098b1b70403041316": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa80c969c2c14b478caffc7d37384384": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c084b8dc32d442fe9310792cd36e657c",
       "IPY_MODEL_7f14314f8e9947a4bc9f83737f137386"
      ],
      "layout": "IPY_MODEL_b60402a9beae452eb791e6d3caf644ea"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
