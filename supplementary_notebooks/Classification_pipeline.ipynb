{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification_pipeline.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"p270iyuaIsPb"},"source":["This notebook is written based on [this reference implementation](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb).\n","\n","Other refs for model:\n","* https://stackoverflow.com/questions/65205582/how-can-i-add-a-bi-lstm-layer-on-top-of-bert-model\n","* https://discuss.pytorch.org/t/how-to-connect-hook-two-or-even-more-models-together/21033\n","* https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n","* https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n","\n","Other refs for torchtext:\n","* https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84\n","* https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496\n","* http://anie.me/On-Torchtext/"]},{"cell_type":"markdown","metadata":{"id":"FRXm2FfxzC9A"},"source":["# Imports and setup"]},{"cell_type":"code","metadata":{"id":"Kd2O789-y-Hh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621561230751,"user_tz":420,"elapsed":1338,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"13421344154837126638"}},"outputId":"5c134834-a2bd-4d23-b99a-dc73b27d27c9"},"source":["# Mount Google Drive.\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sPBhuqfq2PcN","executionInfo":{"status":"ok","timestamp":1621557184665,"user_tz":420,"elapsed":7790,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"13421344154837126638"}},"outputId":"94bec5b8-567b-41ed-b743-53a4a9487c4e"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 8.8MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Collecting huggingface-hub==0.0.8\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 37.3MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 24.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QtVQ8yVAzMTS"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import random\n","random.seed(1)\n","import re\n","\n","# Data processing.\n","import torch\n","from torchtext.legacy import data \n","\n","# Model.\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import DistilBertModel, DistilBertTokenizer\n","\n","# Training.\n","from sklearn.model_selection import KFold\n","\n","# Visualization.\n","import matplotlib.pyplot as plt\n","\n","# Set working directory.\n","os.chdir('/content/gdrive/My Drive/personal/CS224U/project')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ql2Dve4xza-Z"},"source":["# Load a pre-trained BERT model"]},{"cell_type":"code","metadata":{"id":"gLPcwP45zoL5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621561242438,"user_tz":420,"elapsed":3856,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"13421344154837126638"}},"outputId":"c11fff5a-9fc5-4b19-ff39-8973d1c90705"},"source":["WEIGHTS_NAME = 'distilbert-base-uncased'\n","\n","tokenizer = DistilBertTokenizer.from_pretrained(WEIGHTS_NAME)\n","bert = DistilBertModel.from_pretrained(WEIGHTS_NAME)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"PL4p_28SzGCv"},"source":["# Read the data"]},{"cell_type":"code","metadata":{"id":"HwNKmWDey-q9"},"source":["# For illustrative purposes, reading just one study as the \"dataset\".\n","# TODO: read and combine the data from the rest of the studies.\n","sheet_df = pd.read_excel(\"Idea Ratings_Berg_2019_OBHDP.xlsx\", sheet_name=0)\n","sheet_df.dropna(inplace=True) # For some reason, first sheet has an extra NaN row at the bottom. This makes sure it's removed.\n","data_df = sheet_df[['Final_Idea', 'Creativity_Combined']].rename(columns={'Final_Idea': 'text', 'Creativity_Combined': 'label'})\n","\n","# shuffle the rows\n","data_df = data_df.sample(frac=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5vqzjM_WJ8l"},"source":["# A utility function for reading data\n","# Takes the number of the study/sample and the label we want to extract (e.g., \"Novelty_Combined\")\n","# Return the a df with a column named 'text' and a column named 'label'\n","# Can also choose \n","\n","def get_data(study, metric, shuffle = True):\n","\n","  sheet_df = pd.read_excel(\"Idea Ratings_Berg_2019_OBHDP.xlsx\", sheet_name=study-1) \n","  sheet_df.dropna(inplace=True)\n","  data_df = sheet_df[['Final_Idea', 'Creativity_Combined']].rename(columns={'Final_Idea': 'text', metric: 'label'})\n","\n","  if shuffle:\n","    data_df = data_df.sample(frac=1)\n","  return data_df\n","\n","# Take a list with the numbers of studies\n","# Extract multiple datasets with get_data and concatenate them\n","\n","def get_multiple_datasets(study_list, metric, shuffle = True):\n","  dfs = [get_data(study, metric, shuffle) for study in study_list]\n","  return pd.concat(dfs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"GV8Gg0ytm73b","executionInfo":{"status":"ok","timestamp":1621571184880,"user_tz":420,"elapsed":1621,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"13421344154837126638"}},"outputId":"af9ef8fa-5d97-437b-faf7-a471f2c9fe18"},"source":["# illustrate the function\n","get_multiple_datasets([1,3], \"Creativity_Combined\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>69</th>\n","      <td>The idea is a product that can help with more ...</td>\n","      <td>4.325</td>\n","    </tr>\n","    <tr>\n","      <th>250</th>\n","      <td>I think it would be good to ad some sort of fa...</td>\n","      <td>2.975</td>\n","    </tr>\n","    <tr>\n","      <th>121</th>\n","      <td>A new kind of treadmill that is fully accessib...</td>\n","      <td>4.000</td>\n","    </tr>\n","    <tr>\n","      <th>252</th>\n","      <td>The device would have straps that attached to ...</td>\n","      <td>2.975</td>\n","    </tr>\n","    <tr>\n","      <th>273</th>\n","      <td>A foldable step machine that you can use as a ...</td>\n","      <td>2.675</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>This is a European Tour done by train in which...</td>\n","      <td>4.225</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>Take a Haunted Trip Destinations could be to A...</td>\n","      <td>4.825</td>\n","    </tr>\n","    <tr>\n","      <th>310</th>\n","      <td>A train travel of Bernina Express between Chur...</td>\n","      <td>2.175</td>\n","    </tr>\n","    <tr>\n","      <th>278</th>\n","      <td>a small train with about ten cabs behind the f...</td>\n","      <td>3.300</td>\n","    </tr>\n","    <tr>\n","      <th>186</th>\n","      <td>This vacation is a comedy tour across Europe. ...</td>\n","      <td>4.125</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>615 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  text  label\n","69   The idea is a product that can help with more ...  4.325\n","250  I think it would be good to ad some sort of fa...  2.975\n","121  A new kind of treadmill that is fully accessib...  4.000\n","252  The device would have straps that attached to ...  2.975\n","273  A foldable step machine that you can use as a ...  2.675\n","..                                                 ...    ...\n","174  This is a European Tour done by train in which...  4.225\n","60   Take a Haunted Trip Destinations could be to A...  4.825\n","310  A train travel of Bernina Express between Chur...  2.175\n","278  a small train with about ten cabs behind the f...  3.300\n","186  This vacation is a comedy tour across Europe. ...  4.125\n","\n","[615 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8I4oKjCgT8HV","executionInfo":{"status":"ok","timestamp":1621571184880,"user_tz":420,"elapsed":6,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"13421344154837126638"}},"outputId":"4d4757da-8e53-45b0-fcda-2de1ed9e4166"},"source":["# For prototype purposes:\n","# assign binary classification labels\n","# score <= 3.8 --> negative (not creative)\n","# score > 3.8 --> positive (creative)\n","data_df['label'] = data_df['label'].apply(lambda x: 0 if x <= 3.8 else 1)\n","print(data_df.head(1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                                  text  label\n","204  My idea is to make a bike that can be used for...      0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wvWVLrupUEnt"},"source":["# For prototype purposes:\n","# split into train, test sets. (Train set will be further split into \n","# train+validation sets, via k-fold CV.)\n","train_df = data_df[:200]\n","test_df = data_df[200:]\n","\n","# write them to CSV files\n","train_df.to_csv('train.csv', index=False, header=False)\n","test_df.to_csv('test.csv', index=False, header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXVH4t-UC_wD"},"source":["## Preprocessing and transform into torchtext Dataset format.\n","\n","From what I understand, some preprocessing is done when data.Field() is applied."]},{"cell_type":"code","metadata":{"id":"ifqSE8yj76Vb"},"source":["INIT_TOKEN_IDX = tokenizer.cls_token_id\n","EOS_TOKEN_IDX = tokenizer.sep_token_id\n","PAD_TOKEN_IDX = tokenizer.pad_token_id\n","UNK_TOKEN_IDX = tokenizer.unk_token_id\n","\n","# BERT input can be at most 512 words\n","MAX_INPUT_LENGTH = tokenizer.max_model_input_sizes[WEIGHTS_NAME]\n","\n","# Apply tokenization and some preprocessing steps to the input sentence.\n","# Namely, this trims examples down to MAX_INPUT_LENGTH. (There is a -2 \n","# since the [CLS] and [SEP] tokens will be added)\n","def tokenize_and_cut(sentence):\n","  tokens = tokenizer.tokenize(sentence) \n","  tokens = tokens[:MAX_INPUT_LENGTH-2]\n","  return tokens\n","\n","# text_fields defines preprocessing and handling of the text of an example.\n","text_fields = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = tokenize_and_cut,\n","                  preprocessing = tokenizer.convert_tokens_to_ids,\n","                  init_token = INIT_TOKEN_IDX, # add [CLS] token\n","                  eos_token = EOS_TOKEN_IDX, # add [SEP] token\n","                  pad_token = PAD_TOKEN_IDX,\n","                  unk_token = UNK_TOKEN_IDX)\n","\n","# label_fields defines how to handle the label of an example.\n","label_fields = data.LabelField(dtype = torch.float)\n","all_fields = [('text', text_fields), ('label', label_fields)]\n","\n","train_dataset, test_dataset = data.TabularDataset.splits(\n","  path='', # path='' because the csvs are in the same directory\n","  train='train.csv', test='test.csv', format='csv',\n","  fields=all_fields # must match order of cols in csv \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dNREAkRrITo","executionInfo":{"status":"ok","timestamp":1621571185780,"user_tz":420,"elapsed":11,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"13421344154837126638"}},"outputId":"a412254e-0d0a-4a60-8b7b-75a3d66f2728"},"source":["# Just inspect what the tokenizer is doing\n","# // and escape characters \\ are kept. We may want to remove them\n","print(data_df['text'][1])\n","tokenize_and_cut(data_df['text'][1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The \"Real Row\" is an exercise machine that simulates the real feeling of rowing down your favorite river. With the capability to change yaw, pitch, and roll (to a limited degree) you'll feel like you're outside enjoying the water. /  / Sit down, strap on the seatbelt, and choose a route from the monitor. The machine will them program all the motion that would occur on that river as you row down it. The resistance will change based on water conditions. The boat will twist and turn, rise and fall, as you cross simulated waves. /  / As you row, the monitor will display a beautifully rendered landscape along with the river you're on. You can actually see more challenging or less challenging paths, and steer towards your preference. /  / The two-person variant offers you a chance to work out with a partner, and will provide both visual and audio feedback on how well you are working together.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['the',\n"," '\"',\n"," 'real',\n"," 'row',\n"," '\"',\n"," 'is',\n"," 'an',\n"," 'exercise',\n"," 'machine',\n"," 'that',\n"," 'simulate',\n"," '##s',\n"," 'the',\n"," 'real',\n"," 'feeling',\n"," 'of',\n"," 'rowing',\n"," 'down',\n"," 'your',\n"," 'favorite',\n"," 'river',\n"," '.',\n"," 'with',\n"," 'the',\n"," 'capability',\n"," 'to',\n"," 'change',\n"," 'ya',\n"," '##w',\n"," ',',\n"," 'pitch',\n"," ',',\n"," 'and',\n"," 'roll',\n"," '(',\n"," 'to',\n"," 'a',\n"," 'limited',\n"," 'degree',\n"," ')',\n"," 'you',\n"," \"'\",\n"," 'll',\n"," 'feel',\n"," 'like',\n"," 'you',\n"," \"'\",\n"," 're',\n"," 'outside',\n"," 'enjoying',\n"," 'the',\n"," 'water',\n"," '.',\n"," '/',\n"," '/',\n"," 'sit',\n"," 'down',\n"," ',',\n"," 'strap',\n"," 'on',\n"," 'the',\n"," 'seat',\n"," '##belt',\n"," ',',\n"," 'and',\n"," 'choose',\n"," 'a',\n"," 'route',\n"," 'from',\n"," 'the',\n"," 'monitor',\n"," '.',\n"," 'the',\n"," 'machine',\n"," 'will',\n"," 'them',\n"," 'program',\n"," 'all',\n"," 'the',\n"," 'motion',\n"," 'that',\n"," 'would',\n"," 'occur',\n"," 'on',\n"," 'that',\n"," 'river',\n"," 'as',\n"," 'you',\n"," 'row',\n"," 'down',\n"," 'it',\n"," '.',\n"," 'the',\n"," 'resistance',\n"," 'will',\n"," 'change',\n"," 'based',\n"," 'on',\n"," 'water',\n"," 'conditions',\n"," '.',\n"," 'the',\n"," 'boat',\n"," 'will',\n"," 'twist',\n"," 'and',\n"," 'turn',\n"," ',',\n"," 'rise',\n"," 'and',\n"," 'fall',\n"," ',',\n"," 'as',\n"," 'you',\n"," 'cross',\n"," 'simulated',\n"," 'waves',\n"," '.',\n"," '/',\n"," '/',\n"," 'as',\n"," 'you',\n"," 'row',\n"," ',',\n"," 'the',\n"," 'monitor',\n"," 'will',\n"," 'display',\n"," 'a',\n"," 'beautifully',\n"," 'rendered',\n"," 'landscape',\n"," 'along',\n"," 'with',\n"," 'the',\n"," 'river',\n"," 'you',\n"," \"'\",\n"," 're',\n"," 'on',\n"," '.',\n"," 'you',\n"," 'can',\n"," 'actually',\n"," 'see',\n"," 'more',\n"," 'challenging',\n"," 'or',\n"," 'less',\n"," 'challenging',\n"," 'paths',\n"," ',',\n"," 'and',\n"," 'steer',\n"," 'towards',\n"," 'your',\n"," 'preference',\n"," '.',\n"," '/',\n"," '/',\n"," 'the',\n"," 'two',\n"," '-',\n"," 'person',\n"," 'variant',\n"," 'offers',\n"," 'you',\n"," 'a',\n"," 'chance',\n"," 'to',\n"," 'work',\n"," 'out',\n"," 'with',\n"," 'a',\n"," 'partner',\n"," ',',\n"," 'and',\n"," 'will',\n"," 'provide',\n"," 'both',\n"," 'visual',\n"," 'and',\n"," 'audio',\n"," 'feedback',\n"," 'on',\n"," 'how',\n"," 'well',\n"," 'you',\n"," 'are',\n"," 'working',\n"," 'together',\n"," '.']"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBiWBfQ4A-Fa","executionInfo":{"status":"ok","timestamp":1621571185781,"user_tz":420,"elapsed":10,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"13421344154837126638"}},"outputId":"b461de12-5583-4c29-ae68-4b7e75ee3f09"},"source":["# We have to build a 'vocabulary' for the labels.\n","label_fields.build_vocab(train_dataset)\n","# TODO: make this 0=1 situation less confusing...\n","print(label_fields.vocab.stoi)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["defaultdict(None, {'1': 0, '0': 1})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zfxEmIj65PFa"},"source":["# Transform train_dataset into an np array representation.\n","# This will be used for generating the K folds.\n","train_exs_arr = np.array(train_dataset.examples)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXHfsEVkJ8k6"},"source":["# Define the BERT-RNN model"]},{"cell_type":"code","metadata":{"id":"AHsN05EpJ-dz"},"source":["class BERTRNN(nn.Module):\n","  def __init__(self,\n","               bert,\n","               hidden_dim,\n","               output_dim,\n","               n_layers,\n","               bidirectional,\n","               dropout):\n","    super().__init__()\n","    self.bert = bert\n","    # Modify this if we want to concatenate something onto BERT embedding\n","    # Note: 'dim' is equivalent of 'hidden_size' for BERT model\n","    embedding_dim = bert.config.to_dict()['dim']\n","\n","    # TODO: change to lstm cells.\n","    self.rnn = nn.GRU(embedding_dim,\n","                      hidden_dim,\n","                      num_layers = n_layers,\n","                      bidirectional = bidirectional,\n","                      batch_first = True,\n","                      dropout = 0 if n_layers < 2 else dropout)\n","    \n","    # TODO: need to modify this if bidirectional=True\n","    self.out = nn.Linear(hidden_dim, output_dim)\n","    self.dropout = nn.Dropout(dropout)\n","    # TODO: we probably need some regression output layer instead.\n","\n","  def forward(self, text):\n","    # forward pass of bert; then take the output of CLS token\n","    embedded = self.bert(text)[0]\n","\n","    _, hidden = self.rnn(embedded)\n","\n","    # TODO: need to modify this if bidirectional=True\n","    # for prototype purposes, assume we won't use bidirectional\n","    hidden = self.dropout(hidden[-1,:,:])\n","    output = self.out(hidden)\n","    return output\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Atbwnil81dS"},"source":["# Instantiate the model\n","HIDDEN_DIM = 10 # TODO: this should be much bigger\n","OUTPUT_DIM = 1\n","N_LAYERS = 1\n","BIDIRECTIONAL = False\n","DROPOUT = 0.25\n","\n","model = BERTRNN(bert,\n","                HIDDEN_DIM,\n","                OUTPUT_DIM,\n","                N_LAYERS,\n","                BIDIRECTIONAL,\n","                DROPOUT)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wb9qlt-_0xlO"},"source":["# Training pipeline begins here\n"]},{"cell_type":"markdown","metadata":{"id":"aSm060wNE-NF"},"source":["## Define training parameters"]},{"cell_type":"code","metadata":{"id":"Ht9cEYK_TeGB"},"source":["BATCH_SIZE = 16 # TODO increase this\n","N_EPOCHS = 2 # TODO we can increase this\n","\n","optimizer = optim.Adam(model.parameters())\n","criterion = nn.BCEWithLogitsLoss()\n","# TODO: place model + criterion onto GPU device."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Puhyx2QX_Ax4"},"source":["# model.train() # Uncomment to view structure of model."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8Dx5wUUFP2h"},"source":["## Define helper functions"]},{"cell_type":"code","metadata":{"id":"vy6A8AMkTuK5"},"source":["def binary_accuracy(preds, y):\n","  rounded_preds = torch.round(torch.sigmoid(preds))\n","  correct = (rounded_preds == y).float()\n","  acc = correct.sum() / len(correct)\n","  return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8GqqMgkoSXAW"},"source":["def train(model, iterator, optimizer, criterion):\n","  epoch_loss = 0\n","  epoch_acc = 0\n","  \n","  model.train()\n","\n","  for batch in iterator:\n","    optimizer.zero_grad()\n","    predictions = model(batch.text).squeeze(1)\n","    loss = criterion(predictions, batch.label)\n","    acc = binary_accuracy(predictions, batch.label)\n","    loss.backward()\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xE6LEiJTMAc"},"source":["def evaluate(model, iterator, criterion):\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  model.eval()\n","\n","  with torch.no_grad():\n","    for batch in iterator:\n","      predictions = model(batch.text).squeeze(1)\n","      loss = criterion(predictions, batch.label)\n","      acc = binary_accuracy(predictions, batch.label)\n","      epoch_loss += loss.item()\n","      epoch_acc += acc.item()\n","\n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"87B9_LRZ_Jtk"},"source":["# Given train and validation datasets, returns 2 iterators.\n","def get_iterators(train_data, valid_data):\n","  return data.BucketIterator.splits(\n","    (train_data, valid_data),\n","    batch_size = BATCH_SIZE,\n","    # Below are needed to overcome error when calling evaluate():\n","    # TypeError: '<' not supported between instances of 'Example' and 'Example'\n","    sort_key = lambda x: len(x.text),\n","    sort_within_batch = False,\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pKN3vFd0FSre"},"source":["## The cell where it actually trains!"]},{"cell_type":"code","metadata":{"id":"2rJKFKqxSXLE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621575770034,"user_tz":420,"elapsed":2687589,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"13421344154837126638"}},"outputId":"13b233f8-ac74-4838-8f31-54dd948ad3b3"},"source":["best_valid_loss = float('inf')\n","\n","# The main training loop\n","# TODO: add some sort of weights-saving, either periodically or at the end\n","# This way we can save our trained model and use it easily for downstream\n","# analysis without having to re-train.\n","# TODO: add some sort of timing info / progress bar.\n","def launch_experiment(train_data_df):\n","  best_valid_loss = float('inf') \n","  # best_valid_loss is a local variable in this function and I added this line to prevent a potential error\n","\n","  kf = KFold(n_splits=5)\n","  for train_index, valid_index in kf.split(train_data_df):\n","    train_data = data.Dataset(train_exs_arr[train_index], all_fields)\n","    valid_data = data.Dataset(train_exs_arr[valid_index], all_fields)\n","\n","    train_iterator, valid_iterator = get_iterators(train_data, valid_data)\n","\n","    for epoch in range(N_EPOCHS):\n","      train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","      valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","\n","      if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","      \n","      # Added some \n","      print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","launch_experiment(train_exs_arr)\n","print(best_valid_loss)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\tTrain Loss: 0.704 | Train Acc: 50.62%\n","\t Val. Loss: 0.692 |  Val. Acc: 52.08%\n","\tTrain Loss: 0.701 | Train Acc: 46.88%\n","\t Val. Loss: 0.692 |  Val. Acc: 52.08%\n","\tTrain Loss: 0.685 | Train Acc: 55.00%\n","\t Val. Loss: 0.688 |  Val. Acc: 56.25%\n","\tTrain Loss: 0.704 | Train Acc: 43.75%\n","\t Val. Loss: 0.688 |  Val. Acc: 56.25%\n","\tTrain Loss: 0.696 | Train Acc: 45.62%\n","\t Val. Loss: 0.685 |  Val. Acc: 60.42%\n","\tTrain Loss: 0.680 | Train Acc: 59.38%\n","\t Val. Loss: 0.686 |  Val. Acc: 60.42%\n","\tTrain Loss: 0.695 | Train Acc: 53.75%\n","\t Val. Loss: 0.683 |  Val. Acc: 66.67%\n","\tTrain Loss: 0.721 | Train Acc: 45.62%\n","\t Val. Loss: 0.684 |  Val. Acc: 66.67%\n","\tTrain Loss: 0.687 | Train Acc: 56.25%\n","\t Val. Loss: 0.694 |  Val. Acc: 50.00%\n","\tTrain Loss: 0.703 | Train Acc: 50.00%\n","\t Val. Loss: 0.694 |  Val. Acc: 50.00%\n","inf\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NIXP6rB_FY8K"},"source":["# Test the trained model on held-out dataset."]},{"cell_type":"code","metadata":{"id":"-1w-zVceFsx3"},"source":["# Get a test iterator\n","test_iterator = data.BucketIterator(\n","  test_dataset,\n","  batch_size = BATCH_SIZE,\n","  # Below are needed to overcome error when calling evaluate():\n","  # TypeError: '<' not supported between instances of 'Example' and 'Example'\n","  sort_key = lambda x: len(x.text),\n","  sort_within_batch = False,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XgAhdm2rTGrP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621571325507,"user_tz":420,"elapsed":49311,"user":{"displayName":"Xubo Cao","photoUrl":"","userId":"13421344154837126638"}},"outputId":"a5ce6703-689a-4595-ee14-d8707ad080d5"},"source":["# Accuracy is about chance right now.\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","print(test_loss)\n","print(test_acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6918129069464547\n","0.5267857142857143\n"],"name":"stdout"}]}]}