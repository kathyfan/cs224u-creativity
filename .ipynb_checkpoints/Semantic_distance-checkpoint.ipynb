{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaWDRfwsg8VS"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1621583887592,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "MpUJ3wmbCl7y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "random.seed(1)\n",
    "import re\n",
    "import nltk\n",
    "import semdis\n",
    "\n",
    "\n",
    "# Set working directory.\n",
    "#os.chdir('/content/gdrive/My Drive/personal/CS224U/project')\n",
    "# os.listdir() # Uncomment to sanity check that you're in the right directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwCk0EBEg1sS"
   },
   "source": [
    "# Word count and score distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P4IWfPB5duIZ"
   },
   "outputs": [],
   "source": [
    "# Takes a dataframe and adds a 'wordcount' column (modifies original df)\n",
    "# Requirement: df has existing 'text' column\n",
    "# Returns modified df with 'wordcount' column added\n",
    "def add_wordcount_col(df):\n",
    "  assert 'text' in df\n",
    "  df['wordcount'] = df['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mIdac2q9elXi"
   },
   "outputs": [],
   "source": [
    "# Takes a dataframe and returns a new dataframe containing only rows where wordcount < max_words\n",
    "# Requirement: input df has existing 'wordcount' column\n",
    "def restrict_by_wordcount(df, max_words):\n",
    "  assert 'wordcount' in df\n",
    "  df_restricted = df.loc[df['wordcount'] <= max_words]\n",
    "  return df_restricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 36945,
     "status": "ok",
     "timestamp": 1621584189834,
     "user": {
      "displayName": "Xubo Cao",
      "photoUrl": "",
      "userId": "10438589586656072815"
     },
     "user_tz": 420
    },
    "id": "kjP1rMTECkas"
   },
   "outputs": [],
   "source": [
    "import utils  \n",
    "\n",
    "glove_dict = utils.glove2dict('glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV3MkfpFBAna"
   },
   "source": [
    "## Calculate Semantic Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZWAUD3t8C231"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import scipy.spatial.distance\n",
    "\n",
    "# Takes two sequence and create a composite vector for each sequence.\n",
    "# Return the cosine similarity between the two vectors.\n",
    "\n",
    "def get_distance_between_texts(text1, text2, VSM = glove_dict,\n",
    "                               multiply = True,\n",
    "                               tokenizer = nltk.word_tokenize,\n",
    "                               remove_stopwords = True,\n",
    "                               remove_punct = True):\n",
    "  \n",
    "  v1 = get_text_vector(text1, VSM, multiply, tokenizer, remove_stopwords),\n",
    "  v2 = get_text_vector(text2, VSM, multiply, tokenizer, remove_stopwords)\n",
    "\n",
    "  return scipy.spatial.distance.cosine(v1, v2)\n",
    "\n",
    "# Takes a sequence and a VSM. Return a composite vector that represents the sequence\n",
    "# Extract word vectors from the VSM and combine them with either multiplication or addition (default is multiplication)\n",
    "# Set multiply = False to use addition\n",
    "# Default tokenizer is nltk word tokenizer. \n",
    "# Remove stopwords and punctuations by default.\n",
    "\n",
    "## TODO: Trying weighted sum (e.g., IDF weighting)\n",
    "def get_text_vector(text, \n",
    "                    VSM, # the VSM (a dictionary) used to derive word vectors\n",
    "                    multiply = True,\n",
    "                    tokenizer = nltk.word_tokenize,\n",
    "                    remove_stopwords = True,\n",
    "                    remove_punct = True):\n",
    "  \n",
    "  if remove_punct:\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "  \n",
    "  \n",
    "  words = tokenizer(text)\n",
    "\n",
    "  if remove_stopwords:\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    words = [w for w in words if not w in stop_words] \n",
    "\n",
    "    \n",
    "  \n",
    "  words = [w for w in words if w in VSM] \n",
    "\n",
    "  # Uncomment this for sanity check\n",
    "  #print(len(words))\n",
    "  if len(words) > 0:\n",
    "    v = VSM[words[0]]\n",
    "    for word in words[1:]:\n",
    "      if multiply:\n",
    "        v= np.multiply(v, VSM[word])\n",
    "      else:\n",
    "        v = v+VSM[word]\n",
    "  else:\n",
    "    # If no word is found in the dictionary, return a random vector\n",
    "    v = np.random.rand(300)\n",
    "\n",
    "  return v\n",
    "\n",
    "#test the function\n",
    "get_distance_between_texts(\"test, text\", \"test text\", glove_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5940629763576282, 0.5940629763576282]\n",
      "0.3960419842384188\n",
      "0.5940629763576282\n",
      "0.7448144358708455\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "imp.reload(semdis)\n",
    "\n",
    "# take a sentence and return\n",
    "def distances_within_text(text,\n",
    "                          tokenizer = nltk.word_tokenize,\n",
    "                         remove_stopwords =True,\n",
    "                             remove_punct = True):\n",
    "    if remove_punct:\n",
    "        text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    words = tokenizer(text)\n",
    "    if remove_stopwords:\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        words = [w for w in words if not w in stop_words] \n",
    "    n = len(words)\n",
    "    distances = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if words[i] in glove_dict and words[j] in glove_dict:\n",
    "                distances.append(semdis.get_word_cosine(words[i], words[j], vsm = glove_dict))\n",
    "        else:\n",
    "            continue\n",
    "    return distances\n",
    "\n",
    "# test the function\n",
    "test_text= 'test, test. score'\n",
    "print(distances_within_sentence(test_text))\n",
    "\n",
    "# Take a sequence and a pooling function (e.g., max, min, average)\n",
    "# Calculate the semantic distances between all word pairs and pool them using the given function.\n",
    "def pool_distances_within_text(text, pool = np.average, **kwarg):\n",
    "    distances = distances_within_text(text, **kwarg)\n",
    "    return pool(distances)\n",
    "\n",
    "# Test the function\n",
    "print(pool_distances_within_text(test_text))\n",
    "print(pool_distances_within_text(test_text, pool = max))\n",
    "print(pool_distances_within_text(test_text, pool = max, remove_punct = False))\n",
    "\n",
    "# Two-step pooling \n",
    "# Pool distances within each sentence at first using the function given for the \"sentence_pool\" argument\n",
    "# Pool these results using the function provided for the \"pool\" argument\n",
    "def pool_distances_split_sentence(text, pool = np.average, sentence_pool = np.average, **kwarg):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentence_pooled = [pool_distances_within_text(sentence, pool = sentence_pool, **kwarg) for sentence in sentences]\n",
    "    \n",
    "    # When there is only one word in a sentence, distances cannot be calculated and will return nan\n",
    "    # Drop nan using the following line\n",
    "    sentence_pooled = [x for x in sentence_pooled if np.isnan(x) == False]\n",
    "    return pool(sentence_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use semantic distance to predict creativity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_distances_for_df(responses, prompt, VSM = glove_dict, multiply = True):\n",
    "    return [get_distance_between_texts(prompt, x, VSM, multiply = multiply) for x in responses]\n",
    "\n",
    "# This function can be used to examine whether a certain feature is correlated with a construct\n",
    "# It takes a datafree with a column 'text' and a column 'label'.\n",
    "# The argument \"apply_to_column\" regulates whether the function should be apply to the whole ['text'] column \n",
    "# or each element in the column, the default is element-wise operation.\n",
    "def correlate_feature_with_creativity(df, function, apply_to_column = False, spearman = False, **kwarg):\n",
    "    if apply_to_column:\n",
    "        feature = function(df['text'], **kwarg)\n",
    "    else:\n",
    "        feature = [function(x, **kwarg) for x in df['text']]\n",
    "\n",
    "    \n",
    "    if spearman:\n",
    "        return scipy.stats.spearmanr(feature, df['label'])\n",
    "    else:\n",
    "        return scipy.stats.pearsonr(feature, df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-0d9b7e3f3e40>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-0d9b7e3f3e40>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    prompt = \"self-driving car\") ==\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import dataset\n",
    "fitness_df = dataset.get_data(1, 'Novelty_Combined', shuffle=True)\n",
    "car_df = dataset.get_data(2, 'Novelty_Combined', shuffle=True)\n",
    "\n",
    "correlate_feature_with_creativity(df = fitness_df,\n",
    "                                  function = get_distances_for_df,\n",
    "                                  apply_to_column = True,\n",
    "                                  spearman = True,\n",
    "                                  multiply = True,\n",
    "                                 prompt = \"fitness equipment\")\n",
    "\n",
    "\n",
    "# The following two functions are equivalent\n",
    "correlate_feature_with_creativity(df = car_df,\n",
    "                                  function = get_distances_for_df,\n",
    "                                  apply_to_column = True,\n",
    "                                  spearman = True,\n",
    "                                  multiply = True,\n",
    "                                 prompt = \"self-driving car\")\n",
    "correlate_feature_with_creativity(df = car_df,\n",
    "                                  function = get_distance_between_texts,\n",
    "                                  apply_to_column = False,\n",
    "                                  spearman = True,\n",
    "                                  multiply = True,\n",
    "                                 text2 = \"self-driving car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.1174380910023567, pvalue=0.004350859526027052)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are word counts correlated with word counts\n",
    "feature = get_distances_for_responses(prompt = \"fitness equipment\",\n",
    "                             responses = car_df['text'],\n",
    "                             VSM = glove_dict, multiply = True)\n",
    "add_wordcount_col(car_df)\n",
    "scipy.stats.spearmanr(feature, car_df['wordcount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with Beaty & Johnson's finding. Additive composition generates semantic distances that are negatively correlated with word counts; multiplicative composition generates semantic distances that are positively correlated with word counts."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Semantic Distance.ipynb",
   "provenance": [
    {
     "file_id": "18xltzQFDw5Lsdhzi_4CHL8VuI0Nplu7V",
     "timestamp": 1621333562891
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
